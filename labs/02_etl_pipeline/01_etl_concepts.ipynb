{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptos de ETL y ELT\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Entender qu√© es un pipeline de datos\n",
    "- Diferenciar entre ETL y ELT\n",
    "- Conocer los componentes de un pipeline\n",
    "- Identificar cu√°ndo usar cada enfoque\n",
    "\n",
    "## Prerequisitos\n",
    "- `00_setup/02_spark_basics.ipynb`\n",
    "\n",
    "## Tiempo Estimado\n",
    "‚è±Ô∏è 30 minutos\n",
    "\n",
    "## M√≥dulo AWS Academy Relacionado\n",
    "üìö M√≥dulo 6: Data Processing and Analysis - ETL concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark listo para ETL\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ETL_Concepts\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark listo para ETL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 1 ===\n",
    "## 1. ¬øQu√© es un Pipeline de Datos?\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "Un **pipeline de datos** es un conjunto de procesos que mueven datos desde una o m√°s fuentes hasta uno o m√°s destinos, aplicando transformaciones en el camino.\n",
    "\n",
    "**Analog√≠a del mundo real:** Imagina una f√°brica de jugo de naranja:\n",
    "- **Extract**: Las naranjas llegan de diferentes granjas (fuentes)\n",
    "- **Transform**: Se lavan, pelan, exprimen y filtran (procesamiento)\n",
    "- **Load**: El jugo se embotella y distribuye (destino)\n",
    "\n",
    "En datos:\n",
    "- **Extract**: Leer datos de bases de datos, APIs, archivos\n",
    "- **Transform**: Limpiar, normalizar, enriquecer, agregar\n",
    "- **Load**: Guardar en data warehouse, data lake, base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATOS CRUDOS (Extract):\n",
      "+---+--------------+------+----------+------+\n",
      "|id |nombre        |ciudad|fecha     |monto |\n",
      "+---+--------------+------+----------+------+\n",
      "|001|  Juan P√©rez  |CDMX  |2024-01-15|1500.5|\n",
      "|002|maria garcia  |gdl   |2024/01/16|2300.0|\n",
      "|003|CARLOS LOPEZ  |MTY   |15-01-2024|NULL  |\n",
      "|002|maria garcia  |gdl   |2024/01/16|2300.0|\n",
      "+---+--------------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo simple de pipeline\n",
    "\n",
    "# 1. EXTRACT - Simular datos de fuente\n",
    "datos_crudos = [\n",
    "    (\"001\", \"  Juan P√©rez  \", \"CDMX\", \"2024-01-15\", 1500.50),\n",
    "    (\"002\", \"maria garcia\", \"gdl\", \"2024/01/16\", 2300.00),\n",
    "    (\"003\", \"CARLOS LOPEZ\", \"MTY\", \"15-01-2024\", None),  # Valor nulo\n",
    "    (\"002\", \"maria garcia\", \"gdl\", \"2024/01/16\", 2300.00),  # Duplicado\n",
    "]\n",
    "\n",
    "df_raw = spark.createDataFrame(\n",
    "    datos_crudos, \n",
    "    [\"id\", \"nombre\", \"ciudad\", \"fecha\", \"monto\"]\n",
    ")\n",
    "\n",
    "print(\"DATOS CRUDOS (Extract):\")\n",
    "df_raw.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATOS TRANSFORMADOS (Transform):\n",
      "+---+------------+------+----------+------+--------------------------+\n",
      "|id |nombre      |ciudad|fecha     |monto |fecha_proceso             |\n",
      "+---+------------+------+----------+------+--------------------------+\n",
      "|001|Juan P√©rez  |CDMX  |2024-01-15|1500.5|2026-02-12 00:27:54.680298|\n",
      "|002|Maria Garcia|GDL   |2024/01/16|2300.0|2026-02-12 00:27:54.680298|\n",
      "|003|Carlos Lopez|MTY   |15-01-2024|0.0   |2026-02-12 00:27:54.680298|\n",
      "+---+------------+------+----------+------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. TRANSFORM - Limpiar y normalizar\n",
    "\n",
    "df_transformed = df_raw \\\n",
    "    .dropDuplicates([\"id\"]) \\\n",
    "    .withColumn(\"nombre\", F.initcap(F.trim(F.col(\"nombre\")))) \\\n",
    "    .withColumn(\"ciudad\", F.upper(F.col(\"ciudad\"))) \\\n",
    "    .withColumn(\"monto\", F.coalesce(F.col(\"monto\"), F.lit(0.0))) \\\n",
    "    .withColumn(\"fecha_proceso\", F.current_timestamp())\n",
    "\n",
    "print(\"DATOS TRANSFORMADOS (Transform):\")\n",
    "df_transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATOS LISTOS PARA CARGA (Load):\n",
      "  Registros: 3\n",
      "  Columnas: ['id', 'nombre', 'ciudad', 'fecha', 'monto', 'fecha_proceso']\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- nombre: string (nullable = true)\n",
      " |-- ciudad: string (nullable = true)\n",
      " |-- fecha: string (nullable = true)\n",
      " |-- monto: double (nullable = false)\n",
      " |-- fecha_proceso: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. LOAD - Guardar (simulado)\n",
    "\n",
    "# En un pipeline real, aqui guardariamos a Parquet, base de datos, etc.\n",
    "# df_transformed.write.mode(\"overwrite\").parquet(\"/data/processed/clientes\")\n",
    "\n",
    "print(\"DATOS LISTOS PARA CARGA (Load):\")\n",
    "print(f\"  Registros: {df_transformed.count()}\")\n",
    "print(f\"  Columnas: {df_transformed.columns}\")\n",
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 2 ===\n",
    "## 2. ETL vs ELT\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "**ETL (Extract-Transform-Load)**\n",
    "- Transforma los datos ANTES de cargarlos al destino\n",
    "- El procesamiento ocurre en un servidor ETL dedicado\n",
    "- Tradicional, usado con data warehouses on-premise\n",
    "\n",
    "**ELT (Extract-Load-Transform)**\n",
    "- Carga los datos crudos primero, transforma despu√©s\n",
    "- El procesamiento ocurre en el destino (data lake/warehouse)\n",
    "- Moderno, aprovecha poder de c√≥mputo en la nube\n",
    "\n",
    "**¬øCu√°ndo usar cada uno?**\n",
    "\n",
    "| Criterio | ETL | ELT |\n",
    "|----------|-----|-----|\n",
    "| Volumen de datos | Peque√±o-Mediano | Grande |\n",
    "| Destino | Data Warehouse tradicional | Data Lake / Cloud DW |\n",
    "| C√≥mputo | Limitado en destino | Escalable en destino |\n",
    "| Latencia | Mayor | Menor (carga r√°pida) |\n",
    "| Flexibilidad | Menos (esquema fijo) | M√°s (esquema on-read) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENFOQUE ETL ===\n",
      "\n",
      "1. Extract: Leer ventas del d√≠a\n",
      "2. Transform EN SPARK:\n",
      "   - Limpiar datos\n",
      "   - Calcular totales\n",
      "   - Agregar por regi√≥n\n",
      "3. Load: Guardar resumen en Data Warehouse\n",
      "\n",
      "Ventaja: Solo datos limpios llegan al DW\n",
      "Desventaja: Si necesitas los datos crudos, no los tienes\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo conceptual: ETL tradicional\n",
    "print(\"=== ENFOQUE ETL ===\")\n",
    "print()\n",
    "print(\"1. Extract: Leer ventas del d√≠a\")\n",
    "print(\"2. Transform EN SPARK:\")\n",
    "print(\"   - Limpiar datos\")\n",
    "print(\"   - Calcular totales\")\n",
    "print(\"   - Agregar por regi√≥n\")\n",
    "print(\"3. Load: Guardar resumen en Data Warehouse\")\n",
    "print()\n",
    "print(\"Ventaja: Solo datos limpios llegan al DW\")\n",
    "print(\"Desventaja: Si necesitas los datos crudos, no los tienes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENFOQUE ELT ===\n",
      "\n",
      "1. Extract: Leer ventas del d√≠a\n",
      "2. Load: Guardar datos CRUDOS en Data Lake (S3)\n",
      "3. Transform EN EL DATA LAKE:\n",
      "   - Crear vista limpia\n",
      "   - Crear vista agregada\n",
      "   - Mantener datos crudos disponibles\n",
      "\n",
      "Ventaja: Flexibilidad, datos crudos siempre disponibles\n",
      "Desventaja: Requiere m√°s almacenamiento\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo conceptual: ELT moderno\n",
    "print(\"=== ENFOQUE ELT ===\")\n",
    "print()\n",
    "print(\"1. Extract: Leer ventas del d√≠a\")\n",
    "print(\"2. Load: Guardar datos CRUDOS en Data Lake (S3)\")\n",
    "print(\"3. Transform EN EL DATA LAKE:\")\n",
    "print(\"   - Crear vista limpia\")\n",
    "print(\"   - Crear vista agregada\")\n",
    "print(\"   - Mantener datos crudos disponibles\")\n",
    "print()\n",
    "print(\"Ventaja: Flexibilidad, datos crudos siempre disponibles\")\n",
    "print(\"Desventaja: Requiere m√°s almacenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 3 ===\n",
    "## 3. Componentes de un Pipeline ETL\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "Un pipeline robusto incluye:\n",
    "1. **Fuentes de datos**: Bases de datos, archivos, APIs\n",
    "2. **Validaci√≥n de entrada**: Verificar que los datos llegaron\n",
    "3. **Transformaciones**: Limpieza, enriquecimiento\n",
    "4. **Validaci√≥n de salida**: Verificar calidad de datos\n",
    "5. **Carga**: Escribir al destino\n",
    "6. **Logging**: Registrar qu√© pas√≥\n",
    "7. **Manejo de errores**: Qu√© hacer si algo falla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase ETLPipeline definida\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de pipeline estructurado\n",
    "\n",
    "class ETLPipeline:\n",
    "    \"\"\"Pipeline ETL simple para demostraci√≥n\"\"\"\n",
    "    \n",
    "    def __init__(self, spark, nombre):\n",
    "        self.spark = spark\n",
    "        self.nombre = nombre\n",
    "        self.logs = []\n",
    "    \n",
    "    def log(self, mensaje):\n",
    "        \"\"\"Registrar evento en el pipeline\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        log_entry = f\"[{timestamp}] {self.nombre}: {mensaje}\"\n",
    "        self.logs.append(log_entry)\n",
    "        print(log_entry)\n",
    "    \n",
    "    def extract(self, datos, schema):\n",
    "        \"\"\"Fase de extracci√≥n\"\"\"\n",
    "        self.log(\"Iniciando extracci√≥n...\")\n",
    "        df = self.spark.createDataFrame(datos, schema)\n",
    "        self.log(f\"Extra√≠dos {df.count()} registros\")\n",
    "        return df\n",
    "    \n",
    "    def validate(self, df, reglas):\n",
    "        \"\"\"Validar datos seg√∫n reglas\"\"\"\n",
    "        self.log(\"Validando datos...\")\n",
    "        errores = []\n",
    "        \n",
    "        for columna, regla in reglas.items():\n",
    "            if regla == \"not_null\":\n",
    "                nulos = df.filter(F.col(columna).isNull()).count()\n",
    "                if nulos > 0:\n",
    "                    errores.append(f\"{columna}: {nulos} nulos\")\n",
    "        \n",
    "        if errores:\n",
    "            self.log(f\"Advertencias: {errores}\")\n",
    "        else:\n",
    "            self.log(\"Validaci√≥n exitosa\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def transform(self, df, transformaciones):\n",
    "        \"\"\"Aplicar transformaciones\"\"\"\n",
    "        self.log(\"Aplicando transformaciones...\")\n",
    "        \n",
    "        for nombre_t, func in transformaciones.items():\n",
    "            df = func(df)\n",
    "            self.log(f\"  - Aplicada: {nombre_t}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def load(self, df, destino):\n",
    "        \"\"\"Cargar datos (simulado)\"\"\"\n",
    "        self.log(f\"Cargando a {destino}...\")\n",
    "        # En producci√≥n: df.write.mode(\"overwrite\").parquet(destino)\n",
    "        self.log(f\"Cargados {df.count()} registros\")\n",
    "        return df\n",
    "\n",
    "print(\"Clase ETLPipeline definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:28:10] VentasPipeline: Iniciando extracci√≥n...\n",
      "[00:28:11] VentasPipeline: Extra√≠dos 3 registros\n",
      "[00:28:11] VentasPipeline: Validando datos...\n",
      "[00:28:11] VentasPipeline: Advertencias: ['monto: 1 nulos']\n",
      "[00:28:11] VentasPipeline: Aplicando transformaciones...\n",
      "[00:28:11] VentasPipeline:   - Aplicada: rellenar_nulos\n",
      "[00:28:11] VentasPipeline:   - Aplicada: agregar_timestamp\n",
      "[00:28:11] VentasPipeline: Cargando a /data/processed/ventas...\n",
      "[00:28:11] VentasPipeline: Cargados 3 registros\n",
      "\n",
      "Resultado final:\n",
      "+--------+-----------+-----+----------+--------------------+\n",
      "|venta_id|producto_id|monto|     fecha|           procesado|\n",
      "+--------+-----------+-----+----------+--------------------+\n",
      "|    V001|       P001|100.0|2024-01-15|2026-02-12 00:28:...|\n",
      "|    V002|       P002|  0.0|2024-01-15|2026-02-12 00:28:...|\n",
      "|    V003|       P001|150.0|2024-01-16|2026-02-12 00:28:...|\n",
      "+--------+-----------+-----+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usar el pipeline\n",
    "\n",
    "# Datos de ejemplo\n",
    "datos_ventas = [\n",
    "    (\"V001\", \"P001\", 100.0, \"2024-01-15\"),\n",
    "    (\"V002\", \"P002\", None, \"2024-01-15\"),\n",
    "    (\"V003\", \"P001\", 150.0, \"2024-01-16\"),\n",
    "]\n",
    "\n",
    "# Transformaciones a aplicar\n",
    "transformaciones = {\n",
    "    \"rellenar_nulos\": lambda df: df.fillna({\"monto\": 0.0}),\n",
    "    \"agregar_timestamp\": lambda df: df.withColumn(\"procesado\", F.current_timestamp())\n",
    "}\n",
    "\n",
    "# Ejecutar pipeline\n",
    "pipeline = ETLPipeline(spark, \"VentasPipeline\")\n",
    "\n",
    "df = pipeline.extract(datos_ventas, [\"venta_id\", \"producto_id\", \"monto\", \"fecha\"])\n",
    "df = pipeline.validate(df, {\"monto\": \"not_null\"})\n",
    "df = pipeline.transform(df, transformaciones)\n",
    "df = pipeline.load(df, \"/data/processed/ventas\")\n",
    "\n",
    "print(\"\\nResultado final:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === EJERCICIOS PR√ÅCTICOS ===\n",
    "\n",
    "### üéØ Ejercicio ETL.1: Identificar Tipo de Pipeline\n",
    "\n",
    "Para cada escenario, indica si usar√≠as ETL o ELT y por qu√©:\n",
    "\n",
    "1. Migrar datos de Oracle a Redshift (cloud)\n",
    "2. Cargar logs de servidores a HDFS para an√°lisis ad-hoc\n",
    "3. Alimentar un dashboard de ventas diarias\n",
    "4. Construir un data lake para machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escribe tus respuestas\n",
    "\n",
    "respuestas = {\n",
    "    \"escenario_1\": \"___\",  # ETL o ELT?\n",
    "    \"escenario_2\": \"___\",\n",
    "    \"escenario_3\": \"___\",\n",
    "    \"escenario_4\": \"___\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio ETL.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuestas = {\n",
    "    \"escenario_1\": \"ELT - Redshift es un cloud DW con poder de c√≥mputo\",\n",
    "    \"escenario_2\": \"ELT - Data lake, queremos datos crudos para an√°lisis flexible\",\n",
    "    \"escenario_3\": \"ETL - Dashboard necesita datos ya procesados y resumidos\",\n",
    "    \"escenario_4\": \"ELT - ML necesita acceso a datos crudos para feature engineering\"\n",
    "}\n",
    "\n",
    "for escenario, respuesta in respuestas.items():\n",
    "    print(f\"{escenario}: {respuesta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio ETL.2: Mini Pipeline\n",
    "\n",
    "Crea un pipeline que:\n",
    "1. Extraiga datos de clientes con campos: id, email, pais\n",
    "2. Transforme: emails a min√∫sculas, pa√≠ses a may√∫sculas\n",
    "3. Valide: que no haya emails nulos\n",
    "4. Agregue timestamp de proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Crea tu pipeline\n",
    "\n",
    "datos_clientes = [\n",
    "    (1, \"ANA@Email.COM\", \"mexico\"),\n",
    "    (2, \"carlos@TEST.com\", \"colombia\"),\n",
    "    (3, None, \"argentina\"),\n",
    "]\n",
    "\n",
    "# Tu c√≥digo aqu√≠\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio ETL.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_clientes = [\n",
    "    (1, \"ANA@Email.COM\", \"mexico\"),\n",
    "    (2, \"carlos@TEST.com\", \"colombia\"),\n",
    "    (3, None, \"argentina\"),\n",
    "]\n",
    "\n",
    "# Extract\n",
    "df_clientes = spark.createDataFrame(datos_clientes, [\"id\", \"email\", \"pais\"])\n",
    "print(\"1. EXTRACT:\")\n",
    "df_clientes.show()\n",
    "\n",
    "# Validate\n",
    "nulos = df_clientes.filter(F.col(\"email\").isNull()).count()\n",
    "print(f\"2. VALIDATE: {nulos} emails nulos encontrados\")\n",
    "\n",
    "# Transform\n",
    "df_transformado = df_clientes \\\n",
    "    .filter(F.col(\"email\").isNotNull()) \\\n",
    "    .withColumn(\"email\", F.lower(F.col(\"email\"))) \\\n",
    "    .withColumn(\"pais\", F.upper(F.col(\"pais\"))) \\\n",
    "    .withColumn(\"fecha_proceso\", F.current_timestamp())\n",
    "\n",
    "print(\"3. TRANSFORM + 4. LOAD (listo):\")\n",
    "df_transformado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## Resumen\n",
    "\n",
    "### Conceptos Clave\n",
    "- **Pipeline de datos**: Flujo de Extract ‚Üí Transform ‚Üí Load\n",
    "- **ETL**: Transforma antes de cargar, tradicional\n",
    "- **ELT**: Carga primero, transforma despu√©s, moderno/cloud\n",
    "- **Componentes**: Fuentes, validaci√≥n, transformaci√≥n, carga, logging, errores\n",
    "\n",
    "### Conexi√≥n con AWS\n",
    "- **AWS Glue**: Servicio ETL serverless\n",
    "- **Glue Studio**: ETL visual (drag & drop)\n",
    "- **Glue Crawlers**: Descubren esquemas autom√°ticamente\n",
    "- **Step Functions**: Orquestar pipelines complejos\n",
    "\n",
    "### Siguiente Paso\n",
    "Contin√∫a con: `02_data_extraction.ipynb` para aprender extracci√≥n de datos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
