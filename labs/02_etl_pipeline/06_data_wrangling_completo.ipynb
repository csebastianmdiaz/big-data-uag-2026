{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Completo: De Fuentes Dispares a Data Lake\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Distinguir entre los procesos ETL y ELT, con sus trade-offs reales\n",
    "- Ejecutar cada paso de data wrangling: Discovery, Structuring, Cleaning, Enriching, Validating, Publishing\n",
    "- Integrar datos de multiples fuentes con esquemas distintos en un solo dataset\n",
    "- Simular localmente el flujo que en AWS involucra S3, Glue Crawlers y Athena\n",
    "\n",
    "## Prerequisitos\n",
    "- `00_setup/02_spark_basics.ipynb`\n",
    "- `02_etl_pipeline/01_etl_concepts.ipynb`\n",
    "\n",
    "## Tiempo Estimado\n",
    "90 minutos\n",
    "\n",
    "## Modulo AWS Academy Relacionado\n",
    "Modulo 6: Ingesting and Preparing Data\n",
    "- ETL vs ELT comparison\n",
    "- Data wrangling steps (Discovery, Structuring, Cleaning, Enriching, Validating, Publishing)\n",
    "- Scenario: Support ticket ingestion from two systems\n",
    "\n",
    "---\n",
    "\n",
    "## Escenario de Negocio\n",
    "\n",
    "Una empresa SaaS adquirio una startup. Ambas usan sistemas de tickets de soporte distintos.\n",
    "Un analista de datos pidio al equipo de ingenieria de datos que integre los tickets de ambos\n",
    "sistemas para analizar relaciones entre experiencias de soporte, volumenes de tickets y\n",
    "renovaciones de contrato.\n",
    "\n",
    "**Requisitos del analista:**\n",
    "- Tickets tecnicos trabajados o cerrados por cliente en 2020\n",
    "- Segmentacion por region de ventas\n",
    "- Cada equipo regional solo debe ver sus propios datos\n",
    "\n",
    "**Restricciones:**\n",
    "- Sistema 1 (supp1): exporta JSON con ciertos campos\n",
    "- Sistema 2 (supp2): exporta JSON con campos diferentes\n",
    "- Data warehouse existente: tiene tabla de clientes con sales_group\n",
    "\n",
    "Este es exactamente el escenario del Modulo 6 de AWS Academy. Lo implementaremos\n",
    "completo con PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark 3.5.0 listo\n",
      "Directorio de trabajo: /home/jovyan/data/m06_wrangling\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# SETUP INICIAL\n",
    "# =================================================================\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataWrangling_M06\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Directorio base para el ejercicio\n",
    "BASE_DIR = \"/home/jovyan/data/m06_wrangling\"\n",
    "os.makedirs(BASE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Spark {spark.version} listo\")\n",
    "print(f\"Directorio de trabajo: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PASO 0: Generar Datos Simulados\n",
    "\n",
    "En un entorno real, estos datos vendrian de exportaciones JSON de dos sistemas\n",
    "de tickets distintos y de una consulta al data warehouse.\n",
    "\n",
    "Aqui los generamos para que el ejercicio sea autocontenido.\n",
    "\n",
    "Nota importante: los dos sistemas tienen esquemas DISTINTOS.\n",
    "Esto es intencional y refleja la realidad cuando se adquiere una empresa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos fuente generados (formato JSONL):\n",
      "  customers.json (647 bytes)\n",
      "  supp2_tickets.json (1240 bytes)\n",
      "  supp1_tickets.json (1958 bytes)\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# Sistema de tickets 1 (supp1) - Sistema original de la empresa\n",
    "# Campos: ticket_id, requestor_id, submitter, assignee, group,\n",
    "#          subject, status, priority (texto), ticket_type,\n",
    "#          create_date, updated_date, solved_date\n",
    "# =================================================================\n",
    "supp1_data = [\n",
    "    {\"ticket_id\": 900862, \"requestor_id\": 1744899, \"submitter\": \"agent_joe\",\n",
    "     \"assignee\": \"agent_maria\", \"group\": \"tier2\", \n",
    "     \"subject\": \"Nightly batch failing\", \"status\": \"Closed\",\n",
    "     \"priority\": \"Medium\", \"ticket_type\": \"Problem\",\n",
    "     \"create_date\": \"2020-01-04 08:06:00\", \"updated_date\": \"2020-04-30 09:00:00\",\n",
    "     \"solved_date\": \"2020-04-30 09:00:00\"},\n",
    "    {\"ticket_id\": 900863, \"requestor_id\": 1744005, \"submitter\": \"agent_joe\",\n",
    "     \"assignee\": \"agent_carlos\", \"group\": \"tier1\",\n",
    "     \"subject\": \"Intermittent error message\", \"status\": \"Open\",\n",
    "     \"priority\": \"Low\", \"ticket_type\": \"Problem\",\n",
    "     \"create_date\": \"2020-01-06 11:14:00\", \"updated_date\": None,\n",
    "     \"solved_date\": None},\n",
    "    {\"ticket_id\": 900870, \"requestor_id\": 1744899, \"submitter\": \"self\",\n",
    "     \"assignee\": \"agent_maria\", \"group\": \"tier1\",\n",
    "     \"subject\": \"Cannot export PDF report\", \"status\": \"Solved\",\n",
    "     \"priority\": \"High\", \"ticket_type\": \"Problem\",\n",
    "     \"create_date\": \"2020-03-15 14:22:00\", \"updated_date\": \"2020-03-16 10:00:00\",\n",
    "     \"solved_date\": \"2020-03-16 10:00:00\"},\n",
    "    {\"ticket_id\": 900871, \"requestor_id\": 2020411, \"submitter\": \"self\",\n",
    "     \"assignee\": \"agent_joe\", \"group\": \"tier1\",\n",
    "     \"subject\": \"How to configure SSO?\", \"status\": \"Closed\",\n",
    "     \"priority\": \"Low\", \"ticket_type\": \"Question\",\n",
    "     \"create_date\": \"2020-05-20 09:30:00\", \"updated_date\": \"2020-05-21 11:00:00\",\n",
    "     \"solved_date\": \"2020-05-21 11:00:00\"},\n",
    "    {\"ticket_id\": 900872, \"requestor_id\": 1744005, \"submitter\": \"agent_carlos\",\n",
    "     \"assignee\": \"agent_carlos\", \"group\": \"tier2\",\n",
    "     \"subject\": \"Performance degradation after update\", \"status\": \"Open\",\n",
    "     \"priority\": \"High\", \"ticket_type\": \"Incident\",\n",
    "     \"create_date\": \"2020-07-10 16:45:00\", \"updated_date\": \"2020-07-12 08:00:00\",\n",
    "     \"solved_date\": None},\n",
    "    # Ticket de 2019 - NO deberia incluirse en el resultado final\n",
    "    {\"ticket_id\": 900850, \"requestor_id\": 1744899, \"submitter\": \"self\",\n",
    "     \"assignee\": \"agent_joe\", \"group\": \"tier1\",\n",
    "     \"subject\": \"Login issue\", \"status\": \"Closed\",\n",
    "     \"priority\": \"Medium\", \"ticket_type\": \"Problem\",\n",
    "     \"create_date\": \"2019-11-20 10:00:00\", \"updated_date\": \"2019-11-21 15:00:00\",\n",
    "     \"solved_date\": \"2019-11-21 15:00:00\"}\n",
    "]\n",
    "\n",
    "# =================================================================\n",
    "# Sistema de tickets 2 (supp2) - Sistema de la startup adquirida\n",
    "# Campos DISTINTOS: issue_id, cust_num, description, status,\n",
    "#                    priority (numerico 1-3), create_date,\n",
    "#                    updated_date, closed_date\n",
    "# Problemas intencionales:\n",
    "#   - cust_num faltante en un registro\n",
    "#   - caracteres basura en description\n",
    "#   - formatos de fecha inconsistentes\n",
    "#   - prioridad numerica vs texto\n",
    "# =================================================================\n",
    "supp2_data = [\n",
    "    {\"issue_id\": 900865, \"cust_num\": None, \"description\": \"Error message 808 <*% &#\",\n",
    "     \"status\": \"Closed\", \"priority\": 2,\n",
    "     \"create_date\": \"6/26/20 2:45:29\", \"updated_date\": \"6/27/20 21:31:48\",\n",
    "     \"closed_date\": \"6/27/20 23:30:09\"},\n",
    "    {\"issue_id\": 900866, \"cust_num\": 2020411, \"description\": \"Nullpointer exception on save\",\n",
    "     \"status\": \"Open\", \"priority\": 1,\n",
    "     \"create_date\": \"4/2/20 12:15\", \"updated_date\": \"4/5/20 11:15\",\n",
    "     \"closed_date\": None},\n",
    "    {\"issue_id\": 900867, \"cust_num\": 2022010, \"description\": \"Question about grades report\",\n",
    "     \"status\": \"Open\", \"priority\": 1,\n",
    "     \"create_date\": \"4/6/20 10:56\", \"updated_date\": None,\n",
    "     \"closed_date\": None},\n",
    "    {\"issue_id\": 900868, \"cust_num\": 2022010, \"description\": \"Dashboard loading slowly \\t\\n\",\n",
    "     \"status\": \"Closed\", \"priority\": 3,\n",
    "     \"create_date\": \"8/15/20 9:00\", \"updated_date\": \"8/16/20 14:30\",\n",
    "     \"closed_date\": \"8/16/20 14:30\"},\n",
    "    # Duplicado intencional del issue 900866\n",
    "    {\"issue_id\": 900866, \"cust_num\": 2020411, \"description\": \"Nullpointer exception on save\",\n",
    "     \"status\": \"Open\", \"priority\": 1,\n",
    "     \"create_date\": \"4/2/20 12:15\", \"updated_date\": \"4/5/20 11:15\",\n",
    "     \"closed_date\": None},\n",
    "    # Ticket de 2021 - NO deberia incluirse\n",
    "    {\"issue_id\": 900900, \"cust_num\": 2020411, \"description\": \"New feature request\",\n",
    "     \"status\": \"Open\", \"priority\": 3,\n",
    "     \"create_date\": \"1/15/21 8:00\", \"updated_date\": None,\n",
    "     \"closed_date\": None}\n",
    "]\n",
    "\n",
    "# =================================================================\n",
    "# Tabla de clientes del data warehouse existente\n",
    "# =================================================================\n",
    "customers_data = [\n",
    "    {\"customer_id\": 1744899, \"cust_name\": \"Acme Corp\", \"primary_poc\": \"John Smith\",\n",
    "     \"status\": \"Active\", \"sales_group\": \"Europe\"},\n",
    "    {\"customer_id\": 1744005, \"cust_name\": \"TechStart Inc\", \"primary_poc\": \"Jane Doe\",\n",
    "     \"status\": \"Active\", \"sales_group\": \"Europe\"},\n",
    "    {\"customer_id\": 2020411, \"cust_name\": \"DataFlow LLC\", \"primary_poc\": \"Carlos Ruiz\",\n",
    "     \"status\": \"Active\", \"sales_group\": \"USEast\"},\n",
    "    {\"customer_id\": 2022010, \"cust_name\": \"CloudNine SAS\", \"primary_poc\": \"Marie Dupont\",\n",
    "     \"status\": \"Active\", \"sales_group\": \"USWest\"},\n",
    "    {\"customer_id\": 2070555, \"cust_name\": \"GlobalTech GmbH\", \"primary_poc\": \"Hans Mueller\",\n",
    "     \"status\": \"Inactive\", \"sales_group\": \"Europe\"}\n",
    "]\n",
    "\n",
    "# Guardar como archivos JSON (simulando exportaciones reales)\n",
    "# Spark espera JSONL (un objeto por línea) o array JSON válido\n",
    "# Usamos JSONL que es el formato más común para Spark\n",
    "sources_dir = os.path.join(BASE_DIR, \"sources\")\n",
    "os.makedirs(sources_dir, exist_ok=True)\n",
    "\n",
    "# Guardar como JSONL (JSON Lines) - un objeto por línea\n",
    "with open(os.path.join(sources_dir, \"supp1_tickets.json\"), \"w\") as f:\n",
    "    for record in supp1_data:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "with open(os.path.join(sources_dir, \"supp2_tickets.json\"), \"w\") as f:\n",
    "    for record in supp2_data:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "with open(os.path.join(sources_dir, \"customers.json\"), \"w\") as f:\n",
    "    for record in customers_data:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "print(\"Archivos fuente generados (formato JSONL):\")\n",
    "for f in os.listdir(sources_dir):\n",
    "    size = os.path.getsize(os.path.join(sources_dir, f))\n",
    "    print(f\"  {f} ({size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PASO 1: DISCOVERY (Descubrimiento)\n",
    "\n",
    "## Que dice la presentacion AWS Academy:\n",
    "- Identificar relaciones entre fuentes\n",
    "- Identificar formatos de datos\n",
    "- Determinar que datos necesitamos y como obtenerlos\n",
    "- Determinar como organizar y controlar acceso\n",
    "- Determinar herramientas necesarias\n",
    "\n",
    "## Que hacemos en la practica:\n",
    "Cargamos los JSON, inspeccionamos esquemas, identificamos\n",
    "las diferencias entre fuentes y planificamos el mapeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISCOVERY: Esquema de supp1 (sistema original)\n",
      "============================================================\n",
      "root\n",
      " |-- assignee: string (nullable = true)\n",
      " |-- create_date: string (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- priority: string (nullable = true)\n",
      " |-- requestor_id: long (nullable = true)\n",
      " |-- solved_date: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- submitter: string (nullable = true)\n",
      " |-- ticket_id: long (nullable = true)\n",
      " |-- ticket_type: string (nullable = true)\n",
      " |-- updated_date: string (nullable = true)\n",
      "\n",
      "Registros: 6\n",
      "+------------+-------------------+-----+--------+------------+-------------------+------+--------------------------+---------+---------+-----------+-------------------+\n",
      "|assignee    |create_date        |group|priority|requestor_id|solved_date        |status|subject                   |submitter|ticket_id|ticket_type|updated_date       |\n",
      "+------------+-------------------+-----+--------+------------+-------------------+------+--------------------------+---------+---------+-----------+-------------------+\n",
      "|agent_maria |2020-01-04 08:06:00|tier2|Medium  |1744899     |2020-04-30 09:00:00|Closed|Nightly batch failing     |agent_joe|900862   |Problem    |2020-04-30 09:00:00|\n",
      "|agent_carlos|2020-01-06 11:14:00|tier1|Low     |1744005     |NULL               |Open  |Intermittent error message|agent_joe|900863   |Problem    |NULL               |\n",
      "|agent_maria |2020-03-15 14:22:00|tier1|High    |1744899     |2020-03-16 10:00:00|Solved|Cannot export PDF report  |self     |900870   |Problem    |2020-03-16 10:00:00|\n",
      "+------------+-------------------+-----+--------+------------+-------------------+------+--------------------------+---------+---------+-----------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 1.1 Cargar y examinar cada fuente\n",
    "# =================================================================\n",
    "\n",
    "# Cargar supp1 (sistema original)\n",
    "df_supp1_raw = spark.read.json(os.path.join(sources_dir, \"supp1_tickets.json\"))\n",
    "\n",
    "# Cargar supp2 (sistema de la startup)\n",
    "df_supp2_raw = spark.read.json(os.path.join(sources_dir, \"supp2_tickets.json\"))\n",
    "\n",
    "# Cargar clientes del data warehouse\n",
    "df_customers = spark.read.json(os.path.join(sources_dir, \"customers.json\"))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISCOVERY: Esquema de supp1 (sistema original)\")\n",
    "print(\"=\" * 60)\n",
    "df_supp1_raw.printSchema()\n",
    "print(f\"Registros: {df_supp1_raw.count()}\")\n",
    "df_supp1_raw.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISCOVERY: Esquema de supp2 (startup adquirida)\n",
      "============================================================\n",
      "root\n",
      " |-- closed_date: string (nullable = true)\n",
      " |-- create_date: string (nullable = true)\n",
      " |-- cust_num: long (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- issue_id: long (nullable = true)\n",
      " |-- priority: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- updated_date: string (nullable = true)\n",
      "\n",
      "Registros: 6\n",
      "+----------------+---------------+--------+-----------------------------+--------+--------+------+----------------+\n",
      "|closed_date     |create_date    |cust_num|description                  |issue_id|priority|status|updated_date    |\n",
      "+----------------+---------------+--------+-----------------------------+--------+--------+------+----------------+\n",
      "|6/27/20 23:30:09|6/26/20 2:45:29|NULL    |Error message 808 <*% &#     |900865  |2       |Closed|6/27/20 21:31:48|\n",
      "|NULL            |4/2/20 12:15   |2020411 |Nullpointer exception on save|900866  |1       |Open  |4/5/20 11:15    |\n",
      "|NULL            |4/6/20 10:56   |2022010 |Question about grades report |900867  |1       |Open  |NULL            |\n",
      "+----------------+---------------+--------+-----------------------------+--------+--------+------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DISCOVERY: Esquema de supp2 (startup adquirida)\")\n",
    "print(\"=\" * 60)\n",
    "df_supp2_raw.printSchema()\n",
    "print(f\"Registros: {df_supp2_raw.count()}\")\n",
    "df_supp2_raw.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISCOVERY: Esquema de clientes (data warehouse)\n",
      "============================================================\n",
      "root\n",
      " |-- cust_name: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- primary_poc: string (nullable = true)\n",
      " |-- sales_group: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n",
      "+---------------+-----------+------------+-----------+--------+\n",
      "|cust_name      |customer_id|primary_poc |sales_group|status  |\n",
      "+---------------+-----------+------------+-----------+--------+\n",
      "|Acme Corp      |1744899    |John Smith  |Europe     |Active  |\n",
      "|TechStart Inc  |1744005    |Jane Doe    |Europe     |Active  |\n",
      "|DataFlow LLC   |2020411    |Carlos Ruiz |USEast     |Active  |\n",
      "|CloudNine SAS  |2022010    |Marie Dupont|USWest     |Active  |\n",
      "|GlobalTech GmbH|2070555    |Hans Mueller|Europe     |Inactive|\n",
      "+---------------+-----------+------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DISCOVERY: Esquema de clientes (data warehouse)\")\n",
    "print(\"=\" * 60)\n",
    "df_customers.printSchema()\n",
    "df_customers.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DISCOVERY: Mapeo de campos identificado\n",
      "============================================================\n",
      "\n",
      "Relaciones entre campos:\n",
      "  supp1.requestor_id = supp2.cust_num = customers.customer_id\n",
      "  supp1.ticket_id ~ supp2.issue_id (campo ID unico)\n",
      "  supp1.subject ~ supp2.description (texto del ticket)\n",
      "  supp1.solved_date ~ supp2.closed_date\n",
      "\n",
      "Diferencias criticas:\n",
      "  PRIORIDAD: supp1 usa texto (High/Medium/Low)\n",
      "             supp2 usa numeros (1=High, 2=Medium, 3=Low)\n",
      "  FECHAS:    supp1 usa formato ISO (2020-01-04 08:06:00)\n",
      "             supp2 usa formato US corto (6/26/20 2:45:29)\n",
      "  TIPO:      supp1 tiene ticket_type\n",
      "             supp2 NO tiene ticket_type\n",
      "  CAMPOS EXTRA: supp1 tiene submitter, assignee, group\n",
      "                supp2 no los tiene\n",
      "\n",
      "Filtros necesarios:\n",
      "  - Solo tickets donde create_date o updated_date sea en 2020\n",
      "  - Acceso segmentado por sales_group del cliente\n",
      "\n",
      "Herramientas disponibles:\n",
      "  - PySpark para transformaciones (equivale a AWS Glue Job)\n",
      "  - Sistema de archivos local (equivale a S3)\n",
      "  - Spark SQL para queries (equivale a Athena)\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# 1.2 Analisis de diferencias entre fuentes\n",
    "# =================================================================\n",
    "# Este analisis es lo que un ingeniero de datos hace ANTES de escribir\n",
    "# codigo de transformacion. Documenta las decisiones.\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISCOVERY: Mapeo de campos identificado\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Relaciones entre campos:\")\n",
    "print(\"  supp1.requestor_id = supp2.cust_num = customers.customer_id\")\n",
    "print(\"  supp1.ticket_id ~ supp2.issue_id (campo ID unico)\")\n",
    "print(\"  supp1.subject ~ supp2.description (texto del ticket)\")\n",
    "print(\"  supp1.solved_date ~ supp2.closed_date\")\n",
    "print()\n",
    "print(\"Diferencias criticas:\")\n",
    "print(\"  PRIORIDAD: supp1 usa texto (High/Medium/Low)\")\n",
    "print(\"             supp2 usa numeros (1=High, 2=Medium, 3=Low)\")\n",
    "print(\"  FECHAS:    supp1 usa formato ISO (2020-01-04 08:06:00)\")\n",
    "print(\"             supp2 usa formato US corto (6/26/20 2:45:29)\")\n",
    "print(\"  TIPO:      supp1 tiene ticket_type\")\n",
    "print(\"             supp2 NO tiene ticket_type\")\n",
    "print(\"  CAMPOS EXTRA: supp1 tiene submitter, assignee, group\")\n",
    "print(\"                supp2 no los tiene\")\n",
    "print()\n",
    "print(\"Filtros necesarios:\")\n",
    "print(\"  - Solo tickets donde create_date o updated_date sea en 2020\")\n",
    "print(\"  - Acceso segmentado por sales_group del cliente\")\n",
    "print()\n",
    "print(\"Herramientas disponibles:\")\n",
    "print(\"  - PySpark para transformaciones (equivale a AWS Glue Job)\")\n",
    "print(\"  - Sistema de archivos local (equivale a S3)\")\n",
    "print(\"  - Spark SQL para queries (equivale a Athena)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflexion de Discovery\n",
    "\n",
    "**Pregunta para el estudiante:** Observa que supp2 no tiene campo `ticket_type`.\n",
    "Hay dos opciones: (a) clasificar manualmente cada ticket de supp2, o\n",
    "(b) dejarlo como NULL y que el analista lo clasifique despues.\n",
    "\n",
    "En un enfoque ETL, elegirias (a). En un enfoque ELT, elegirias (b).\n",
    "Para este ejercicio usaremos un enfoque hibrido: clasificaremos basandonos\n",
    "en palabras clave del campo description.\n",
    "\n",
    "---\n",
    "# PASO 2: STRUCTURING (Estructuracion)\n",
    "\n",
    "## Que dice la presentacion:\n",
    "- Parsear archivos fuente\n",
    "- Mapear campos de fuente a destino\n",
    "- Organizar almacenamiento\n",
    "- Gestionar tamano de archivos\n",
    "\n",
    "## Que hacemos:\n",
    "Renombramos campos de supp2 para que coincidan con supp1,\n",
    "convertimos la prioridad numerica a texto, y parseamos fechas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 2.1 Estructurar supp1: seleccionar campos relevantes\n",
    "# =================================================================\n",
    "# De supp1 tomamos los campos que necesitamos para el analisis.\n",
    "# Los campos submitter, assignee, group no son necesarios\n",
    "# para el requerimiento del analista.\n",
    "\n",
    "df_supp1_structured = df_supp1_raw.select(\n",
    "    F.col(\"ticket_id\").cast(\"long\"),\n",
    "    F.col(\"requestor_id\").cast(\"long\").alias(\"customer_id\"),\n",
    "    F.col(\"subject\"),\n",
    "    F.col(\"status\"),\n",
    "    F.col(\"priority\"),\n",
    "    F.col(\"ticket_type\"),\n",
    "    F.to_timestamp(\"create_date\", \"yyyy-MM-dd HH:mm:ss\").alias(\"create_date\"),\n",
    "    F.to_timestamp(\"updated_date\", \"yyyy-MM-dd HH:mm:ss\").alias(\"updated_date\"),\n",
    "    F.to_timestamp(\"solved_date\", \"yyyy-MM-dd HH:mm:ss\").alias(\"solved_date\")\n",
    ").withColumn(\"source_system\", F.lit(\"supp1\"))\n",
    "\n",
    "print(\"supp1 estructurado:\")\n",
    "df_supp1_structured.printSchema()\n",
    "df_supp1_structured.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 2.2 Estructurar supp2: renombrar campos y convertir tipos\n",
    "# =================================================================\n",
    "# Este paso implementa el MAPEO DE CAMPOS que se describe\n",
    "# en la slide 26-27 de la presentacion.\n",
    "#\n",
    "# Mapeo:\n",
    "#   issue_id    -> ticket_id\n",
    "#   cust_num    -> customer_id\n",
    "#   description -> subject\n",
    "#   closed_date -> solved_date\n",
    "#   priority 1  -> \"High\", 2 -> \"Medium\", 3 -> \"Low\"\n",
    "\n",
    "# Mapeo de prioridad numerica a texto\n",
    "priority_map = F.when(F.col(\"priority\") == 1, \"High\") \\\n",
    "    .when(F.col(\"priority\") == 2, \"Medium\") \\\n",
    "    .when(F.col(\"priority\") == 3, \"Low\") \\\n",
    "    .otherwise(\"Unknown\")\n",
    "\n",
    "# Clasificacion automatica de ticket_type basada en palabras clave\n",
    "# Esto es una decision de ingenieria: en lugar de dejar NULL,\n",
    "# inferimos el tipo a partir del contenido.\n",
    "ticket_type_inferred = F.when(\n",
    "    F.lower(F.col(\"description\")).rlike(\"question|how to|help with\"), \"Question\"\n",
    ").when(\n",
    "    F.lower(F.col(\"description\")).rlike(\"error|exception|failing|slow\"), \"Problem\"\n",
    ").otherwise(\"Problem\")  # Default conservador\n",
    "\n",
    "# Parseo de fechas con formato US corto (M/d/yy H:mm:ss o M/d/yy H:mm)\n",
    "# PySpark no maneja bien formatos ambiguos, asi que usamos coalesce\n",
    "# para probar multiples formatos.\n",
    "def parse_supp2_date(col_name):\n",
    "    return F.coalesce(\n",
    "        F.to_timestamp(F.col(col_name), \"M/d/yy H:mm:ss\"),\n",
    "        F.to_timestamp(F.col(col_name), \"M/d/yy H:mm\")\n",
    "    )\n",
    "\n",
    "df_supp2_structured = df_supp2_raw.select(\n",
    "    F.col(\"issue_id\").cast(\"long\").alias(\"ticket_id\"),\n",
    "    F.col(\"cust_num\").cast(\"long\").alias(\"customer_id\"),\n",
    "    F.col(\"description\").alias(\"subject\"),\n",
    "    F.col(\"status\"),\n",
    "    priority_map.alias(\"priority\"),\n",
    "    ticket_type_inferred.alias(\"ticket_type\"),\n",
    "    parse_supp2_date(\"create_date\").alias(\"create_date\"),\n",
    "    parse_supp2_date(\"updated_date\").alias(\"updated_date\"),\n",
    "    parse_supp2_date(\"closed_date\").alias(\"solved_date\")\n",
    ").withColumn(\"source_system\", F.lit(\"supp2\"))\n",
    "\n",
    "print(\"supp2 estructurado (con campos renombrados y tipos convertidos):\")\n",
    "df_supp2_structured.printSchema()\n",
    "df_supp2_structured.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 2.3 Verificar que ambos DataFrames tienen el mismo esquema\n",
    "# =================================================================\n",
    "# Esto es critico antes de combinarlos.\n",
    "\n",
    "print(\"Columnas supp1:\", df_supp1_structured.columns)\n",
    "print(\"Columnas supp2:\", df_supp2_structured.columns)\n",
    "print()\n",
    "\n",
    "# Verificar que los esquemas coinciden\n",
    "schemas_match = (df_supp1_structured.schema == df_supp2_structured.schema)\n",
    "print(f\"Esquemas identicos: {schemas_match}\")\n",
    "\n",
    "if not schemas_match:\n",
    "    print(\"\\nDiferencias encontradas:\")\n",
    "    for f1, f2 in zip(df_supp1_structured.schema.fields, df_supp2_structured.schema.fields):\n",
    "        if f1 != f2:\n",
    "            print(f\"  {f1.name}: {f1.dataType} vs {f2.dataType}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PASO 3: CLEANING (Limpieza)\n",
    "\n",
    "## Que dice la presentacion:\n",
    "- Eliminar columnas no deseadas, duplicados, valores basura\n",
    "- Rellenar campos obligatorios vacios\n",
    "- Validar/modificar tipos de datos\n",
    "- Identificar y corregir outliers\n",
    "\n",
    "## Que hacemos:\n",
    "Limpiamos cada fuente por separado (como recomienda la presentacion),\n",
    "porque cada fuente tiene sus propios problemas de calidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 3.1 Diagnostico de calidad de datos\n",
    "# =================================================================\n",
    "# Antes de limpiar, necesitamos saber QUE limpiar.\n",
    "\n",
    "def diagnostico_calidad(df, nombre):\n",
    "    \"\"\"Genera un reporte de calidad de datos para un DataFrame.\"\"\"\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"DIAGNOSTICO DE CALIDAD: {nombre}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"Total registros: {df.count()}\")\n",
    "    \n",
    "    # Contar nulos por columna\n",
    "    print(\"\\nNulos por columna:\")\n",
    "    nulos = df.select([\n",
    "        F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    "    for row in nulos.collect():\n",
    "        for col_name in df.columns:\n",
    "            val = row[col_name]\n",
    "            if val > 0:\n",
    "                print(f\"  {col_name}: {val} nulos\")\n",
    "    \n",
    "    # Contar duplicados por campo ID\n",
    "    id_col = \"ticket_id\"\n",
    "    total = df.count()\n",
    "    distintos = df.select(id_col).distinct().count()\n",
    "    if total != distintos:\n",
    "        print(f\"\\nDUPLICADOS: {total - distintos} registros duplicados por {id_col}\")\n",
    "    else:\n",
    "        print(f\"\\nSin duplicados por {id_col}\")\n",
    "\n",
    "diagnostico_calidad(df_supp1_structured, \"supp1\")\n",
    "diagnostico_calidad(df_supp2_structured, \"supp2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 3.2 Limpiar supp1\n",
    "# =================================================================\n",
    "# Problemas identificados:\n",
    "#   - Tiene un ticket de 2019 que no entra en el filtro de 2020\n",
    "#   - Campos solved_date y updated_date pueden ser nulos (normal)\n",
    "\n",
    "# Filtrar solo tickets de 2020 (create_date O updated_date en 2020)\n",
    "df_supp1_clean = df_supp1_structured.filter(\n",
    "    (F.year(\"create_date\") == 2020) | (F.year(\"updated_date\") == 2020)\n",
    ")\n",
    "\n",
    "print(f\"supp1 antes del filtro de fecha: {df_supp1_structured.count()} registros\")\n",
    "print(f\"supp1 despues del filtro 2020:   {df_supp1_clean.count()} registros\")\n",
    "df_supp1_clean.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 3.3 Limpiar supp2\n",
    "# =================================================================\n",
    "# Problemas identificados:\n",
    "#   - Duplicado del issue 900866\n",
    "#   - customer_id nulo en issue 900865\n",
    "#   - Caracteres basura en subject (\"<*% &#\")\n",
    "#   - Caracteres de control en subject (\"\\t\\n\")\n",
    "#   - Ticket de 2021 que no entra en el filtro\n",
    "\n",
    "import re\n",
    "\n",
    "df_supp2_clean = df_supp2_structured \\\n",
    "    .dropDuplicates([\"ticket_id\"]) \\\n",
    "    .filter(\n",
    "        (F.year(\"create_date\") == 2020) | (F.year(\"updated_date\") == 2020)\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"customer_id\",\n",
    "        F.when(F.col(\"customer_id\").isNull(), F.lit(9999999))\n",
    "         .otherwise(F.col(\"customer_id\"))\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"subject\",\n",
    "        F.regexp_replace(F.col(\"subject\"), r\"[<>*%&#\\t\\n\\r]\", \"\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"subject\",\n",
    "        F.trim(F.col(\"subject\"))\n",
    "    )\n",
    "\n",
    "print(f\"supp2 antes de limpieza: {df_supp2_structured.count()} registros\")\n",
    "print(f\"supp2 despues de limpieza: {df_supp2_clean.count()} registros\")\n",
    "print()\n",
    "print(\"Registros limpios:\")\n",
    "df_supp2_clean.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 3.4 Log de limpieza\n",
    "# =================================================================\n",
    "# En un pipeline real, registrarias estas metricas en un sistema\n",
    "# de monitoreo (CloudWatch, Datadog, etc.).\n",
    "\n",
    "print(\"RESUMEN DE LIMPIEZA:\")\n",
    "print(f\"  supp1: {df_supp1_structured.count()} -> {df_supp1_clean.count()} \"\n",
    "      f\"(eliminados: {df_supp1_structured.count() - df_supp1_clean.count()})\")\n",
    "print(f\"  supp2: {df_supp2_structured.count()} -> {df_supp2_clean.count()} \"\n",
    "      f\"(eliminados: {df_supp2_structured.count() - df_supp2_clean.count()})\")\n",
    "print(f\"  Motivos: filtro de fecha 2020, duplicados, datos fuera de rango\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PASO 4: ENRICHING (Enriquecimiento)\n",
    "\n",
    "## Que dice la presentacion:\n",
    "- Combinar datos de fuentes limpias en un solo dataset\n",
    "- Agregar valores adicionales para soportar el analisis\n",
    "\n",
    "## Que hacemos:\n",
    "Combinamos supp1 + supp2 (UNION) y enriquecemos con la region\n",
    "de ventas (JOIN con clientes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 4.1 Combinar ambas fuentes (UNION)\n",
    "# =================================================================\n",
    "# Esto equivale a \"append rows from supp1 to the bottom of supp2\"\n",
    "# como describe la slide 37.\n",
    "\n",
    "df_combined = df_supp1_clean.unionByName(df_supp2_clean)\n",
    "\n",
    "print(f\"Registros supp1: {df_supp1_clean.count()}\")\n",
    "print(f\"Registros supp2: {df_supp2_clean.count()}\")\n",
    "print(f\"Registros combinados: {df_combined.count()}\")\n",
    "print()\n",
    "df_combined.orderBy(\"ticket_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 4.2 Enriquecer con la region de ventas (JOIN con clientes)\n",
    "# =================================================================\n",
    "# La presentacion describe esto como:\n",
    "# \"query the sales system to get the sales owner by customer_id\n",
    "#  and use that to add the sales_region column\"\n",
    "\n",
    "df_enriched = df_combined.join(\n",
    "    df_customers.select(\"customer_id\", \"cust_name\", \"sales_group\"),\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\"  # LEFT JOIN porque customer_id 9999999 no existira\n",
    ").withColumnRenamed(\"sales_group\", \"sales_region\")\n",
    "\n",
    "print(\"Dataset enriquecido con region de ventas:\")\n",
    "df_enriched.select(\n",
    "    \"ticket_id\", \"customer_id\", \"cust_name\", \"subject\", \n",
    "    \"priority\", \"ticket_type\", \"sales_region\", \"source_system\"\n",
    ").orderBy(\"ticket_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PASO 5: VALIDATING (Validacion)\n",
    "\n",
    "## Que dice la presentacion:\n",
    "- Contar filas esperadas\n",
    "- Verificar consistencia\n",
    "- Verificar formatos y tipos de datos\n",
    "- Verificar duplicados (post-merge)\n",
    "- Verificar PII\n",
    "- Verificar outliers\n",
    "\n",
    "## Que hacemos:\n",
    "Ejecutamos una bateria de validaciones sobre el dataset enriquecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 5.1 Validacion de conteo de filas\n",
    "# =================================================================\n",
    "# La slide 42 muestra exactamente esta verificacion:\n",
    "# \"Count total rows and check against rows from individual systems\"\n",
    "\n",
    "count_supp1 = df_supp1_clean.count()\n",
    "count_supp2 = df_supp2_clean.count()\n",
    "count_total = df_enriched.count()\n",
    "\n",
    "print(\"VALIDACION 1: Conteo de filas\")\n",
    "print(f\"  supp1: {count_supp1}\")\n",
    "print(f\"  supp2: {count_supp2}\")\n",
    "print(f\"  Esperado: {count_supp1 + count_supp2}\")\n",
    "print(f\"  Real:     {count_total}\")\n",
    "\n",
    "if count_total == count_supp1 + count_supp2:\n",
    "    print(\"  RESULTADO: OK - Conteos coinciden\")\n",
    "else:\n",
    "    print(\"  RESULTADO: ERROR - Conteos no coinciden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 5.2 Validacion de duplicados post-merge\n",
    "# =================================================================\n",
    "# Despues de combinar, podrian existir ticket_ids duplicados\n",
    "# entre los dos sistemas (el mismo numero en sistemas distintos).\n",
    "\n",
    "print(\"VALIDACION 2: Duplicados de ticket_id post-merge\")\n",
    "duplicados = df_enriched.groupBy(\"ticket_id\") \\\n",
    "    .agg(F.count(\"*\").alias(\"apariciones\")) \\\n",
    "    .filter(F.col(\"apariciones\") > 1)\n",
    "\n",
    "num_dups = duplicados.count()\n",
    "if num_dups > 0:\n",
    "    print(f\"  ADVERTENCIA: {num_dups} ticket_ids duplicados entre sistemas\")\n",
    "    duplicados.show()\n",
    "    # Mostrar detalle de los duplicados\n",
    "    dup_ids = [row.ticket_id for row in duplicados.collect()]\n",
    "    df_enriched.filter(F.col(\"ticket_id\").isin(dup_ids)) \\\n",
    "        .select(\"ticket_id\", \"source_system\", \"subject\", \"customer_id\") \\\n",
    "        .show(truncate=False)\n",
    "    print(\"  NOTA: Son tickets de DIFERENTES sistemas, no son duplicados reales.\")\n",
    "    print(\"  Accion: No requiere correccion si source_system es distinto.\")\n",
    "else:\n",
    "    print(\"  OK - Sin duplicados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 5.3 Validacion de nulos en campos criticos\n",
    "# =================================================================\n",
    "print(\"VALIDACION 3: Nulos en campos criticos\")\n",
    "campos_criticos = [\"ticket_id\", \"customer_id\", \"subject\", \"status\", \n",
    "                   \"priority\", \"create_date\"]\n",
    "\n",
    "for campo in campos_criticos:\n",
    "    nulos = df_enriched.filter(F.col(campo).isNull()).count()\n",
    "    estado = \"OK\" if nulos == 0 else f\"ADVERTENCIA: {nulos} nulos\"\n",
    "    print(f\"  {campo}: {estado}\")\n",
    "\n",
    "# Verificar registros sin region (customer_id placeholder)\n",
    "sin_region = df_enriched.filter(F.col(\"sales_region\").isNull()).count()\n",
    "print(f\"  sales_region: {'OK' if sin_region == 0 else f'ADVERTENCIA: {sin_region} sin region'}\")\n",
    "\n",
    "if sin_region > 0:\n",
    "    print(\"  Detalle de registros sin region:\")\n",
    "    df_enriched.filter(F.col(\"sales_region\").isNull()) \\\n",
    "        .select(\"ticket_id\", \"customer_id\", \"subject\") \\\n",
    "        .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 5.4 Validacion de consistencia de valores\n",
    "# =================================================================\n",
    "print(\"VALIDACION 4: Consistencia de valores categoricos\")\n",
    "print()\n",
    "print(\"Valores de priority:\")\n",
    "df_enriched.groupBy(\"priority\").count().orderBy(\"priority\").show()\n",
    "\n",
    "print(\"Valores de status:\")\n",
    "df_enriched.groupBy(\"status\").count().orderBy(\"status\").show()\n",
    "\n",
    "print(\"Valores de ticket_type:\")\n",
    "df_enriched.groupBy(\"ticket_type\").count().orderBy(\"ticket_type\").show()\n",
    "\n",
    "print(\"Distribucion por source_system:\")\n",
    "df_enriched.groupBy(\"source_system\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 5.5 Correccion post-validacion\n",
    "# =================================================================\n",
    "# Basandonos en las validaciones, asignamos \"Unknown\" a sales_region\n",
    "# para registros sin region (customer_id placeholder).\n",
    "\n",
    "df_validated = df_enriched.withColumn(\n",
    "    \"sales_region\",\n",
    "    F.coalesce(F.col(\"sales_region\"), F.lit(\"Unassigned\"))\n",
    ").withColumn(\n",
    "    \"cust_name\",\n",
    "    F.coalesce(F.col(\"cust_name\"), F.lit(\"Unknown Customer\"))\n",
    ")\n",
    "\n",
    "# Verificar que ya no hay nulos en sales_region\n",
    "sin_region = df_validated.filter(F.col(\"sales_region\").isNull()).count()\n",
    "print(f\"Registros sin region despues de correccion: {sin_region}\")\n",
    "print()\n",
    "print(\"Dataset validado final:\")\n",
    "df_validated.select(\n",
    "    \"ticket_id\", \"customer_id\", \"cust_name\", \"subject\",\n",
    "    \"status\", \"priority\", \"ticket_type\", \"sales_region\", \"source_system\"\n",
    ").orderBy(\"sales_region\", \"ticket_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PASO 6: PUBLISHING (Publicacion)\n",
    "\n",
    "## Que dice la presentacion:\n",
    "- Mover datos a almacenamiento permanente\n",
    "- Aplicar tecnicas de gestion de archivos (formatos, compresion, organizacion)\n",
    "- Aplicar controles de acceso\n",
    "- Guardar metadatos\n",
    "- Configurar frecuencia de actualizacion\n",
    "\n",
    "## Que hacemos:\n",
    "Particionamos por sales_region (equivale a crear carpetas por region en S3),\n",
    "guardamos en formato Parquet con compresion Snappy, y registramos como\n",
    "tabla SQL para consultar con Spark SQL (equivale a Glue Crawler + Athena).\n",
    "\n",
    "## Equivalencia con AWS:\n",
    "| Accion local | Equivalente AWS |\n",
    "|---|---|\n",
    "| Escribir Parquet particionado | S3 con prefijos por region |\n",
    "| Compresion Snappy | Configuracion de formato en S3 |\n",
    "| Registro como tabla SQL | AWS Glue Crawler + Data Catalog |\n",
    "| Consultas con Spark SQL | Amazon Athena |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 6.1 Publicar datos particionados por region\n",
    "# =================================================================\n",
    "# Esto es equivalente a lo que la presentacion describe en la slide 47:\n",
    "# \"group the dataset by sales_region and export each region's rows\"\n",
    "# \"upload each file to the bucket or folder associated with the region\"\n",
    "\n",
    "# Seleccionar columnas finales (sin campos internos)\n",
    "df_final = df_validated.select(\n",
    "    \"ticket_id\", \"customer_id\", \"cust_name\", \"subject\",\n",
    "    \"status\", \"priority\", \"ticket_type\",\n",
    "    \"create_date\", \"updated_date\", \"solved_date\",\n",
    "    \"sales_region\", \"source_system\"\n",
    ")\n",
    "\n",
    "# Ruta de publicacion (equivale a s3://cs-tickets-2020/)\n",
    "publish_path = os.path.join(BASE_DIR, \"published\", \"cs-tickets-2020\")\n",
    "\n",
    "# Limpiar si existe de una ejecucion anterior\n",
    "if os.path.exists(publish_path):\n",
    "    shutil.rmtree(publish_path)\n",
    "\n",
    "# Escribir particionado por sales_region en formato Parquet\n",
    "# Esto crea carpetas: sales_region=Europe/, sales_region=USEast/, etc.\n",
    "df_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"sales_region\") \\\n",
    "    .parquet(publish_path)\n",
    "\n",
    "print(\"Datos publicados en:\")\n",
    "print(f\"  {publish_path}\")\n",
    "print()\n",
    "\n",
    "# Mostrar estructura de carpetas creada\n",
    "print(\"Estructura de carpetas (equivale a prefijos de S3):\")\n",
    "for root, dirs, files in os.walk(publish_path):\n",
    "    level = root.replace(publish_path, '').count(os.sep)\n",
    "    indent = '  ' * level\n",
    "    folder_name = os.path.basename(root)\n",
    "    if files:\n",
    "        total_size = sum(os.path.getsize(os.path.join(root, f)) for f in files)\n",
    "        print(f\"{indent}{folder_name}/ ({len(files)} archivo(s), {total_size:,} bytes)\")\n",
    "    else:\n",
    "        print(f\"{indent}{folder_name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 6.2 Registrar como tabla SQL (equivale a Glue Crawler + Catalog)\n",
    "# =================================================================\n",
    "# En AWS, el Glue Crawler escanea los archivos en S3, infiere el\n",
    "# esquema y lo registra en el Data Catalog. Athena usa ese catalogo\n",
    "# para ejecutar queries SQL directamente sobre los archivos.\n",
    "#\n",
    "# Localmente, hacemos lo equivalente con Spark SQL.\n",
    "\n",
    "df_published = spark.read.parquet(publish_path)\n",
    "df_published.createOrReplaceTempView(\"cs_tickets_2020\")\n",
    "\n",
    "print(\"Tabla 'cs_tickets_2020' registrada para SQL\")\n",
    "print()\n",
    "print(\"Esquema (equivale a lo que Glue Crawler descubriria):\")\n",
    "df_published.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# 6.3 Consultas SQL (equivale a Amazon Athena)\n",
    "# =================================================================\n",
    "# Estas son las consultas que el analista de datos ejecutaria\n",
    "# usando Athena sobre los datos en S3.\n",
    "\n",
    "# Consulta 1: Resumen por region\n",
    "print(\"CONSULTA 1: Tickets por region y prioridad\")\n",
    "print(\"(Equivalente Athena: SELECT ... FROM cs_tickets_2020 WHERE ...)\")\n",
    "print()\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        sales_region,\n",
    "        priority,\n",
    "        COUNT(*) AS num_tickets,\n",
    "        SUM(CASE WHEN status IN ('Closed', 'Solved') THEN 1 ELSE 0 END) AS resueltos,\n",
    "        SUM(CASE WHEN status = 'Open' THEN 1 ELSE 0 END) AS abiertos\n",
    "    FROM cs_tickets_2020\n",
    "    GROUP BY sales_region, priority\n",
    "    ORDER BY sales_region, priority\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta 2: Tickets por cliente (lo que pidio el analista)\n",
    "print(\"CONSULTA 2: Tickets por cliente con tipos de issues\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        cust_name,\n",
    "        sales_region,\n",
    "        COUNT(*) AS total_tickets,\n",
    "        SUM(CASE WHEN ticket_type = 'Problem' THEN 1 ELSE 0 END) AS problems,\n",
    "        SUM(CASE WHEN ticket_type = 'Question' THEN 1 ELSE 0 END) AS questions,\n",
    "        SUM(CASE WHEN ticket_type = 'Incident' THEN 1 ELSE 0 END) AS incidents\n",
    "    FROM cs_tickets_2020\n",
    "    GROUP BY customer_id, cust_name, sales_region\n",
    "    ORDER BY total_tickets DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta 3: Simular acceso por region\n",
    "# En AWS, esto se lograria con bucket policies + IAM groups.\n",
    "# Aqui simulamos el filtro que aplicaria cada equipo regional.\n",
    "\n",
    "region = \"Europe\"  # Cambiar a \"USEast\" o \"USWest\" para probar\n",
    "\n",
    "print(f\"CONSULTA 3: Vista del equipo de ventas '{region}'\")\n",
    "print(f\"(En AWS: solo verian s3://cs-tickets-2020/{region.lower()}/)\")\n",
    "print()\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT ticket_id, cust_name, subject, status, priority, ticket_type\n",
    "    FROM cs_tickets_2020\n",
    "    WHERE sales_region = '{region}'\n",
    "    ORDER BY create_date DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EJERCICIOS PRACTICOS\n",
    "\n",
    "### Ejercicio 1: Pregunta tipo examen (slide 54-56 de la presentacion)\n",
    "\n",
    "Un ingeniero de datos debe proporcionar datos para un nuevo reporte de ventas.\n",
    "El reporte combinara datos de ventas de 4 productos distintos, rastreados en\n",
    "4 sistemas diferentes. Los datos resultantes deben segmentarse por region,\n",
    "y cada dataset regional solo debe estar disponible para los equipos de ventas\n",
    "de esa region.\n",
    "\n",
    "Que deberia hacer PRIMERO el ingeniero de datos?\n",
    "\n",
    "A) Crear un bucket S3 con carpetas para cada producto  \n",
    "B) Exportar datos de cada sistema y combinarlos en un solo archivo  \n",
    "C) Identificar las relaciones entre campos de cada fuente  \n",
    "D) Reemplazar valores nulos en cada sistema con ceros  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tu respuesta y justificacion:\n",
    "respuesta = \"___\"  # A, B, C o D\n",
    "justificacion = \"___\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUCION:\n",
    "print(\"Respuesta correcta: C\")\n",
    "print()\n",
    "print(\"Justificacion:\")\n",
    "print(\"C es correcto porque identificar relaciones entre campos\")\n",
    "print(\"es una tarea de DISCOVERY, que es el PRIMER paso del\")\n",
    "print(\"proceso de data wrangling.\")\n",
    "print()\n",
    "print(\"Por que las otras son incorrectas:\")\n",
    "print(\"A) Crear el bucket seria STRUCTURING/PUBLISHING (paso posterior).\")\n",
    "print(\"   Ademas, las carpetas deberian ser por REGION, no por producto.\")\n",
    "print(\"B) Exportar y combinar seria ENRICHING, pero sin discovery\")\n",
    "print(\"   no sabes como mapear los campos entre sistemas.\")\n",
    "print(\"D) Reemplazar nulos seria CLEANING, pero sin discovery\")\n",
    "print(\"   no sabes si reemplazar con ceros es apropiado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Agregar una nueva fuente\n",
    "\n",
    "Imagina que llega un tercer sistema de tickets (supp3) con el siguiente esquema:\n",
    "\n",
    "```\n",
    "case_number (int), account_id (int), title (string),\n",
    "case_status (string), urgency (string: \"critical\"/\"normal\"/\"minor\"),\n",
    "opened_at (string: \"YYYY/MM/DD\"), resolved_at (string: \"YYYY/MM/DD\")\n",
    "```\n",
    "\n",
    "Escribe el codigo PySpark para:\n",
    "1. Mapear los campos al esquema unificado\n",
    "2. Convertir urgency a la escala High/Medium/Low\n",
    "3. Parsear las fechas\n",
    "4. Combinarlo con el dataset existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de ejemplo de supp3\n",
    "supp3_data = [\n",
    "    (500001, 2020411, \"API rate limiting issue\", \"Resolved\", \"critical\", \"2020/03/10\", \"2020/03/12\"),\n",
    "    (500002, 1744899, \"Billing question\", \"Open\", \"minor\", \"2020/06/01\", None),\n",
    "    (500003, 2022010, \"Integration failing after update\", \"Resolved\", \"normal\", \"2020/09/15\", \"2020/09/17\"),\n",
    "]\n",
    "\n",
    "df_supp3_raw = spark.createDataFrame(\n",
    "    supp3_data,\n",
    "    [\"case_number\", \"account_id\", \"title\", \"case_status\", \n",
    "     \"urgency\", \"opened_at\", \"resolved_at\"]\n",
    ")\n",
    "\n",
    "# TODO: Tu codigo aqui\n",
    "# 1. Mapear campos al esquema unificado\n",
    "# 2. Convertir urgency: critical->High, normal->Medium, minor->Low\n",
    "# 3. Parsear fechas con formato YYYY/MM/DD\n",
    "# 4. Combinar con df_validated usando unionByName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUCION Ejercicio 2\n",
    "\n",
    "# Mapeo de urgency a priority\n",
    "urgency_map = F.when(F.col(\"urgency\") == \"critical\", \"High\") \\\n",
    "    .when(F.col(\"urgency\") == \"normal\", \"Medium\") \\\n",
    "    .when(F.col(\"urgency\") == \"minor\", \"Low\") \\\n",
    "    .otherwise(\"Unknown\")\n",
    "\n",
    "# Inferir ticket_type\n",
    "type_inferred = F.when(\n",
    "    F.lower(F.col(\"title\")).rlike(\"question|billing|how\"), \"Question\"\n",
    ").otherwise(\"Problem\")\n",
    "\n",
    "# Mapeo de status: supp3 usa \"Resolved\" en lugar de \"Solved\" o \"Closed\"\n",
    "status_map = F.when(F.col(\"case_status\") == \"Resolved\", \"Closed\") \\\n",
    "    .otherwise(F.col(\"case_status\"))\n",
    "\n",
    "df_supp3_structured = df_supp3_raw.select(\n",
    "    F.col(\"case_number\").cast(\"long\").alias(\"ticket_id\"),\n",
    "    F.col(\"account_id\").cast(\"long\").alias(\"customer_id\"),\n",
    "    F.col(\"title\").alias(\"subject\"),\n",
    "    status_map.alias(\"status\"),\n",
    "    urgency_map.alias(\"priority\"),\n",
    "    type_inferred.alias(\"ticket_type\"),\n",
    "    F.to_timestamp(\"opened_at\", \"yyyy/MM/dd\").alias(\"create_date\"),\n",
    "    F.lit(None).cast(\"timestamp\").alias(\"updated_date\"),\n",
    "    F.to_timestamp(\"resolved_at\", \"yyyy/MM/dd\").alias(\"solved_date\")\n",
    ").withColumn(\"source_system\", F.lit(\"supp3\"))\n",
    "\n",
    "# Enriquecer con clientes\n",
    "df_supp3_enriched = df_supp3_structured.join(\n",
    "    df_customers.select(\"customer_id\", \"cust_name\", \"sales_group\"),\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\"\n",
    ").withColumnRenamed(\"sales_group\", \"sales_region\")\n",
    "\n",
    "print(\"supp3 estructurado y enriquecido:\")\n",
    "df_supp3_enriched.select(\n",
    "    \"ticket_id\", \"customer_id\", \"cust_name\", \"subject\",\n",
    "    \"priority\", \"ticket_type\", \"sales_region\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Combinar con el dataset existente\n",
    "# Nota: necesitamos asegurar que las columnas coincidan\n",
    "df_all = df_validated.unionByName(df_supp3_enriched)\n",
    "print(f\"\\nTotal de tickets combinados (3 fuentes): {df_all.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 3: Reflexion ETL vs ELT\n",
    "\n",
    "Analiza el pipeline que acabamos de construir y responde:\n",
    "\n",
    "1. Este pipeline sigue un enfoque ETL o ELT? Justifica.\n",
    "2. Que partes moveriamos a un enfoque ELT si quisieras mayor flexibilidad?\n",
    "3. Que ventajas tendria cargar primero los datos crudos al data lake?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe tus respuestas:\n",
    "respuesta_1 = \"___\"\n",
    "respuesta_2 = \"___\"\n",
    "respuesta_3 = \"___\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUCION Ejercicio 3\n",
    "\n",
    "print(\"1. ENFOQUE DEL PIPELINE\")\n",
    "print(\"   Este pipeline sigue un enfoque ETL: transformamos ANTES\")\n",
    "print(\"   de cargar al almacenamiento final. Toda la limpieza,\")\n",
    "print(\"   estructuracion y enriquecimiento ocurre antes de escribir\")\n",
    "print(\"   los archivos Parquet particionados.\")\n",
    "print()\n",
    "print(\"2. CONVERSION A ELT\")\n",
    "print(\"   En un enfoque ELT:\")\n",
    "print(\"   - Cargariamos los JSON crudos directamente a S3 (raw zone)\")\n",
    "print(\"   - Las transformaciones se harian con Athena o Glue Jobs\")\n",
    "print(\"     sobre los datos ya almacenados\")\n",
    "print(\"   - El analista tendria acceso a los datos crudos\")\n",
    "print(\"     para hacer sus propias exploraciones\")\n",
    "print()\n",
    "print(\"3. VENTAJAS DE CARGAR DATOS CRUDOS\")\n",
    "print(\"   - Si el analista cambia de opinion sobre que campos\")\n",
    "print(\"     necesita, los datos originales estan disponibles\")\n",
    "print(\"   - Cambios en las transformaciones se aplican a datos\")\n",
    "print(\"     historicos (no se pierden los campos truncados)\")\n",
    "print(\"   - Otros equipos pueden usar los mismos datos crudos\")\n",
    "print(\"     para otros propositos sin depender del ingeniero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# RESUMEN FINAL\n",
    "\n",
    "## Pasos de Data Wrangling ejecutados\n",
    "\n",
    "| Paso | Actividad Local | Equivalente AWS |\n",
    "|------|----------------|------------------|\n",
    "| Discovery | Cargar JSON, comparar esquemas, documentar mapeos | Consultar fuentes, analizar con Glue DataBrew |\n",
    "| Structuring | Renombrar campos, convertir tipos, parsear fechas | Glue Job con DynamicFrame mappings |\n",
    "| Cleaning | Eliminar duplicados, rellenar nulos, limpiar texto | Glue Job transforms, DataBrew recipes |\n",
    "| Enriching | UNION de fuentes + LEFT JOIN con clientes | Glue Job con joins, Athena CTAS |\n",
    "| Validating | Conteos, consistencia, verificacion de nulos | Glue Data Quality, Great Expectations |\n",
    "| Publishing | Parquet particionado + registro SQL | S3 + Glue Crawler + Data Catalog + Athena |\n",
    "\n",
    "## Conexion directa con el Modulo 6 de AWS Academy\n",
    "- El escenario de tickets de soporte es identico al de la presentacion\n",
    "- Los 6 pasos de wrangling se implementaron en codigo ejecutable\n",
    "- Las consultas SQL finales simulan lo que Athena haria sobre S3\n",
    "- El particionamiento por region simula los prefijos S3 con bucket policies\n",
    "\n",
    "## Siguiente Paso\n",
    "- Modulo 7 (AWS Academy): Ingesting by Batch or by Stream\n",
    "- Lab asociado: Querying Data by Using Athena\n",
    "- En nuestros labs: `03_batch_processing/` y `04_streaming_simulation/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza\n",
    "if os.path.exists(BASE_DIR):\n",
    "    shutil.rmtree(BASE_DIR)\n",
    "    print(\"Archivos temporales eliminados\")\n",
    "\n",
    "spark.catalog.dropTempView(\"cs_tickets_2020\")\n",
    "print(\"Tablas temporales eliminadas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
