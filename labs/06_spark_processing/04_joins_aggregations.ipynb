{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins y Agregaciones Avanzadas\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Dominar todos los tipos de joins en Spark\n",
    "- Aplicar agregaciones complejas con m√∫ltiples niveles\n",
    "- Usar pivot y unpivot para reestructurar datos\n",
    "- Optimizar joins para mejor rendimiento\n",
    "\n",
    "## Prerequisitos\n",
    "- `06_spark_processing/03_spark_sql.ipynb`\n",
    "\n",
    "## Tiempo Estimado\n",
    "‚è±Ô∏è 60 minutos\n",
    "\n",
    "## M√≥dulo AWS Academy Relacionado\n",
    "üìö M√≥dulo 9: Big Data Processing - Operaciones de datos complejas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JoinsAggregations\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark listo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrames de ejemplo mas completos\n",
    "# Empleados\n",
    "empleados = spark.createDataFrame([\n",
    "    (1, \"Ana\", \"Ventas\", 45000, date(2020, 3, 15)),\n",
    "    (2, \"Carlos\", \"IT\", 55000, date(2019, 7, 1)),\n",
    "    (3, \"Maria\", \"Ventas\", 48000, date(2021, 1, 10)),\n",
    "    (4, \"Juan\", \"IT\", 52000, date(2020, 6, 20)),\n",
    "    (5, \"Laura\", \"Marketing\", 42000, date(2022, 2, 1)),\n",
    "    (6, \"Pedro\", \"IT\", 60000, date(2018, 11, 5)),\n",
    "    (7, \"Sofia\", \"Ventas\", 47000, date(2021, 8, 15)),\n",
    "    (8, \"Diego\", None, 38000, date(2023, 1, 1))  # Sin departamento\n",
    "], [\"id\", \"nombre\", \"departamento\", \"salario\", \"fecha_ingreso\"])\n",
    "\n",
    "# Departamentos\n",
    "departamentos = spark.createDataFrame([\n",
    "    (\"Ventas\", \"Piso 1\", 100000),\n",
    "    (\"IT\", \"Piso 2\", 200000),\n",
    "    (\"Marketing\", \"Piso 1\", 80000),\n",
    "    (\"RRHH\", \"Piso 3\", 60000)  # Sin empleados\n",
    "], [\"nombre_depto\", \"ubicacion\", \"presupuesto\"])\n",
    "\n",
    "# Proyectos\n",
    "proyectos = spark.createDataFrame([\n",
    "    (101, \"Web App\", \"IT\", date(2024, 1, 1), date(2024, 6, 30)),\n",
    "    (102, \"Campa√±a Q1\", \"Marketing\", date(2024, 1, 1), date(2024, 3, 31)),\n",
    "    (103, \"CRM\", \"Ventas\", date(2024, 2, 1), date(2024, 12, 31)),\n",
    "    (104, \"Mobile App\", \"IT\", date(2024, 3, 1), date(2024, 9, 30))\n",
    "], [\"proyecto_id\", \"nombre_proyecto\", \"departamento\", \"inicio\", \"fin\"])\n",
    "\n",
    "# Asignaciones empleado-proyecto\n",
    "asignaciones = spark.createDataFrame([\n",
    "    (1, 103, 20),  # Ana en CRM, 20 horas/semana\n",
    "    (2, 101, 40),  # Carlos en Web App\n",
    "    (3, 103, 30),  # Maria en CRM\n",
    "    (4, 101, 20),  # Juan en Web App\n",
    "    (4, 104, 20),  # Juan tambien en Mobile\n",
    "    (5, 102, 40),  # Laura en Campa√±a\n",
    "    (6, 104, 40),  # Pedro en Mobile\n",
    "    (7, 103, 25)   # Sofia en CRM\n",
    "], [\"empleado_id\", \"proyecto_id\", \"horas_semana\"])\n",
    "\n",
    "print(\"DataFrames creados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 1 ===\n",
    "## 1. Tipos de Joins\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Un **JOIN** combina filas de dos tablas bas√°ndose en una condici√≥n (generalmente igualdad de columnas).\n",
    "\n",
    "**Tipos:**\n",
    "- `inner`: Solo coincidencias en ambas tablas\n",
    "- `left` / `left_outer`: Todo de la izquierda + coincidencias\n",
    "- `right` / `right_outer`: Todo de la derecha + coincidencias\n",
    "- `full` / `full_outer`: Todo de ambas tablas\n",
    "- `cross`: Producto cartesiano (cada fila con cada fila)\n",
    "- `left_semi`: Filas de izquierda que tienen match (sin columnas de derecha)\n",
    "- `left_anti`: Filas de izquierda que NO tienen match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN: Solo empleados con departamento valido\n",
    "print(\"INNER JOIN - Empleados con departamento:\")\n",
    "empleados.join(\n",
    "    departamentos,\n",
    "    empleados[\"departamento\"] == departamentos[\"nombre_depto\"],\n",
    "    \"inner\"\n",
    ").select(\"nombre\", \"departamento\", \"ubicacion\", \"salario\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT JOIN: Todos los empleados, con o sin departamento\n",
    "print(\"LEFT JOIN - Todos los empleados:\")\n",
    "empleados.join(\n",
    "    departamentos,\n",
    "    empleados[\"departamento\"] == departamentos[\"nombre_depto\"],\n",
    "    \"left\"\n",
    ").select(\"nombre\", \"departamento\", \"ubicacion\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIGHT JOIN: Todos los departamentos, con o sin empleados\n",
    "print(\"RIGHT JOIN - Todos los departamentos:\")\n",
    "empleados.join(\n",
    "    departamentos,\n",
    "    empleados[\"departamento\"] == departamentos[\"nombre_depto\"],\n",
    "    \"right\"\n",
    ").select(\"nombre_depto\", \"nombre\", \"salario\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT SEMI: Empleados que SI estan en algun proyecto\n",
    "print(\"LEFT SEMI - Empleados asignados a proyectos:\")\n",
    "empleados.join(\n",
    "    asignaciones,\n",
    "    empleados[\"id\"] == asignaciones[\"empleado_id\"],\n",
    "    \"left_semi\"  # Solo retorna columnas de la tabla izquierda\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT ANTI: Empleados que NO estan en ningun proyecto\n",
    "print(\"LEFT ANTI - Empleados sin proyectos:\")\n",
    "empleados.join(\n",
    "    asignaciones,\n",
    "    empleados[\"id\"] == asignaciones[\"empleado_id\"],\n",
    "    \"left_anti\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 2 ===\n",
    "## 2. Joins M√∫ltiples\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Frecuentemente necesitamos unir m√°s de dos tablas para obtener una vista completa de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join de 4 tablas: Vista completa de asignaciones\n",
    "vista_completa = asignaciones \\\n",
    "    .join(empleados, asignaciones[\"empleado_id\"] == empleados[\"id\"]) \\\n",
    "    .join(proyectos, asignaciones[\"proyecto_id\"] == proyectos[\"proyecto_id\"]) \\\n",
    "    .join(\n",
    "        departamentos, \n",
    "        empleados[\"departamento\"] == departamentos[\"nombre_depto\"],\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        empleados[\"nombre\"].alias(\"empleado\"),\n",
    "        empleados[\"departamento\"],\n",
    "        proyectos[\"nombre_proyecto\"].alias(\"proyecto\"),\n",
    "        asignaciones[\"horas_semana\"],\n",
    "        departamentos[\"ubicacion\"]\n",
    "    )\n",
    "\n",
    "print(\"Vista completa de asignaciones:\")\n",
    "vista_completa.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 3 ===\n",
    "## 3. Agregaciones Avanzadas\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Las agregaciones resumen datos. Spark permite agregaciones m√∫ltiples, condicionales y anidadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregaciones multiples por grupo\n",
    "print(\"Estadisticas por departamento:\")\n",
    "empleados.filter(F.col(\"departamento\").isNotNull()) \\\n",
    "    .groupBy(\"departamento\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_empleados\"),\n",
    "        F.sum(\"salario\").alias(\"salario_total\"),\n",
    "        F.round(F.avg(\"salario\"), 2).alias(\"salario_promedio\"),\n",
    "        F.min(\"salario\").alias(\"salario_min\"),\n",
    "        F.max(\"salario\").alias(\"salario_max\"),\n",
    "        F.round(F.stddev(\"salario\"), 2).alias(\"desviacion_std\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"num_empleados\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregacion condicional con CASE/WHEN\n",
    "print(\"Conteo condicional:\")\n",
    "empleados.agg(\n",
    "    F.count(\"*\").alias(\"total_empleados\"),\n",
    "    F.sum(F.when(F.col(\"salario\") > 50000, 1).otherwise(0)).alias(\"salario_alto\"),\n",
    "    F.sum(F.when(F.col(\"salario\") <= 50000, 1).otherwise(0)).alias(\"salario_normal\"),\n",
    "    F.sum(F.when(F.year(F.col(\"fecha_ingreso\")) >= 2022, 1).otherwise(0)).alias(\"nuevos_2022+\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect_list y collect_set: Agregar valores en listas\n",
    "print(\"Empleados por departamento (lista):\")\n",
    "empleados.filter(F.col(\"departamento\").isNotNull()) \\\n",
    "    .groupBy(\"departamento\") \\\n",
    "    .agg(\n",
    "        F.collect_list(\"nombre\").alias(\"empleados\"),\n",
    "        F.collect_set(\"nombre\").alias(\"empleados_unicos\")  # Sin duplicados\n",
    "    ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 4 ===\n",
    "## 4. Pivot y Unpivot\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "**Pivot** transforma filas en columnas (de formato largo a ancho).\n",
    "**Unpivot** hace lo contrario (de ancho a largo).\n",
    "\n",
    "**Analog√≠a:** Es como reorganizar una tabla de Excel. Pivot es cuando conviertes categor√≠as de una columna en m√∫ltiples columnas separadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de ventas mensuales para pivot\n",
    "ventas_mensuales = spark.createDataFrame([\n",
    "    (\"Norte\", \"Enero\", 15000),\n",
    "    (\"Norte\", \"Febrero\", 18000),\n",
    "    (\"Norte\", \"Marzo\", 20000),\n",
    "    (\"Sur\", \"Enero\", 12000),\n",
    "    (\"Sur\", \"Febrero\", 14000),\n",
    "    (\"Sur\", \"Marzo\", 16000),\n",
    "    (\"Centro\", \"Enero\", 22000),\n",
    "    (\"Centro\", \"Febrero\", 25000),\n",
    "    (\"Centro\", \"Marzo\", 28000)\n",
    "], [\"region\", \"mes\", \"ventas\"])\n",
    "\n",
    "print(\"Datos originales (formato largo):\")\n",
    "ventas_mensuales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIVOT: Convertir meses a columnas\n",
    "print(\"Despues de PIVOT (formato ancho):\")\n",
    "ventas_pivot = ventas_mensuales \\\n",
    "    .groupBy(\"region\") \\\n",
    "    .pivot(\"mes\", [\"Enero\", \"Febrero\", \"Marzo\"]) \\\n",
    "    .sum(\"ventas\")\n",
    "\n",
    "ventas_pivot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNPIVOT: Volver al formato largo\n",
    "# Spark no tiene unpivot directo, se usa stack()\n",
    "print(\"UNPIVOT (volver a formato largo):\")\n",
    "ventas_unpivot = ventas_pivot.select(\n",
    "    \"region\",\n",
    "    F.expr(\"stack(3, 'Enero', Enero, 'Febrero', Febrero, 'Marzo', Marzo) as (mes, ventas)\")\n",
    ")\n",
    "\n",
    "ventas_unpivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 5 ===\n",
    "## 5. Agregaciones con Rollup y Cube\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "- **ROLLUP**: Crea subtotales jer√°rquicos (de m√°s detalle a menos)\n",
    "- **CUBE**: Crea todas las combinaciones posibles de subtotales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos para demostrar rollup/cube\n",
    "ventas_detalle = spark.createDataFrame([\n",
    "    (\"Norte\", \"Electronica\", 2024, 150000),\n",
    "    (\"Norte\", \"Muebles\", 2024, 80000),\n",
    "    (\"Sur\", \"Electronica\", 2024, 120000),\n",
    "    (\"Sur\", \"Muebles\", 2024, 60000),\n",
    "    (\"Norte\", \"Electronica\", 2023, 140000),\n",
    "    (\"Norte\", \"Muebles\", 2023, 75000),\n",
    "    (\"Sur\", \"Electronica\", 2023, 110000),\n",
    "    (\"Sur\", \"Muebles\", 2023, 55000)\n",
    "], [\"region\", \"categoria\", \"anio\", \"ventas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROLLUP: Subtotales jerarquicos\n",
    "print(\"ROLLUP - Subtotales por region -> categoria:\")\n",
    "ventas_detalle.rollup(\"region\", \"categoria\") \\\n",
    "    .agg(F.sum(\"ventas\").alias(\"total_ventas\")) \\\n",
    "    .orderBy(\"region\", \"categoria\") \\\n",
    "    .show()\n",
    "\n",
    "# Las filas con NULL son subtotales\n",
    "# NULL, NULL = gran total\n",
    "# region, NULL = total por region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUBE: Todas las combinaciones\n",
    "print(\"CUBE - Todas las combinaciones de subtotales:\")\n",
    "ventas_detalle.cube(\"region\", \"categoria\") \\\n",
    "    .agg(F.sum(\"ventas\").alias(\"total_ventas\")) \\\n",
    "    .orderBy(\"region\", \"categoria\") \\\n",
    "    .show()\n",
    "\n",
    "# Incluye totales por cada dimension individualmente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === EJERCICIOS PR√ÅCTICOS ===\n",
    "\n",
    "### üéØ Ejercicio J.1: An√°lisis de Proyectos\n",
    "\n",
    "Crea un reporte que muestre para cada proyecto:\n",
    "- Nombre del proyecto\n",
    "- Departamento responsable\n",
    "- N√∫mero de empleados asignados\n",
    "- Total de horas semanales\n",
    "- Costo semanal (horas √ó salario/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa el ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio J.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte_proyectos = proyectos \\\n",
    "    .join(asignaciones, \"proyecto_id\") \\\n",
    "    .join(empleados, asignaciones[\"empleado_id\"] == empleados[\"id\"]) \\\n",
    "    .groupBy(\n",
    "        proyectos[\"proyecto_id\"],\n",
    "        proyectos[\"nombre_proyecto\"],\n",
    "        proyectos[\"departamento\"]\n",
    "    ) \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_empleados\"),\n",
    "        F.sum(\"horas_semana\").alias(\"total_horas\"),\n",
    "        F.round(\n",
    "            F.sum(F.col(\"horas_semana\") * F.col(\"salario\") / 40 / 52),\n",
    "            2\n",
    "        ).alias(\"costo_semanal\")\n",
    "    ) \\\n",
    "    .orderBy(F.desc(\"costo_semanal\"))\n",
    "\n",
    "print(\"Reporte de proyectos:\")\n",
    "reporte_proyectos.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio J.2: Matriz de Asignaci√≥n\n",
    "\n",
    "Crea una matriz (pivot) que muestre:\n",
    "- Filas: Nombres de empleados\n",
    "- Columnas: Nombres de proyectos\n",
    "- Valores: Horas semanales (0 si no est√° asignado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa el ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio J.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener lista de proyectos para el pivot\n",
    "lista_proyectos = [row.nombre_proyecto for row in proyectos.select(\"nombre_proyecto\").collect()]\n",
    "\n",
    "matriz = empleados \\\n",
    "    .join(asignaciones, empleados[\"id\"] == asignaciones[\"empleado_id\"], \"left\") \\\n",
    "    .join(proyectos, \"proyecto_id\", \"left\") \\\n",
    "    .groupBy(empleados[\"nombre\"].alias(\"empleado\")) \\\n",
    "    .pivot(\"nombre_proyecto\", lista_proyectos) \\\n",
    "    .agg(F.coalesce(F.sum(\"horas_semana\"), F.lit(0))) \\\n",
    "    .fillna(0)\n",
    "\n",
    "print(\"Matriz de asignacion:\")\n",
    "matriz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio J.3: Reporte con Rollup\n",
    "\n",
    "Usando los datos de empleados:\n",
    "1. Crea un reporte con ROLLUP por departamento y a√±o de ingreso\n",
    "2. Muestra conteo y salario promedio\n",
    "3. A√±ade una columna que indique si es subtotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa el ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio J.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte_rollup = empleados \\\n",
    "    .filter(F.col(\"departamento\").isNotNull()) \\\n",
    "    .withColumn(\"anio_ingreso\", F.year(F.col(\"fecha_ingreso\"))) \\\n",
    "    .rollup(\"departamento\", \"anio_ingreso\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_empleados\"),\n",
    "        F.round(F.avg(\"salario\"), 2).alias(\"salario_promedio\")\n",
    "    ) \\\n",
    "    .withColumn(\"tipo_fila\",\n",
    "        F.when(F.col(\"departamento\").isNull(), \"GRAN TOTAL\")\n",
    "        .when(F.col(\"anio_ingreso\").isNull(), \"SUBTOTAL DEPTO\")\n",
    "        .otherwise(\"DETALLE\")\n",
    "    ) \\\n",
    "    .orderBy(\"departamento\", \"anio_ingreso\")\n",
    "\n",
    "print(\"Reporte con ROLLUP:\")\n",
    "reporte_rollup.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## Resumen\n",
    "\n",
    "### Conceptos Clave\n",
    "- **Joins**: `inner`, `left`, `right`, `full`, `semi`, `anti` para combinar datos\n",
    "- **Agregaciones**: `groupBy` + `agg` con m√∫ltiples funciones\n",
    "- **Pivot/Unpivot**: Reestructurar entre formato largo y ancho\n",
    "- **Rollup/Cube**: Subtotales jer√°rquicos y multidimensionales\n",
    "- **collect_list/set**: Agregar valores en arrays\n",
    "\n",
    "### Conexi√≥n con AWS\n",
    "- **Athena**: Soporta joins y agregaciones SQL similares\n",
    "- **Redshift**: Data warehouse con estas operaciones optimizadas\n",
    "- **Glue**: ETL con estas transformaciones\n",
    "\n",
    "### Siguiente Paso\n",
    "Contin√∫a con: `05_window_functions.ipynb` para funciones de ventana avanzadas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
