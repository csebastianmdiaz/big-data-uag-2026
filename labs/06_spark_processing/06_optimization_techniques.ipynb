{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T√©cnicas de Optimizaci√≥n en Spark\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Entender c√≥mo Spark ejecuta los trabajos\n",
    "- Identificar y resolver cuellos de botella\n",
    "- Aplicar t√©cnicas de particionamiento y caching\n",
    "- Optimizar joins y agregaciones\n",
    "\n",
    "## Prerequisitos\n",
    "- Todos los notebooks anteriores del m√≥dulo 06\n",
    "\n",
    "## Tiempo Estimado\n",
    "‚è±Ô∏è 60 minutos\n",
    "\n",
    "## M√≥dulo AWS Academy Relacionado\n",
    "üìö M√≥dulo 9: Optimizaci√≥n de clusters EMR y jobs Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkOptimization\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 1 ===\n",
    "## 1. Modelo de Ejecuci√≥n de Spark\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Spark divide el trabajo en:\n",
    "- **Job**: Conjunto de tareas disparadas por una acci√≥n\n",
    "- **Stage**: Conjunto de tareas que pueden ejecutarse en paralelo (sin shuffle)\n",
    "- **Task**: Unidad m√≠nima de trabajo en una partici√≥n\n",
    "\n",
    "**Shuffle**: Redistribuci√≥n de datos entre nodos. Es costoso porque implica escritura a disco y transferencia de red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos de ejemplo\n",
    "datos = [(i, f\"producto_{i % 100}\", i * 10.5, i % 10) \n",
    "         for i in range(100000)]\n",
    "\n",
    "df = spark.createDataFrame(datos, [\"id\", \"producto\", \"precio\", \"categoria\"])\n",
    "\n",
    "print(f\"Registros: {df.count():,}\")\n",
    "print(f\"Particiones iniciales: {df.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el plan de ejecucion\n",
    "# explain() muestra como Spark procesara la consulta\n",
    "\n",
    "resultado = df.filter(F.col(\"categoria\") == 5) \\\n",
    "    .groupBy(\"producto\") \\\n",
    "    .agg(F.sum(\"precio\").alias(\"total\"))\n",
    "\n",
    "print(\"Plan de ejecucion:\")\n",
    "resultado.explain()\n",
    "\n",
    "# explain(True) muestra mas detalles\n",
    "print(\"\\nPlan detallado:\")\n",
    "resultado.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 2 ===\n",
    "## 2. Particionamiento\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "El **particionamiento** determina c√≥mo se distribuyen los datos. Un buen particionamiento:\n",
    "- Balancea la carga entre workers\n",
    "- Minimiza shuffles\n",
    "- Aprovecha el paralelismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition: Cambia el numero de particiones (causa shuffle)\n",
    "df_8_particiones = df.repartition(8)\n",
    "print(f\"Despues de repartition(8): {df_8_particiones.rdd.getNumPartitions()} particiones\")\n",
    "\n",
    "# Coalesce: Reduce particiones SIN shuffle (mas eficiente)\n",
    "df_4_particiones = df_8_particiones.coalesce(4)\n",
    "print(f\"Despues de coalesce(4): {df_4_particiones.rdd.getNumPartitions()} particiones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repartition por columna: Agrupa datos por clave\n",
    "# Util antes de joins o groupBy\n",
    "\n",
    "df_por_categoria = df.repartition(10, \"categoria\")\n",
    "print(f\"Particiones por categoria: {df_por_categoria.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Ver distribucion de datos en particiones\n",
    "distribucion = df_por_categoria.groupBy(F.spark_partition_id().alias(\"particion\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(\"particion\")\n",
    "\n",
    "print(\"\\nDistribucion de registros por particion:\")\n",
    "distribucion.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 3 ===\n",
    "## 3. Caching y Persistencia\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "**Cache** guarda un DataFrame en memoria para reutilizarlo sin recalcular.\n",
    "- `cache()`: Guarda en memoria\n",
    "- `persist()`: Permite elegir nivel de almacenamiento\n",
    "- `unpersist()`: Libera la memoria\n",
    "\n",
    "**Cu√°ndo usar cache:**\n",
    "- Cuando un DataFrame se usa m√∫ltiples veces\n",
    "- Despu√©s de transformaciones costosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Crear DataFrame con transformaciones\n",
    "df_transformado = df.filter(F.col(\"precio\") > 100) \\\n",
    "    .withColumn(\"precio_iva\", F.col(\"precio\") * 1.16)\n",
    "\n",
    "# Sin cache: Cada accion recalcula\n",
    "start = time.time()\n",
    "count1 = df_transformado.count()\n",
    "count2 = df_transformado.filter(F.col(\"categoria\") == 3).count()\n",
    "tiempo_sin_cache = time.time() - start\n",
    "\n",
    "# Con cache: Segunda accion usa datos en memoria\n",
    "df_transformado.cache()\n",
    "start = time.time()\n",
    "count1 = df_transformado.count()  # Esto llena el cache\n",
    "count2 = df_transformado.filter(F.col(\"categoria\") == 3).count()  # Usa cache\n",
    "tiempo_con_cache = time.time() - start\n",
    "\n",
    "print(f\"Sin cache: {tiempo_sin_cache:.3f}s\")\n",
    "print(f\"Con cache: {tiempo_con_cache:.3f}s\")\n",
    "\n",
    "# Liberar memoria\n",
    "df_transformado.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Niveles de persistencia\n",
    "print(\"Niveles de StorageLevel disponibles:\")\n",
    "print(\"  MEMORY_ONLY: Solo memoria (default de cache)\")\n",
    "print(\"  MEMORY_AND_DISK: Memoria, overflow a disco\")\n",
    "print(\"  DISK_ONLY: Solo disco\")\n",
    "print(\"  MEMORY_ONLY_SER: Serializado en memoria (menos RAM)\")\n",
    "\n",
    "# Ejemplo con persist\n",
    "df_persistido = df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_persistido.count()  # Materializa\n",
    "df_persistido.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 4 ===\n",
    "## 4. Optimizaci√≥n de Joins\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Los **joins** son operaciones costosas. Estrategias de optimizaci√≥n:\n",
    "- **Broadcast Join**: Env√≠a la tabla peque√±a a todos los workers\n",
    "- **Sort-Merge Join**: Para tablas grandes ordenadas por clave\n",
    "- **Bucket Join**: Pre-particiona datos por clave de join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla grande y peque√±a\n",
    "df_grande = spark.range(1000000).withColumn(\"valor\", F.rand())\n",
    "df_pequena = spark.createDataFrame(\n",
    "    [(i, f\"cat_{i}\") for i in range(100)],\n",
    "    [\"id\", \"nombre\"]\n",
    ")\n",
    "\n",
    "print(f\"Tabla grande: {df_grande.count():,} filas\")\n",
    "print(f\"Tabla peque√±a: {df_pequena.count()} filas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast Join: Para tablas pequenas (< 10MB por default)\n",
    "# F.broadcast() fuerza broadcast de una tabla\n",
    "\n",
    "join_broadcast = df_grande.join(\n",
    "    F.broadcast(df_pequena),\n",
    "    df_grande[\"id\"] % 100 == df_pequena[\"id\"]\n",
    ")\n",
    "\n",
    "print(\"Plan con Broadcast:\")\n",
    "join_broadcast.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar tiempos\n",
    "# Join sin broadcast\n",
    "start = time.time()\n",
    "df_grande.join(df_pequena, df_grande[\"id\"] % 100 == df_pequena[\"id\"]).count()\n",
    "tiempo_normal = time.time() - start\n",
    "\n",
    "# Join con broadcast\n",
    "start = time.time()\n",
    "df_grande.join(F.broadcast(df_pequena), df_grande[\"id\"] % 100 == df_pequena[\"id\"]).count()\n",
    "tiempo_broadcast = time.time() - start\n",
    "\n",
    "print(f\"Join normal: {tiempo_normal:.3f}s\")\n",
    "print(f\"Broadcast join: {tiempo_broadcast:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 5 ===\n",
    "## 5. Evitar Anti-Patrones\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Algunos patrones de c√≥digo causan problemas de rendimiento:\n",
    "- **UDFs en Python**: M√°s lentas que funciones nativas\n",
    "- **collect() en datos grandes**: Puede colapsar el driver\n",
    "- **Shuffles innecesarios**: Evitar repartition sin necesidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAL: UDF de Python (lento)\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def calcular_iva_udf(precio):\n",
    "    return precio * 1.16 if precio else None\n",
    "\n",
    "# BIEN: Funcion nativa de Spark (rapido)\n",
    "def calcular_iva_nativo(col):\n",
    "    return col * 1.16\n",
    "\n",
    "# Comparar tiempos\n",
    "start = time.time()\n",
    "df.withColumn(\"iva_udf\", calcular_iva_udf(\"precio\")).count()\n",
    "tiempo_udf = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "df.withColumn(\"iva_nativo\", calcular_iva_nativo(F.col(\"precio\"))).count()\n",
    "tiempo_nativo = time.time() - start\n",
    "\n",
    "print(f\"UDF Python: {tiempo_udf:.3f}s\")\n",
    "print(f\"Funcion nativa: {tiempo_nativo:.3f}s\")\n",
    "print(f\"Nativo es {tiempo_udf/tiempo_nativo:.1f}x mas rapido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buenas practicas\n",
    "print(\"BUENAS PR√ÅCTICAS:\")\n",
    "print()\n",
    "print(\"1. Filtrar temprano: Reduce datos antes de joins/aggregations\")\n",
    "print(\"2. Seleccionar columnas necesarias: No arrastrar columnas innecesarias\")\n",
    "print(\"3. Usar funciones nativas: Evitar UDFs cuando sea posible\")\n",
    "print(\"4. Cache con cuidado: Solo si se reutiliza el DataFrame\")\n",
    "print(\"5. Broadcast tablas peque√±as: < 10MB en joins\")\n",
    "print(\"6. Evitar collect(): Usar take(), show() o write()\")\n",
    "print(\"7. Particionar inteligentemente: Por columnas de join/groupBy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === EJERCICIOS PR√ÅCTICOS ===\n",
    "\n",
    "### üéØ Ejercicio O.1: Optimizar Consulta\n",
    "\n",
    "Optimiza esta consulta ineficiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta INEFICIENTE - Optimizala\n",
    "def consulta_lenta():\n",
    "    return df \\\n",
    "        .repartition(100) \\\n",
    "        .select(\"*\") \\\n",
    "        .filter(F.col(\"categoria\") == 5) \\\n",
    "        .groupBy(\"producto\") \\\n",
    "        .agg(F.collect_list(\"precio\").alias(\"precios\"))\n",
    "\n",
    "# TODO: Escribe version optimizada\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio O.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta_optimizada():\n",
    "    return df \\\n",
    "        .filter(F.col(\"categoria\") == 5) \\\n",
    "        .select(\"producto\", \"precio\") \\\n",
    "        .groupBy(\"producto\") \\\n",
    "        .agg(\n",
    "            F.sum(\"precio\").alias(\"total\"),\n",
    "            F.count(\"*\").alias(\"count\")\n",
    "        )\n",
    "\n",
    "# Comparar\n",
    "start = time.time()\n",
    "consulta_lenta().count()\n",
    "t1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "consulta_optimizada().count()\n",
    "t2 = time.time() - start\n",
    "\n",
    "print(f\"Lenta: {t1:.3f}s\")\n",
    "print(f\"Optimizada: {t2:.3f}s\")\n",
    "\n",
    "# Mejoras:\n",
    "# 1. Filtrar ANTES de repartition\n",
    "# 2. Seleccionar solo columnas necesarias\n",
    "# 3. No repartition innecesario\n",
    "# 4. Usar sum/count en lugar de collect_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## Resumen\n",
    "\n",
    "### Conceptos Clave\n",
    "- **Particionamiento**: `repartition()` vs `coalesce()`, particionar por clave\n",
    "- **Caching**: `cache()`, `persist()`, `unpersist()` para reutilizar datos\n",
    "- **Broadcast Join**: Para tablas peque√±as\n",
    "- **Evitar**: UDFs, collect() en datos grandes, shuffles innecesarios\n",
    "- **Mejores pr√°cticas**: Filtrar temprano, seleccionar columnas, usar funciones nativas\n",
    "\n",
    "### Conexi√≥n con AWS\n",
    "- **EMR**: Configuraci√≥n de clusters para optimizar Spark\n",
    "- **Glue**: Tiene optimizaciones autom√°ticas (pushdown, partitioning)\n",
    "- **S3**: Particionamiento de datos en S3 mejora lecturas\n",
    "\n",
    "### Siguiente Paso\n",
    "¬°Felicidades! Has completado el m√≥dulo de Spark Processing. Contin√∫a con:\n",
    "- `07_ml_data_preparation/` para preparaci√≥n de datos ML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
