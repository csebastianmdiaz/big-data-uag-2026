{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Ejecutar consultas SQL sobre DataFrames\n",
    "- Usar tablas temporales y vistas\n",
    "- Aplicar funciones SQL avanzadas\n",
    "- Combinar SQL con la API de DataFrames\n",
    "\n",
    "## Prerequisitos\n",
    "- `06_spark_processing/02_dataframes_api.ipynb`\n",
    "- Conocimientos bÃ¡sicos de SQL\n",
    "\n",
    "## Tiempo Estimado\n",
    "â±ï¸ 45 minutos\n",
    "\n",
    "## MÃ³dulo AWS Academy Relacionado\n",
    "ðŸ“š MÃ³dulo 9 y 11: Spark SQL es similar a Amazon Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import date\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark SQL {spark.version} listo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCIÃ“N 1 ===\n",
    "## 1. Crear Datos de Ejemplo\n",
    "\n",
    "### ExplicaciÃ³n Conceptual\n",
    "Crearemos tablas de ejemplo que simulan un sistema de e-commerce para practicar SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla de productos\n",
    "productos_data = [\n",
    "    (1, \"Laptop HP\", \"Electronica\", 15000.00),\n",
    "    (2, \"Mouse Logitech\", \"Electronica\", 350.00),\n",
    "    (3, \"Monitor Dell\", \"Electronica\", 8000.00),\n",
    "    (4, \"Silla Ergonomica\", \"Muebles\", 4500.00),\n",
    "    (5, \"Escritorio\", \"Muebles\", 6000.00),\n",
    "    (6, \"Teclado Mecanico\", \"Electronica\", 1200.00),\n",
    "    (7, \"Webcam HD\", \"Electronica\", 800.00),\n",
    "    (8, \"Lampara LED\", \"Muebles\", 450.00)\n",
    "]\n",
    "df_productos = spark.createDataFrame(\n",
    "    productos_data, \n",
    "    [\"producto_id\", \"nombre\", \"categoria\", \"precio\"]\n",
    ")\n",
    "\n",
    "# Tabla de clientes\n",
    "clientes_data = [\n",
    "    (\"C001\", \"Ana Garcia\", \"CDMX\", \"Premium\"),\n",
    "    (\"C002\", \"Carlos Lopez\", \"Guadalajara\", \"Regular\"),\n",
    "    (\"C003\", \"Maria Rodriguez\", \"Monterrey\", \"Premium\"),\n",
    "    (\"C004\", \"Juan Martinez\", \"CDMX\", \"Regular\"),\n",
    "    (\"C005\", \"Laura Sanchez\", \"Puebla\", \"Premium\")\n",
    "]\n",
    "df_clientes = spark.createDataFrame(\n",
    "    clientes_data,\n",
    "    [\"cliente_id\", \"nombre\", \"ciudad\", \"tipo\"]\n",
    ")\n",
    "\n",
    "# Tabla de ventas\n",
    "ventas_data = [\n",
    "    (\"V001\", \"C001\", 1, 2, date(2024, 1, 15)),\n",
    "    (\"V002\", \"C001\", 2, 3, date(2024, 1, 15)),\n",
    "    (\"V003\", \"C002\", 3, 1, date(2024, 1, 16)),\n",
    "    (\"V004\", \"C003\", 1, 1, date(2024, 1, 17)),\n",
    "    (\"V005\", \"C002\", 6, 2, date(2024, 1, 17)),\n",
    "    (\"V006\", \"C004\", 4, 1, date(2024, 1, 18)),\n",
    "    (\"V007\", \"C001\", 7, 2, date(2024, 1, 18)),\n",
    "    (\"V008\", \"C005\", 5, 1, date(2024, 1, 19)),\n",
    "    (\"V009\", \"C003\", 2, 5, date(2024, 1, 19)),\n",
    "    (\"V010\", \"C002\", 8, 3, date(2024, 1, 20))\n",
    "]\n",
    "df_ventas = spark.createDataFrame(\n",
    "    ventas_data,\n",
    "    [\"venta_id\", \"cliente_id\", \"producto_id\", \"cantidad\", \"fecha\"]\n",
    ")\n",
    "\n",
    "print(\"Tablas creadas:\")\n",
    "print(f\"  Productos: {df_productos.count()} filas\")\n",
    "print(f\"  Clientes: {df_clientes.count()} filas\")\n",
    "print(f\"  Ventas: {df_ventas.count()} filas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar DataFrames como tablas SQL temporales\n",
    "# createOrReplaceTempView() crea una vista accesible via SQL\n",
    "\n",
    "df_productos.createOrReplaceTempView(\"productos\")\n",
    "df_clientes.createOrReplaceTempView(\"clientes\")\n",
    "df_ventas.createOrReplaceTempView(\"ventas\")\n",
    "\n",
    "print(\"Tablas registradas para SQL:\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCIÃ“N 2 ===\n",
    "## 2. Consultas SELECT BÃ¡sicas\n",
    "\n",
    "### ExplicaciÃ³n Conceptual\n",
    "Spark SQL soporta SQL estÃ¡ndar ANSI. Puedes usar `spark.sql()` para ejecutar cualquier consulta SQL y obtener un DataFrame como resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT basico\n",
    "print(\"Todos los productos:\")\n",
    "spark.sql(\"SELECT * FROM productos\").show()\n",
    "\n",
    "# SELECT con columnas especificas\n",
    "print(\"Solo nombre y precio:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT nombre, precio \n",
    "    FROM productos\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHERE para filtrar\n",
    "print(\"Productos de Electronica con precio > 1000:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT nombre, categoria, precio\n",
    "    FROM productos\n",
    "    WHERE categoria = 'Electronica'\n",
    "      AND precio > 1000\n",
    "    ORDER BY precio DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas calculadas y alias\n",
    "print(\"Productos con precio con IVA:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        nombre,\n",
    "        precio AS precio_sin_iva,\n",
    "        precio * 1.16 AS precio_con_iva,\n",
    "        UPPER(categoria) AS categoria_mayusculas\n",
    "    FROM productos\n",
    "    ORDER BY precio DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCIÃ“N 3 ===\n",
    "## 3. Agregaciones con GROUP BY\n",
    "\n",
    "### ExplicaciÃ³n Conceptual\n",
    "GROUP BY agrupa filas y permite aplicar funciones de agregaciÃ³n como SUM, COUNT, AVG, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregaciones basicas\n",
    "print(\"Resumen de productos por categoria:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        categoria,\n",
    "        COUNT(*) AS num_productos,\n",
    "        ROUND(AVG(precio), 2) AS precio_promedio,\n",
    "        MIN(precio) AS precio_minimo,\n",
    "        MAX(precio) AS precio_maximo,\n",
    "        SUM(precio) AS suma_precios\n",
    "    FROM productos\n",
    "    GROUP BY categoria\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventas por cliente\n",
    "print(\"Total de ventas por cliente:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cliente_id,\n",
    "        COUNT(*) AS num_compras,\n",
    "        SUM(cantidad) AS unidades_totales\n",
    "    FROM ventas\n",
    "    GROUP BY cliente_id\n",
    "    ORDER BY num_compras DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAVING para filtrar despues de agrupar\n",
    "print(\"Clientes con mas de 2 compras:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cliente_id,\n",
    "        COUNT(*) AS num_compras\n",
    "    FROM ventas\n",
    "    GROUP BY cliente_id\n",
    "    HAVING COUNT(*) > 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCIÃ“N 4 ===\n",
    "## 4. JOINs entre Tablas\n",
    "\n",
    "### ExplicaciÃ³n Conceptual\n",
    "Los JOINs combinan datos de mÃºltiples tablas basÃ¡ndose en columnas relacionadas.\n",
    "\n",
    "**Tipos de JOIN:**\n",
    "- **INNER JOIN**: Solo filas que coinciden en ambas tablas\n",
    "- **LEFT JOIN**: Todas las filas de la izquierda + coincidencias\n",
    "- **RIGHT JOIN**: Todas las filas de la derecha + coincidencias\n",
    "- **FULL OUTER JOIN**: Todas las filas de ambas tablas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN: Ventas con informacion de productos\n",
    "print(\"Detalle de ventas con productos:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        v.venta_id,\n",
    "        p.nombre AS producto,\n",
    "        v.cantidad,\n",
    "        p.precio,\n",
    "        v.cantidad * p.precio AS total\n",
    "    FROM ventas v\n",
    "    INNER JOIN productos p ON v.producto_id = p.producto_id\n",
    "    ORDER BY total DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN de 3 tablas: Ventas completas\n",
    "print(\"Reporte completo de ventas:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        v.venta_id,\n",
    "        v.fecha,\n",
    "        c.nombre AS cliente,\n",
    "        c.ciudad,\n",
    "        p.nombre AS producto,\n",
    "        p.categoria,\n",
    "        v.cantidad,\n",
    "        p.precio,\n",
    "        v.cantidad * p.precio AS total\n",
    "    FROM ventas v\n",
    "    JOIN clientes c ON v.cliente_id = c.cliente_id\n",
    "    JOIN productos p ON v.producto_id = p.producto_id\n",
    "    ORDER BY v.fecha, v.venta_id\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT JOIN: Todos los clientes, incluso sin ventas\n",
    "# Primero agregamos un cliente sin ventas\n",
    "df_clientes_extra = spark.createDataFrame(\n",
    "    [(\"C006\", \"Pedro Nuevo\", \"Tijuana\", \"Regular\")],\n",
    "    [\"cliente_id\", \"nombre\", \"ciudad\", \"tipo\"]\n",
    ")\n",
    "df_clientes.union(df_clientes_extra).createOrReplaceTempView(\"clientes\")\n",
    "\n",
    "print(\"Todos los clientes con sus ventas (incluso sin ventas):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.cliente_id,\n",
    "        c.nombre,\n",
    "        COUNT(v.venta_id) AS num_ventas,\n",
    "        COALESCE(SUM(v.cantidad), 0) AS unidades_totales\n",
    "    FROM clientes c\n",
    "    LEFT JOIN ventas v ON c.cliente_id = v.cliente_id\n",
    "    GROUP BY c.cliente_id, c.nombre\n",
    "    ORDER BY num_ventas DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCIÃ“N 5 ===\n",
    "## 5. Subconsultas y CTEs\n",
    "\n",
    "### ExplicaciÃ³n Conceptual\n",
    "Las **subconsultas** son consultas dentro de otras consultas. Los **CTEs (Common Table Expressions)** con `WITH` hacen las consultas mÃ¡s legibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subconsulta en WHERE\n",
    "print(\"Productos con precio mayor al promedio:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT nombre, precio\n",
    "    FROM productos\n",
    "    WHERE precio > (SELECT AVG(precio) FROM productos)\n",
    "    ORDER BY precio DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subconsulta en FROM (tabla derivada)\n",
    "print(\"Categoria con mayor promedio de precio:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM (\n",
    "        SELECT \n",
    "            categoria,\n",
    "            ROUND(AVG(precio), 2) AS precio_promedio,\n",
    "            COUNT(*) AS num_productos\n",
    "        FROM productos\n",
    "        GROUP BY categoria\n",
    "    ) subquery\n",
    "    WHERE precio_promedio > 2000\n",
    "    ORDER BY precio_promedio DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTE (WITH) - mas legible para consultas complejas\n",
    "print(\"Analisis de clientes con CTE:\")\n",
    "spark.sql(\"\"\"\n",
    "    WITH ventas_cliente AS (\n",
    "        SELECT \n",
    "            cliente_id,\n",
    "            COUNT(*) AS num_compras,\n",
    "            SUM(cantidad) AS unidades\n",
    "        FROM ventas\n",
    "        GROUP BY cliente_id\n",
    "    ),\n",
    "    promedio_compras AS (\n",
    "        SELECT AVG(num_compras) AS promedio\n",
    "        FROM ventas_cliente\n",
    "    )\n",
    "    SELECT \n",
    "        c.nombre,\n",
    "        c.tipo,\n",
    "        vc.num_compras,\n",
    "        vc.unidades,\n",
    "        CASE \n",
    "            WHEN vc.num_compras > p.promedio THEN 'Por encima del promedio'\n",
    "            ELSE 'Por debajo del promedio'\n",
    "        END AS clasificacion\n",
    "    FROM ventas_cliente vc\n",
    "    JOIN clientes c ON vc.cliente_id = c.cliente_id\n",
    "    CROSS JOIN promedio_compras p\n",
    "    ORDER BY vc.num_compras DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCIÃ“N 6 ===\n",
    "## 6. Funciones de Ventana (Window Functions)\n",
    "\n",
    "### ExplicaciÃ³n Conceptual\n",
    "Las funciones de ventana calculan valores sobre un conjunto de filas relacionadas, sin agrupar (no reducen el nÃºmero de filas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROW_NUMBER: Numerar filas dentro de grupos\n",
    "print(\"Productos numerados por categoria (por precio):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        categoria,\n",
    "        nombre,\n",
    "        precio,\n",
    "        ROW_NUMBER() OVER (PARTITION BY categoria ORDER BY precio DESC) AS ranking\n",
    "    FROM productos\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANK y DENSE_RANK\n",
    "print(\"Comparacion de rankings:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        nombre,\n",
    "        precio,\n",
    "        ROW_NUMBER() OVER (ORDER BY precio DESC) AS row_num,\n",
    "        RANK() OVER (ORDER BY precio DESC) AS rank,\n",
    "        DENSE_RANK() OVER (ORDER BY precio DESC) AS dense_rank\n",
    "    FROM productos\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acumulados y promedios moviles\n",
    "print(\"Ventas con acumulado por fecha:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        fecha,\n",
    "        venta_id,\n",
    "        cantidad,\n",
    "        SUM(cantidad) OVER (ORDER BY fecha ROWS UNBOUNDED PRECEDING) AS cantidad_acumulada,\n",
    "        AVG(cantidad) OVER (ORDER BY fecha ROWS BETWEEN 2 PRECEDING AND CURRENT ROW) AS promedio_movil_3\n",
    "    FROM ventas\n",
    "    ORDER BY fecha, venta_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAG y LEAD: Valores anteriores y siguientes\n",
    "print(\"Comparar con venta anterior:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        venta_id,\n",
    "        fecha,\n",
    "        cantidad,\n",
    "        LAG(cantidad, 1) OVER (ORDER BY fecha, venta_id) AS cantidad_anterior,\n",
    "        LEAD(cantidad, 1) OVER (ORDER BY fecha, venta_id) AS cantidad_siguiente,\n",
    "        cantidad - LAG(cantidad, 1) OVER (ORDER BY fecha, venta_id) AS diferencia\n",
    "    FROM ventas\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === EJERCICIOS PRÃCTICOS ===\n",
    "\n",
    "### ðŸŽ¯ Ejercicio SQL.1: Reporte de Ventas\n",
    "\n",
    "Crea un reporte que muestre:\n",
    "- Fecha\n",
    "- Total de ventas del dÃ­a (suma de cantidad Ã— precio)\n",
    "- NÃºmero de transacciones\n",
    "- Producto mÃ¡s vendido del dÃ­a\n",
    "\n",
    "Ordena por fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escribe tu consulta SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… SoluciÃ³n Ejercicio SQL.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH ventas_detalle AS (\n",
    "        SELECT \n",
    "            v.fecha,\n",
    "            v.venta_id,\n",
    "            p.nombre AS producto,\n",
    "            v.cantidad,\n",
    "            v.cantidad * p.precio AS total_venta\n",
    "        FROM ventas v\n",
    "        JOIN productos p ON v.producto_id = p.producto_id\n",
    "    ),\n",
    "    producto_top AS (\n",
    "        SELECT \n",
    "            fecha,\n",
    "            producto,\n",
    "            SUM(cantidad) AS unidades,\n",
    "            ROW_NUMBER() OVER (PARTITION BY fecha ORDER BY SUM(cantidad) DESC) AS rn\n",
    "        FROM ventas_detalle\n",
    "        GROUP BY fecha, producto\n",
    "    )\n",
    "    SELECT \n",
    "        vd.fecha,\n",
    "        ROUND(SUM(vd.total_venta), 2) AS venta_total_dia,\n",
    "        COUNT(DISTINCT vd.venta_id) AS num_transacciones,\n",
    "        MAX(pt.producto) AS producto_mas_vendido\n",
    "    FROM ventas_detalle vd\n",
    "    JOIN producto_top pt ON vd.fecha = pt.fecha AND pt.rn = 1\n",
    "    GROUP BY vd.fecha, pt.producto\n",
    "    ORDER BY vd.fecha\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Ejercicio SQL.2: Ranking de Clientes\n",
    "\n",
    "Crea una consulta que:\n",
    "1. Calcule el gasto total de cada cliente\n",
    "2. Asigne un ranking por gasto\n",
    "3. Calcule el porcentaje del total de ventas que representa cada cliente\n",
    "4. Solo muestre los top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escribe tu consulta SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… SoluciÃ³n Ejercicio SQL.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH gasto_cliente AS (\n",
    "        SELECT \n",
    "            c.cliente_id,\n",
    "            c.nombre,\n",
    "            c.tipo,\n",
    "            SUM(v.cantidad * p.precio) AS gasto_total\n",
    "        FROM clientes c\n",
    "        JOIN ventas v ON c.cliente_id = v.cliente_id\n",
    "        JOIN productos p ON v.producto_id = p.producto_id\n",
    "        GROUP BY c.cliente_id, c.nombre, c.tipo\n",
    "    ),\n",
    "    total_ventas AS (\n",
    "        SELECT SUM(gasto_total) AS total FROM gasto_cliente\n",
    "    )\n",
    "    SELECT \n",
    "        gc.nombre,\n",
    "        gc.tipo,\n",
    "        gc.gasto_total,\n",
    "        RANK() OVER (ORDER BY gc.gasto_total DESC) AS ranking,\n",
    "        ROUND(gc.gasto_total / tv.total * 100, 2) AS porcentaje_total\n",
    "    FROM gasto_cliente gc\n",
    "    CROSS JOIN total_ventas tv\n",
    "    ORDER BY ranking\n",
    "    LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Ejercicio SQL.3: AnÃ¡lisis de Tendencias\n",
    "\n",
    "Usando funciones de ventana, muestra para cada venta:\n",
    "- La venta actual\n",
    "- El total acumulado hasta esa fecha\n",
    "- La diferencia con la venta anterior\n",
    "- Si la venta fue mayor, menor o igual a la anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Escribe tu consulta SQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœ… SoluciÃ³n Ejercicio SQL.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH ventas_valor AS (\n",
    "        SELECT \n",
    "            v.venta_id,\n",
    "            v.fecha,\n",
    "            v.cantidad * p.precio AS monto\n",
    "        FROM ventas v\n",
    "        JOIN productos p ON v.producto_id = p.producto_id\n",
    "    )\n",
    "    SELECT \n",
    "        venta_id,\n",
    "        fecha,\n",
    "        monto AS venta_actual,\n",
    "        SUM(monto) OVER (ORDER BY fecha, venta_id ROWS UNBOUNDED PRECEDING) AS acumulado,\n",
    "        LAG(monto) OVER (ORDER BY fecha, venta_id) AS venta_anterior,\n",
    "        monto - LAG(monto) OVER (ORDER BY fecha, venta_id) AS diferencia,\n",
    "        CASE \n",
    "            WHEN monto > LAG(monto) OVER (ORDER BY fecha, venta_id) THEN 'MAYOR'\n",
    "            WHEN monto < LAG(monto) OVER (ORDER BY fecha, venta_id) THEN 'MENOR'\n",
    "            WHEN monto = LAG(monto) OVER (ORDER BY fecha, venta_id) THEN 'IGUAL'\n",
    "            ELSE 'PRIMERA'\n",
    "        END AS tendencia\n",
    "    FROM ventas_valor\n",
    "    ORDER BY fecha, venta_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## Resumen\n",
    "\n",
    "### Conceptos Clave\n",
    "- **Vistas temporales**: `createOrReplaceTempView()` registra DataFrames para SQL\n",
    "- **Agregaciones**: `GROUP BY` con `COUNT`, `SUM`, `AVG`, `MIN`, `MAX`\n",
    "- **JOINs**: `INNER`, `LEFT`, `RIGHT`, `FULL OUTER` para combinar tablas\n",
    "- **Subconsultas**: Consultas anidadas en `WHERE`, `FROM`, o `SELECT`\n",
    "- **CTEs**: `WITH` para consultas mÃ¡s legibles\n",
    "- **Window Functions**: `ROW_NUMBER`, `RANK`, `LAG`, `LEAD`, agregados con `OVER`\n",
    "\n",
    "### ConexiÃ³n con AWS\n",
    "- **Amazon Athena**: SQL sobre datos en S3, sintaxis muy similar\n",
    "- **Amazon Redshift**: Data warehouse con SQL compatible\n",
    "- **AWS Glue**: Soporta Spark SQL para ETL\n",
    "\n",
    "### Siguiente Paso\n",
    "ContinÃºa con: `04_joins_aggregations.ipynb` para joins y agregaciones avanzadas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
