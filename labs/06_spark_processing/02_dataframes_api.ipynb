{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API de DataFrames en Profundidad\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Dominar las operaciones avanzadas con DataFrames\n",
    "- Trabajar con diferentes tipos de datos y esquemas\n",
    "- Usar funciones built-in de Spark para transformaciones\n",
    "- Manejar valores nulos y datos faltantes\n",
    "\n",
    "## Prerequisitos\n",
    "- `00_setup/02_spark_basics.ipynb`\n",
    "- `06_spark_processing/01_rdd_fundamentals.ipynb`\n",
    "\n",
    "## Tiempo Estimado\n",
    "‚è±Ô∏è 60 minutos\n",
    "\n",
    "## M√≥dulo AWS Academy Relacionado\n",
    "üìö M√≥dulo 9: Big Data Processing - DataFrames y procesamiento estructurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necesarios\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Crear sesion de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrames_API\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(f\"Spark {spark.version} inicializado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 1 ===\n",
    "## 1. Creaci√≥n de DataFrames con Esquemas\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Un **esquema** define la estructura de un DataFrame: nombres de columnas, tipos de datos, y si permiten nulos. Definir el esquema expl√≠citamente es m√°s eficiente que dejar que Spark lo infiera.\n",
    "\n",
    "**Analog√≠a:** El esquema es como el plano de una casa. Define cu√°ntas habitaciones hay (columnas), qu√© tipo de habitaci√≥n es cada una (tipo de dato), y si son opcionales (nullable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir un esquema complejo\n",
    "# StructType es una lista de campos (columnas)\n",
    "# StructField define: nombre, tipo, nullable\n",
    "\n",
    "esquema_pedidos = StructType([\n",
    "    StructField(\"pedido_id\", StringType(), False),      # ID no puede ser nulo\n",
    "    StructField(\"cliente_id\", StringType(), False),\n",
    "    StructField(\"fecha_pedido\", DateType(), True),\n",
    "    StructField(\"producto\", StringType(), True),\n",
    "    StructField(\"cantidad\", IntegerType(), True),\n",
    "    StructField(\"precio_unitario\", DoubleType(), True),\n",
    "    StructField(\"descuento\", DoubleType(), True),       # Puede ser nulo\n",
    "    StructField(\"enviado\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Datos de ejemplo (incluyendo valores nulos)\n",
    "datos_pedidos = [\n",
    "    (\"P001\", \"C001\", date(2024, 1, 15), \"Laptop\", 2, 15000.0, 0.10, True),\n",
    "    (\"P002\", \"C002\", date(2024, 1, 16), \"Mouse\", 5, 350.0, None, True),\n",
    "    (\"P003\", \"C001\", date(2024, 1, 17), \"Monitor\", 1, 8000.0, 0.05, False),\n",
    "    (\"P004\", \"C003\", date(2024, 1, 18), \"Teclado\", 3, 800.0, None, True),\n",
    "    (\"P005\", \"C002\", date(2024, 1, 19), \"Laptop\", 1, 15000.0, 0.15, False),\n",
    "    (\"P006\", \"C004\", None, \"Silla\", 4, 2500.0, 0.0, None),  # Fecha nula\n",
    "    (\"P007\", \"C001\", date(2024, 1, 20), None, 2, 500.0, None, True)  # Producto nulo\n",
    "]\n",
    "\n",
    "# Crear DataFrame con esquema explicito\n",
    "df_pedidos = spark.createDataFrame(datos_pedidos, esquema_pedidos)\n",
    "\n",
    "print(\"DataFrame de pedidos:\")\n",
    "df_pedidos.show()\n",
    "df_pedidos.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 2 ===\n",
    "## 2. Selecci√≥n y Proyecci√≥n de Columnas\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Seleccionar columnas es como elegir qu√© informaci√≥n necesitas de una tabla. Hay m√∫ltiples formas de hacerlo en Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diferentes formas de seleccionar columnas\n",
    "\n",
    "# Forma 1: Por nombre (strings)\n",
    "df_pedidos.select(\"pedido_id\", \"producto\", \"cantidad\").show(3)\n",
    "\n",
    "# Forma 2: Usando col()\n",
    "df_pedidos.select(F.col(\"pedido_id\"), F.col(\"producto\")).show(3)\n",
    "\n",
    "# Forma 3: Usando notacion de corchetes\n",
    "df_pedidos.select(df_pedidos[\"pedido_id\"], df_pedidos[\"producto\"]).show(3)\n",
    "\n",
    "# Forma 4: Seleccionar con expresiones calculadas\n",
    "df_pedidos.select(\n",
    "    F.col(\"producto\"),\n",
    "    F.col(\"cantidad\"),\n",
    "    F.col(\"precio_unitario\"),\n",
    "    (F.col(\"cantidad\") * F.col(\"precio_unitario\")).alias(\"subtotal\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar todas las columnas excepto algunas\n",
    "# drop() elimina columnas especificas\n",
    "\n",
    "df_sin_descuento = df_pedidos.drop(\"descuento\", \"enviado\")\n",
    "print(\"Sin descuento y enviado:\")\n",
    "df_sin_descuento.show(3)\n",
    "\n",
    "# Seleccionar columnas que cumplan un patron\n",
    "# Ejemplo: todas las columnas que empiecen con 'precio'\n",
    "columnas_precio = [c for c in df_pedidos.columns if 'precio' in c.lower()]\n",
    "print(f\"Columnas con 'precio': {columnas_precio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 3 ===\n",
    "## 3. Filtrado Avanzado\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Filtrar es seleccionar filas que cumplen ciertas condiciones. Spark soporta condiciones complejas con operadores l√≥gicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrado basico\n",
    "print(\"Pedidos con cantidad > 2:\")\n",
    "df_pedidos.filter(F.col(\"cantidad\") > 2).show()\n",
    "\n",
    "# Filtro con multiples condiciones (AND)\n",
    "print(\"Laptops enviadas:\")\n",
    "df_pedidos.filter(\n",
    "    (F.col(\"producto\") == \"Laptop\") & \n",
    "    (F.col(\"enviado\") == True)\n",
    ").show()\n",
    "\n",
    "# Filtro con OR\n",
    "print(\"Laptop o Monitor:\")\n",
    "df_pedidos.filter(\n",
    "    (F.col(\"producto\") == \"Laptop\") | \n",
    "    (F.col(\"producto\") == \"Monitor\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtros con funciones especiales\n",
    "\n",
    "# isin(): Esta en una lista de valores\n",
    "print(\"Productos en lista:\")\n",
    "df_pedidos.filter(F.col(\"producto\").isin([\"Laptop\", \"Mouse\", \"Teclado\"])).show()\n",
    "\n",
    "# like(): Patron SQL (% = cualquier cosa)\n",
    "print(\"Productos que empiezan con 'M':\")\n",
    "df_pedidos.filter(F.col(\"producto\").like(\"M%\")).show()\n",
    "\n",
    "# contains(): Contiene un substring\n",
    "print(\"Productos que contienen 'o':\")\n",
    "df_pedidos.filter(F.col(\"producto\").contains(\"o\")).show()\n",
    "\n",
    "# between(): Rango de valores\n",
    "print(\"Precio entre 500 y 5000:\")\n",
    "df_pedidos.filter(F.col(\"precio_unitario\").between(500, 5000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar valores nulos\n",
    "\n",
    "# isNull(): Es nulo\n",
    "print(\"Pedidos sin descuento (nulo):\")\n",
    "df_pedidos.filter(F.col(\"descuento\").isNull()).show()\n",
    "\n",
    "# isNotNull(): No es nulo\n",
    "print(\"Pedidos con descuento:\")\n",
    "df_pedidos.filter(F.col(\"descuento\").isNotNull()).show()\n",
    "\n",
    "# Filas con CUALQUIER valor nulo\n",
    "print(\"Pedidos con algun valor nulo:\")\n",
    "from functools import reduce\n",
    "condicion_nulos = reduce(\n",
    "    lambda a, b: a | b,\n",
    "    [F.col(c).isNull() for c in df_pedidos.columns]\n",
    ")\n",
    "df_pedidos.filter(condicion_nulos).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 4 ===\n",
    "## 4. Agregar y Modificar Columnas\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "`withColumn()` permite agregar nuevas columnas o modificar existentes. Las transformaciones son inmutables: siempre crean un nuevo DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar columnas calculadas\n",
    "df_con_calculos = df_pedidos \\\n",
    "    .withColumn(\"subtotal\", F.col(\"cantidad\") * F.col(\"precio_unitario\")) \\\n",
    "    .withColumn(\"descuento_monto\", \n",
    "                F.when(F.col(\"descuento\").isNotNull(), \n",
    "                       F.col(\"subtotal\") * F.col(\"descuento\"))\n",
    "                .otherwise(0)) \\\n",
    "    .withColumn(\"total\", F.col(\"subtotal\") - F.col(\"descuento_monto\"))\n",
    "\n",
    "df_con_calculos.select(\n",
    "    \"pedido_id\", \"producto\", \"subtotal\", \"descuento_monto\", \"total\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas condicionales con when/otherwise\n",
    "# Es como un IF-THEN-ELSE\n",
    "\n",
    "df_categorizado = df_pedidos.withColumn(\n",
    "    \"categoria_precio\",\n",
    "    F.when(F.col(\"precio_unitario\") >= 10000, \"Premium\")\n",
    "    .when(F.col(\"precio_unitario\") >= 1000, \"Medio\")\n",
    "    .otherwise(\"Economico\")\n",
    ")\n",
    "\n",
    "df_categorizado.select(\"producto\", \"precio_unitario\", \"categoria_precio\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar columnas\n",
    "\n",
    "# withColumnRenamed(): Renombra una columna\n",
    "df_renombrado = df_pedidos.withColumnRenamed(\"precio_unitario\", \"precio\")\n",
    "print(\"Columnas renombradas:\")\n",
    "print(df_renombrado.columns)\n",
    "\n",
    "# Renombrar multiples columnas con alias en select\n",
    "df_pedidos.select(\n",
    "    F.col(\"pedido_id\").alias(\"id\"),\n",
    "    F.col(\"producto\").alias(\"item\"),\n",
    "    F.col(\"precio_unitario\").alias(\"precio\")\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 5 ===\n",
    "## 5. Funciones de Cadenas de Texto\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Spark incluye muchas funciones para manipular strings, similares a las funciones de SQL o Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame de ejemplo con strings\n",
    "df_strings = spark.createDataFrame([\n",
    "    (\"  Ana Garcia  \", \"ana.garcia@email.com\"),\n",
    "    (\"CARLOS LOPEZ\", \"Carlos.Lopez@EMPRESA.COM\"),\n",
    "    (\"maria rodriguez\", \"MARIA@test.com\")\n",
    "], [\"nombre\", \"email\"])\n",
    "\n",
    "df_strings.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de strings comunes\n",
    "\n",
    "df_limpio = df_strings.select(\n",
    "    F.col(\"nombre\"),\n",
    "    F.trim(F.col(\"nombre\")).alias(\"nombre_trim\"),           # Eliminar espacios\n",
    "    F.lower(F.col(\"nombre\")).alias(\"nombre_lower\"),         # Minusculas\n",
    "    F.upper(F.col(\"nombre\")).alias(\"nombre_upper\"),         # Mayusculas\n",
    "    F.initcap(F.col(\"nombre\")).alias(\"nombre_title\"),       # Primera letra mayuscula\n",
    "    F.length(F.trim(F.col(\"nombre\"))).alias(\"longitud\")     # Longitud\n",
    ")\n",
    "\n",
    "df_limpio.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer partes de strings\n",
    "\n",
    "df_email = df_strings.select(\n",
    "    F.col(\"email\"),\n",
    "    # split() divide por un delimitador y retorna un array\n",
    "    F.split(F.col(\"email\"), \"@\").alias(\"partes\"),\n",
    "    # Acceder a elementos del array con getItem()\n",
    "    F.split(F.col(\"email\"), \"@\").getItem(0).alias(\"usuario\"),\n",
    "    F.split(F.col(\"email\"), \"@\").getItem(1).alias(\"dominio\"),\n",
    "    # substring(col, inicio, longitud) - inicio es 1-based\n",
    "    F.substring(F.col(\"email\"), 1, 3).alias(\"primeros_3\")\n",
    ")\n",
    "\n",
    "df_email.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar y concatenar\n",
    "\n",
    "df_transformado = df_strings.select(\n",
    "    F.col(\"nombre\"),\n",
    "    # regexp_replace(col, patron, reemplazo)\n",
    "    F.regexp_replace(F.col(\"nombre\"), \"\\\\s+\", \"_\").alias(\"con_underscore\"),\n",
    "    # concat() une strings\n",
    "    F.concat(F.col(\"nombre\"), F.lit(\" - \"), F.col(\"email\")).alias(\"combinado\"),\n",
    "    # concat_ws() une con separador\n",
    "    F.concat_ws(\" | \", F.col(\"nombre\"), F.col(\"email\")).alias(\"con_separador\")\n",
    ")\n",
    "\n",
    "df_transformado.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 6 ===\n",
    "## 6. Funciones de Fecha y Hora\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Las fechas y horas son fundamentales en an√°lisis de datos. Spark tiene funciones robustas para manipularlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de fecha\n",
    "\n",
    "df_fechas = df_pedidos.filter(F.col(\"fecha_pedido\").isNotNull()).select(\n",
    "    F.col(\"pedido_id\"),\n",
    "    F.col(\"fecha_pedido\"),\n",
    "    # Extraer componentes de fecha\n",
    "    F.year(F.col(\"fecha_pedido\")).alias(\"anio\"),\n",
    "    F.month(F.col(\"fecha_pedido\")).alias(\"mes\"),\n",
    "    F.dayofmonth(F.col(\"fecha_pedido\")).alias(\"dia\"),\n",
    "    F.dayofweek(F.col(\"fecha_pedido\")).alias(\"dia_semana\"),  # 1=Domingo\n",
    "    F.dayofyear(F.col(\"fecha_pedido\")).alias(\"dia_anio\"),\n",
    "    F.weekofyear(F.col(\"fecha_pedido\")).alias(\"semana\"),\n",
    "    F.quarter(F.col(\"fecha_pedido\")).alias(\"trimestre\")\n",
    ")\n",
    "\n",
    "df_fechas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operaciones con fechas\n",
    "\n",
    "df_operaciones = df_pedidos.filter(F.col(\"fecha_pedido\").isNotNull()).select(\n",
    "    F.col(\"pedido_id\"),\n",
    "    F.col(\"fecha_pedido\"),\n",
    "    # Fecha actual\n",
    "    F.current_date().alias(\"hoy\"),\n",
    "    # Diferencia en dias\n",
    "    F.datediff(F.current_date(), F.col(\"fecha_pedido\")).alias(\"dias_transcurridos\"),\n",
    "    # Sumar/restar dias\n",
    "    F.date_add(F.col(\"fecha_pedido\"), 30).alias(\"fecha_mas_30\"),\n",
    "    F.date_sub(F.col(\"fecha_pedido\"), 7).alias(\"fecha_menos_7\"),\n",
    "    # Diferencia en meses\n",
    "    F.months_between(F.current_date(), F.col(\"fecha_pedido\")).alias(\"meses_transcurridos\")\n",
    ")\n",
    "\n",
    "df_operaciones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formateo de fechas\n",
    "\n",
    "df_formato = df_pedidos.filter(F.col(\"fecha_pedido\").isNotNull()).select(\n",
    "    F.col(\"fecha_pedido\"),\n",
    "    # date_format() convierte fecha a string con formato\n",
    "    F.date_format(F.col(\"fecha_pedido\"), \"dd/MM/yyyy\").alias(\"formato_mx\"),\n",
    "    F.date_format(F.col(\"fecha_pedido\"), \"MMMM dd, yyyy\").alias(\"formato_largo\"),\n",
    "    F.date_format(F.col(\"fecha_pedido\"), \"EEEE\").alias(\"nombre_dia\")\n",
    ")\n",
    "\n",
    "df_formato.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 7 ===\n",
    "## 7. Manejo de Valores Nulos\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Los datos del mundo real contienen valores faltantes. Spark ofrece varias estrategias para manejarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver donde hay nulos\n",
    "print(\"Conteo de nulos por columna:\")\n",
    "df_pedidos.select([\n",
    "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df_pedidos.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con nulos\n",
    "\n",
    "# Eliminar filas con CUALQUIER nulo\n",
    "df_sin_nulos = df_pedidos.dropna()\n",
    "print(f\"Original: {df_pedidos.count()} filas\")\n",
    "print(f\"Sin ningun nulo: {df_sin_nulos.count()} filas\")\n",
    "\n",
    "# Eliminar filas con nulo en columnas especificas\n",
    "df_sin_nulos_producto = df_pedidos.dropna(subset=[\"producto\", \"fecha_pedido\"])\n",
    "print(f\"Sin nulo en producto/fecha: {df_sin_nulos_producto.count()} filas\")\n",
    "\n",
    "# Eliminar solo si hay minimo N nulos\n",
    "df_con_algunos = df_pedidos.dropna(thresh=6)  # Mantener si hay al menos 6 no-nulos\n",
    "print(f\"Con al menos 6 valores: {df_con_algunos.count()} filas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar nulos\n",
    "\n",
    "# Rellenar con valor especifico\n",
    "df_rellenado = df_pedidos.fillna({\n",
    "    \"descuento\": 0.0,\n",
    "    \"enviado\": False,\n",
    "    \"producto\": \"Sin especificar\"\n",
    "})\n",
    "\n",
    "print(\"Con nulos rellenados:\")\n",
    "df_rellenado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar con coalesce() - primer valor no nulo\n",
    "\n",
    "df_coalesce = df_pedidos.withColumn(\n",
    "    \"descuento_final\",\n",
    "    F.coalesce(F.col(\"descuento\"), F.lit(0.0))\n",
    ")\n",
    "\n",
    "df_coalesce.select(\"pedido_id\", \"descuento\", \"descuento_final\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === EJERCICIOS PR√ÅCTICOS ===\n",
    "\n",
    "### üéØ Ejercicio DF.1: Transformaci√≥n Completa\n",
    "\n",
    "Dado el DataFrame de pedidos:\n",
    "1. Filtra pedidos del cliente \"C001\"\n",
    "2. Calcula el total (cantidad √ó precio - descuento)\n",
    "3. Agrega una columna \"urgente\" que sea True si el pedido no ha sido enviado\n",
    "4. Ordena por total descendente\n",
    "\n",
    "**Pistas:**\n",
    "- Usa `coalesce()` para manejar descuentos nulos\n",
    "- Usa `when()` para la columna urgente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa el ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio DF.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion\n",
    "resultado = df_pedidos \\\n",
    "    .filter(F.col(\"cliente_id\") == \"C001\") \\\n",
    "    .withColumn(\"subtotal\", F.col(\"cantidad\") * F.col(\"precio_unitario\")) \\\n",
    "    .withColumn(\"descuento_valor\", \n",
    "                F.col(\"subtotal\") * F.coalesce(F.col(\"descuento\"), F.lit(0.0))) \\\n",
    "    .withColumn(\"total\", F.col(\"subtotal\") - F.col(\"descuento_valor\")) \\\n",
    "    .withColumn(\"urgente\", \n",
    "                F.when(F.col(\"enviado\") == False, True)\n",
    "                .when(F.col(\"enviado\").isNull(), True)\n",
    "                .otherwise(False)) \\\n",
    "    .orderBy(F.col(\"total\").desc())\n",
    "\n",
    "resultado.select(\n",
    "    \"pedido_id\", \"producto\", \"total\", \"enviado\", \"urgente\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio DF.2: An√°lisis de Fechas\n",
    "\n",
    "1. Agrega columna con el d√≠a de la semana en espa√±ol\n",
    "2. Agrega columna indicando si es fin de semana\n",
    "3. Muestra cu√°ntos pedidos hay por d√≠a de la semana\n",
    "\n",
    "**Pistas:**\n",
    "- `dayofweek()` retorna 1=Domingo, 2=Lunes, etc.\n",
    "- Usa `when()` encadenado para mapear n√∫meros a nombres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa el ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio DF.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion\n",
    "df_dias = df_pedidos \\\n",
    "    .filter(F.col(\"fecha_pedido\").isNotNull()) \\\n",
    "    .withColumn(\"dia_num\", F.dayofweek(F.col(\"fecha_pedido\"))) \\\n",
    "    .withColumn(\"dia_nombre\",\n",
    "        F.when(F.col(\"dia_num\") == 1, \"Domingo\")\n",
    "        .when(F.col(\"dia_num\") == 2, \"Lunes\")\n",
    "        .when(F.col(\"dia_num\") == 3, \"Martes\")\n",
    "        .when(F.col(\"dia_num\") == 4, \"Miercoles\")\n",
    "        .when(F.col(\"dia_num\") == 5, \"Jueves\")\n",
    "        .when(F.col(\"dia_num\") == 6, \"Viernes\")\n",
    "        .when(F.col(\"dia_num\") == 7, \"Sabado\")\n",
    "    ) \\\n",
    "    .withColumn(\"es_fin_semana\",\n",
    "        F.col(\"dia_num\").isin([1, 7])\n",
    "    )\n",
    "\n",
    "print(\"Pedidos con dia de la semana:\")\n",
    "df_dias.select(\"pedido_id\", \"fecha_pedido\", \"dia_nombre\", \"es_fin_semana\").show()\n",
    "\n",
    "print(\"Pedidos por dia:\")\n",
    "df_dias.groupBy(\"dia_nombre\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio DF.3: Limpieza de Datos\n",
    "\n",
    "1. Identifica y muestra las filas con valores nulos\n",
    "2. Crea una versi√≥n \"limpia\" del DataFrame:\n",
    "   - Rellena descuentos nulos con 0\n",
    "   - Rellena productos nulos con \"DESCONOCIDO\"\n",
    "   - Elimina filas sin fecha\n",
    "3. Verifica que no hay nulos en la versi√≥n limpia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa el ejercicio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio DF.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion\n",
    "\n",
    "# 1. Mostrar filas con nulos\n",
    "print(\"Filas con al menos un valor nulo:\")\n",
    "condicion = reduce(\n",
    "    lambda a, b: a | b,\n",
    "    [F.col(c).isNull() for c in df_pedidos.columns]\n",
    ")\n",
    "df_pedidos.filter(condicion).show()\n",
    "\n",
    "# 2. Crear version limpia\n",
    "df_limpio = df_pedidos \\\n",
    "    .fillna({\"descuento\": 0.0, \"producto\": \"DESCONOCIDO\", \"enviado\": False}) \\\n",
    "    .dropna(subset=[\"fecha_pedido\"])\n",
    "\n",
    "print(\"\\nDataFrame limpio:\")\n",
    "df_limpio.show()\n",
    "\n",
    "# 3. Verificar que no hay nulos\n",
    "print(\"\\nConteo de nulos en DataFrame limpio:\")\n",
    "df_limpio.select([\n",
    "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df_limpio.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## Resumen\n",
    "\n",
    "### Conceptos Clave\n",
    "- **Esquema**: Define estructura (tipos y nullable) - mejor expl√≠cito que inferido\n",
    "- **Selecci√≥n**: `select()`, `drop()` para elegir columnas\n",
    "- **Filtrado**: `filter()` con condiciones, `isin()`, `like()`, `isNull()`\n",
    "- **Transformaci√≥n**: `withColumn()`, `when/otherwise` para condicionales\n",
    "- **Strings**: `trim()`, `lower()`, `split()`, `regexp_replace()`\n",
    "- **Fechas**: `year()`, `month()`, `datediff()`, `date_format()`\n",
    "- **Nulos**: `dropna()`, `fillna()`, `coalesce()`\n",
    "\n",
    "### Conexi√≥n con AWS\n",
    "- **AWS Glue**: Usa DynamicFrames con operaciones similares\n",
    "- **Athena**: Funciones SQL equivalentes sobre datos en S3\n",
    "- **EMR**: Ejecuta este mismo c√≥digo en clusters grandes\n",
    "\n",
    "### Siguiente Paso\n",
    "Contin√∫a con: `03_spark_sql.ipynb` para dominar SQL en Spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
