{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de RDD (Resilient Distributed Datasets)\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Entender qu√© es un RDD y por qu√© es importante\n",
    "- Crear RDDs desde diferentes fuentes\n",
    "- Aplicar transformaciones y acciones b√°sicas\n",
    "- Comprender el concepto de particiones y distribuci√≥n\n",
    "\n",
    "## Prerequisitos\n",
    "- `00_setup/01_environment_check.ipynb`\n",
    "- `00_setup/02_spark_basics.ipynb`\n",
    "\n",
    "## Tiempo Estimado\n",
    "‚è±Ô∏è 45 minutos\n",
    "\n",
    "## M√≥dulo AWS Academy Relacionado\n",
    "üìö M√≥dulo 9: Big Data Processing - Fundamentos de procesamiento distribuido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 1 ===\n",
    "## 1. ¬øQu√© es un RDD?\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "**RDD (Resilient Distributed Dataset)** es la abstracci√≥n fundamental de Spark. Es una colecci√≥n de elementos:\n",
    "- **Resilient**: Si una parte falla, se puede reconstruir\n",
    "- **Distributed**: Los datos est√°n repartidos en m√∫ltiples m√°quinas\n",
    "- **Dataset**: Es una colecci√≥n de datos\n",
    "\n",
    "**Analog√≠a del mundo real:** Imagina que tienes que contar todas las palabras en una biblioteca de 1 mill√≥n de libros. Un RDD te permite dividir esos libros entre 100 personas (workers), cada una cuenta sus libros (particiones), y al final sumas los resultados. Si alguien pierde su cuenta, puede volver a hacerla porque sabe qu√© libros ten√≠a.\n",
    "\n",
    "### ¬øPor qu√© aprender RDDs si existen DataFrames?\n",
    "- DataFrames usan RDDs internamente\n",
    "- Entender RDDs ayuda a debuggear problemas\n",
    "- Algunas operaciones de bajo nivel requieren RDDs\n",
    "- C√≥digo legacy usa RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos SparkSession y SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos la sesion de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD_Fundamentals\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# SparkContext es el punto de entrada para crear RDDs\n",
    "# sc es una convencion comun para nombrar el SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Reducimos logs para ver mejor los outputs\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"SparkContext creado\")\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Master: {sc.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 2 ===\n",
    "## 2. Crear RDDs\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Hay dos formas principales de crear un RDD:\n",
    "1. **parallelize()**: Desde una colecci√≥n de Python (listas, tuplas)\n",
    "2. **textFile()**: Desde archivos externos (CSV, TXT, etc.)\n",
    "\n",
    "**Analog√≠a:** Es como crear una lista de tareas. Puedes escribirla t√∫ mismo (parallelize) o importarla de un archivo (textFile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METODO 1: Crear RDD desde una lista de Python\n",
    "# parallelize() distribuye los datos en el cluster\n",
    "\n",
    "# Lista de numeros del 1 al 10\n",
    "numeros = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Creamos el RDD con 4 particiones\n",
    "# Las particiones son \"pedazos\" del RDD distribuidos en workers\n",
    "rdd_numeros = sc.parallelize(numeros, 4)\n",
    "\n",
    "# Verificamos que se creo correctamente\n",
    "print(f\"Tipo: {type(rdd_numeros)}\")\n",
    "print(f\"Numero de particiones: {rdd_numeros.getNumPartitions()}\")\n",
    "\n",
    "# collect() es una ACCION que trae todos los datos al driver\n",
    "# CUIDADO: No usar collect() con datos grandes!\n",
    "print(f\"Contenido: {rdd_numeros.collect()}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Tipo: <class 'pyspark.rdd.RDD'>\n",
    "# Numero de particiones: 4\n",
    "# Contenido: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver como estan distribuidos los datos en cada particion\n",
    "# glom() agrupa elementos por particion\n",
    "\n",
    "particiones = rdd_numeros.glom().collect()\n",
    "\n",
    "print(\"Distribucion de datos por particion:\")\n",
    "for i, particion in enumerate(particiones):\n",
    "    print(f\"  Particion {i}: {particion}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Particion 0: [1, 2]\n",
    "# Particion 1: [3, 4, 5]\n",
    "# Particion 2: [6, 7]\n",
    "# Particion 3: [8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METODO 2: Crear RDD desde texto\n",
    "# Primero creamos un archivo de ejemplo\n",
    "\n",
    "# Datos de ejemplo: ventas por linea\n",
    "texto_ventas = \"\"\"2024-01-01,Laptop,15000\n",
    "2024-01-01,Mouse,350\n",
    "2024-01-02,Monitor,8000\n",
    "2024-01-02,Teclado,800\n",
    "2024-01-03,Laptop,15000\"\"\"\n",
    "\n",
    "# Guardamos el archivo\n",
    "ruta_archivo = \"/home/jovyan/data/sample/ventas_rdd.txt\"\n",
    "with open(ruta_archivo, 'w') as f:\n",
    "    f.write(texto_ventas)\n",
    "\n",
    "print(f\"Archivo creado en: {ruta_archivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo como RDD\n",
    "# textFile() lee cada linea como un elemento del RDD\n",
    "rdd_ventas = sc.textFile(ruta_archivo)\n",
    "\n",
    "# Mostramos las primeras lineas\n",
    "# take(n) trae solo n elementos (mas seguro que collect)\n",
    "print(\"Primeras 3 lineas del archivo:\")\n",
    "for linea in rdd_ventas.take(3):\n",
    "    print(f\"  {linea}\")\n",
    "\n",
    "print(f\"\\nTotal de lineas: {rdd_ventas.count()}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Primeras 3 lineas del archivo:\n",
    "#   2024-01-01,Laptop,15000\n",
    "#   2024-01-01,Mouse,350\n",
    "#   2024-01-02,Monitor,8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 3 ===\n",
    "## 3. Transformaciones B√°sicas\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Las transformaciones crean un NUEVO RDD a partir de uno existente. Son **lazy** (no se ejecutan hasta que hay una acci√≥n).\n",
    "\n",
    "**Transformaciones comunes:**\n",
    "- `map()`: Aplica una funci√≥n a cada elemento\n",
    "- `filter()`: Filtra elementos seg√∫n una condici√≥n\n",
    "- `flatMap()`: Como map pero \"aplana\" los resultados\n",
    "\n",
    "**Analog√≠a:** Las transformaciones son como instrucciones en una receta. No cocinas nada hasta que decides servir el plato (acci√≥n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP: Aplica una funcion a cada elemento\n",
    "# Ejemplo: Elevar cada numero al cuadrado\n",
    "\n",
    "# La funcion lambda toma un elemento y retorna su cuadrado\n",
    "rdd_cuadrados = rdd_numeros.map(lambda x: x ** 2)\n",
    "\n",
    "print(\"Numeros originales:\", rdd_numeros.collect())\n",
    "print(\"Numeros al cuadrado:\", rdd_cuadrados.collect())\n",
    "\n",
    "# Output esperado:\n",
    "# Numeros originales: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# Numeros al cuadrado: [1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP con funcion mas compleja\n",
    "# Parsear las lineas de ventas\n",
    "\n",
    "def parsear_venta(linea):\n",
    "    \"\"\"Convierte una linea de texto en una tupla estructurada\"\"\"\n",
    "    # split() divide el string por comas\n",
    "    partes = linea.split(\",\")\n",
    "    # Retornamos una tupla (fecha, producto, precio)\n",
    "    return (partes[0], partes[1], int(partes[2]))\n",
    "\n",
    "# Aplicamos la funcion a cada linea\n",
    "rdd_ventas_parsed = rdd_ventas.map(parsear_venta)\n",
    "\n",
    "print(\"Ventas parseadas:\")\n",
    "for venta in rdd_ventas_parsed.collect():\n",
    "    print(f\"  Fecha: {venta[0]}, Producto: {venta[1]}, Precio: ${venta[2]:,}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Fecha: 2024-01-01, Producto: Laptop, Precio: $15,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER: Selecciona elementos que cumplen una condicion\n",
    "\n",
    "# Filtrar solo numeros pares\n",
    "# lambda x: x % 2 == 0 retorna True si x es par\n",
    "rdd_pares = rdd_numeros.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "print(\"Numeros pares:\", rdd_pares.collect())\n",
    "\n",
    "# Filtrar ventas mayores a 1000\n",
    "rdd_ventas_grandes = rdd_ventas_parsed.filter(lambda v: v[2] > 1000)\n",
    "\n",
    "print(\"\\nVentas mayores a $1,000:\")\n",
    "for venta in rdd_ventas_grandes.collect():\n",
    "    print(f\"  {venta[1]}: ${venta[2]:,}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Numeros pares: [2, 4, 6, 8, 10]\n",
    "# Ventas mayores a $1,000:\n",
    "#   Laptop: $15,000\n",
    "#   Monitor: $8,000\n",
    "#   Laptop: $15,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLATMAP: Map que \"aplana\" los resultados\n",
    "# Util cuando cada elemento genera multiples resultados\n",
    "\n",
    "# Ejemplo: Dividir oraciones en palabras\n",
    "oraciones = [\"Hola mundo\", \"Apache Spark es genial\", \"Big Data\"]\n",
    "rdd_oraciones = sc.parallelize(oraciones)\n",
    "\n",
    "# Con map: cada oracion se convierte en una lista de palabras\n",
    "rdd_con_map = rdd_oraciones.map(lambda x: x.split())\n",
    "print(\"Con map (listas anidadas):\", rdd_con_map.collect())\n",
    "\n",
    "# Con flatMap: las listas se \"aplanan\" en elementos individuales\n",
    "rdd_con_flatmap = rdd_oraciones.flatMap(lambda x: x.split())\n",
    "print(\"Con flatMap (lista plana):\", rdd_con_flatmap.collect())\n",
    "\n",
    "# Output esperado:\n",
    "# Con map: [['Hola', 'mundo'], ['Apache', 'Spark', 'es', 'genial'], ['Big', 'Data']]\n",
    "# Con flatMap: ['Hola', 'mundo', 'Apache', 'Spark', 'es', 'genial', 'Big', 'Data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 4 ===\n",
    "## 4. Acciones Comunes\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Las acciones **disparan la ejecuci√≥n** de todas las transformaciones pendientes y **retornan un resultado** al driver.\n",
    "\n",
    "**Acciones comunes:**\n",
    "- `collect()`: Trae todos los datos al driver (‚ö†Ô∏è cuidado con datos grandes)\n",
    "- `count()`: Cuenta elementos\n",
    "- `take(n)`: Trae los primeros n elementos\n",
    "- `reduce()`: Combina elementos usando una funci√≥n\n",
    "- `foreach()`: Ejecuta una funci√≥n en cada elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNT: Cuenta el numero de elementos\n",
    "total = rdd_numeros.count()\n",
    "print(f\"Total de numeros: {total}\")\n",
    "\n",
    "# FIRST: Obtiene el primer elemento\n",
    "primero = rdd_numeros.first()\n",
    "print(f\"Primer elemento: {primero}\")\n",
    "\n",
    "# TAKE: Obtiene los primeros n elementos\n",
    "primeros_5 = rdd_numeros.take(5)\n",
    "print(f\"Primeros 5 elementos: {primeros_5}\")\n",
    "\n",
    "# TOP: Obtiene los n elementos mas grandes\n",
    "top_3 = rdd_numeros.top(3)\n",
    "print(f\"Top 3 elementos: {top_3}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Total de numeros: 10\n",
    "# Primer elemento: 1\n",
    "# Primeros 5 elementos: [1, 2, 3, 4, 5]\n",
    "# Top 3 elementos: [10, 9, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCE: Combina todos los elementos usando una funcion\n",
    "# La funcion toma 2 elementos y retorna 1\n",
    "\n",
    "# Sumar todos los numeros\n",
    "# lambda a, b: a + b toma dos numeros y retorna su suma\n",
    "suma_total = rdd_numeros.reduce(lambda a, b: a + b)\n",
    "print(f\"Suma de todos los numeros: {suma_total}\")\n",
    "\n",
    "# Encontrar el maximo\n",
    "maximo = rdd_numeros.reduce(lambda a, b: a if a > b else b)\n",
    "print(f\"Numero maximo: {maximo}\")\n",
    "\n",
    "# Concatenar strings\n",
    "palabras = sc.parallelize([\"Hola\", \"Mundo\", \"Spark\"])\n",
    "concatenado = palabras.reduce(lambda a, b: a + \" \" + b)\n",
    "print(f\"Palabras concatenadas: {concatenado}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Suma de todos los numeros: 55\n",
    "# Numero maximo: 10\n",
    "# Palabras concatenadas: Hola Mundo Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otras acciones utiles\n",
    "\n",
    "# sum(): Suma todos los elementos (solo para numericos)\n",
    "print(f\"Suma: {rdd_numeros.sum()}\")\n",
    "\n",
    "# mean(): Promedio\n",
    "print(f\"Promedio: {rdd_numeros.mean()}\")\n",
    "\n",
    "# max() y min(): Maximo y minimo\n",
    "print(f\"Maximo: {rdd_numeros.max()}\")\n",
    "print(f\"Minimo: {rdd_numeros.min()}\")\n",
    "\n",
    "# stdev(): Desviacion estandar\n",
    "print(f\"Desviacion estandar: {rdd_numeros.stdev():.2f}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Suma: 55\n",
    "# Promedio: 5.5\n",
    "# Maximo: 10\n",
    "# Minimo: 1\n",
    "# Desviacion estandar: 2.87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 5 ===\n",
    "## 5. Operaciones de Pares Clave-Valor\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "Muchas operaciones de Big Data trabajan con pares **(clave, valor)**. Spark tiene operaciones especiales para este tipo de datos.\n",
    "\n",
    "**Analog√≠a:** Es como organizar recibos por categor√≠a. La categor√≠a es la \"clave\" y el monto es el \"valor\". Luego puedes sumar todos los montos por categor√≠a.\n",
    "\n",
    "**Operaciones comunes:**\n",
    "- `reduceByKey()`: Agrupa y reduce por clave\n",
    "- `groupByKey()`: Agrupa valores por clave\n",
    "- `sortByKey()`: Ordena por clave\n",
    "- `countByKey()`: Cuenta elementos por clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un RDD de pares (producto, precio)\n",
    "ventas_pares = [\n",
    "    (\"Laptop\", 15000),\n",
    "    (\"Mouse\", 350),\n",
    "    (\"Laptop\", 15000),\n",
    "    (\"Mouse\", 350),\n",
    "    (\"Monitor\", 8000),\n",
    "    (\"Mouse\", 350),\n",
    "    (\"Laptop\", 14500)\n",
    "]\n",
    "\n",
    "rdd_ventas_pares = sc.parallelize(ventas_pares)\n",
    "\n",
    "print(\"Ventas originales:\")\n",
    "for venta in rdd_ventas_pares.collect():\n",
    "    print(f\"  {venta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDUCEBYKEY: Agrupa por clave y reduce los valores\n",
    "# Es la operacion mas comun y eficiente para agregaciones\n",
    "\n",
    "# Sumar ventas por producto\n",
    "# Para cada clave (producto), suma los valores (precios)\n",
    "ventas_por_producto = rdd_ventas_pares.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"Ventas totales por producto:\")\n",
    "for producto, total in ventas_por_producto.collect():\n",
    "    print(f\"  {producto}: ${total:,}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Laptop: $44,500\n",
    "# Mouse: $1,050\n",
    "# Monitor: $8,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTBYKEY: Cuenta ocurrencias por clave\n",
    "conteo = rdd_ventas_pares.countByKey()\n",
    "\n",
    "print(\"Numero de ventas por producto:\")\n",
    "for producto, cantidad in conteo.items():\n",
    "    print(f\"  {producto}: {cantidad} ventas\")\n",
    "\n",
    "# Output esperado:\n",
    "# Laptop: 3 ventas\n",
    "# Mouse: 3 ventas\n",
    "# Monitor: 1 ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROUPBYKEY: Agrupa todos los valores por clave\n",
    "# CUIDADO: Menos eficiente que reduceByKey para agregaciones\n",
    "\n",
    "agrupado = rdd_ventas_pares.groupByKey()\n",
    "\n",
    "print(\"Valores agrupados por producto:\")\n",
    "for producto, valores in agrupado.collect():\n",
    "    # valores es un iterador, lo convertimos a lista\n",
    "    lista_valores = list(valores)\n",
    "    print(f\"  {producto}: {lista_valores}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Laptop: [15000, 15000, 14500]\n",
    "# Mouse: [350, 350, 350]\n",
    "# Monitor: [8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SORTBYKEY: Ordena por clave\n",
    "ordenado = rdd_ventas_pares.sortByKey()\n",
    "\n",
    "print(\"Ventas ordenadas alfabeticamente por producto:\")\n",
    "for venta in ordenado.collect():\n",
    "    print(f\"  {venta}\")\n",
    "\n",
    "# Ordenar descendente\n",
    "ordenado_desc = rdd_ventas_pares.sortByKey(ascending=False)\n",
    "print(\"\\nOrdenado Z-A:\")\n",
    "for venta in ordenado_desc.collect():\n",
    "    print(f\"  {venta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 6 ===\n",
    "## 6. Ejemplo Pr√°ctico: Word Count\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "El \"Word Count\" es el \"Hola Mundo\" del Big Data. Consiste en contar cu√°ntas veces aparece cada palabra en un texto. Es simple pero demuestra el patr√≥n Map-Reduce usado en procesamiento distribuido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de ejemplo\n",
    "texto = \"\"\"\n",
    "Apache Spark es un framework de procesamiento de datos\n",
    "Spark puede procesar datos en memoria\n",
    "Spark es muy rapido para Big Data\n",
    "Big Data es el futuro del procesamiento de datos\n",
    "\"\"\"\n",
    "\n",
    "# Guardamos el texto en un archivo\n",
    "ruta_texto = \"/home/jovyan/data/sample/texto_wordcount.txt\"\n",
    "with open(ruta_texto, 'w') as f:\n",
    "    f.write(texto)\n",
    "\n",
    "# Leemos el archivo como RDD\n",
    "rdd_texto = sc.textFile(ruta_texto)\n",
    "print(\"Lineas del texto:\")\n",
    "for linea in rdd_texto.collect():\n",
    "    if linea.strip():  # Solo lineas no vacias\n",
    "        print(f\"  {linea}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD COUNT - Paso a paso\n",
    "\n",
    "# Paso 1: Dividir cada linea en palabras (flatMap)\n",
    "# flatMap aplana el resultado: [[\"a\", \"b\"], [\"c\"]] -> [\"a\", \"b\", \"c\"]\n",
    "palabras = rdd_texto.flatMap(lambda linea: linea.lower().split())\n",
    "print(f\"Paso 1 - Total de palabras: {palabras.count()}\")\n",
    "print(f\"Muestra: {palabras.take(10)}\")\n",
    "\n",
    "# Paso 2: Crear pares (palabra, 1)\n",
    "# Cada palabra se convierte en un par clave-valor\n",
    "pares = palabras.map(lambda palabra: (palabra, 1))\n",
    "print(f\"\\nPaso 2 - Pares creados:\")\n",
    "print(f\"Muestra: {pares.take(5)}\")\n",
    "\n",
    "# Paso 3: Sumar por clave (reduceByKey)\n",
    "# Agrupa las palabras iguales y suma sus conteos\n",
    "conteos = pares.reduceByKey(lambda a, b: a + b)\n",
    "print(f\"\\nPaso 3 - Palabras unicas: {conteos.count()}\")\n",
    "\n",
    "# Paso 4: Ordenar por conteo (descendente)\n",
    "# Intercambiamos clave-valor para ordenar por conteo\n",
    "ordenado = conteos.map(lambda x: (x[1], x[0])) \\\n",
    "                  .sortByKey(ascending=False) \\\n",
    "                  .map(lambda x: (x[1], x[0]))\n",
    "\n",
    "print(\"\\nTop 10 palabras mas frecuentes:\")\n",
    "for palabra, conteo in ordenado.take(10):\n",
    "    print(f\"  '{palabra}': {conteo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version compacta del Word Count (una linea encadenada)\n",
    "\n",
    "resultado = sc.textFile(ruta_texto) \\\n",
    "    .flatMap(lambda l: l.lower().split()) \\\n",
    "    .map(lambda w: (w, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"Word Count (version compacta):\")\n",
    "for palabra, conteo in resultado.take(5):\n",
    "    print(f\"  {palabra}: {conteo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === EJERCICIOS PR√ÅCTICOS ===\n",
    "\n",
    "### üéØ Ejercicio 6.1: Filtrar y Transformar\n",
    "\n",
    "Dado el RDD de n√∫meros del 1 al 20:\n",
    "1. Filtra solo los n√∫meros divisibles por 3\n",
    "2. Multiplica cada n√∫mero filtrado por 10\n",
    "3. Calcula la suma total\n",
    "\n",
    "**Pistas:**\n",
    "- `x % 3 == 0` verifica si x es divisible por 3\n",
    "- Encadena `filter()`, `map()`, y `reduce()` o `sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa el ejercicio\n",
    "rdd_1_20 = sc.parallelize(range(1, 21))\n",
    "\n",
    "# Tu codigo aqui:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion: Filtrar y transformar\n",
    "\n",
    "rdd_1_20 = sc.parallelize(range(1, 21))\n",
    "\n",
    "# Paso 1: Filtrar divisibles por 3\n",
    "divisibles_3 = rdd_1_20.filter(lambda x: x % 3 == 0)\n",
    "print(f\"Divisibles por 3: {divisibles_3.collect()}\")\n",
    "\n",
    "# Paso 2: Multiplicar por 10\n",
    "multiplicados = divisibles_3.map(lambda x: x * 10)\n",
    "print(f\"Multiplicados por 10: {multiplicados.collect()}\")\n",
    "\n",
    "# Paso 3: Suma total\n",
    "suma = multiplicados.sum()\n",
    "print(f\"Suma total: {suma}\")\n",
    "\n",
    "# Version encadenada:\n",
    "resultado = sc.parallelize(range(1, 21)) \\\n",
    "    .filter(lambda x: x % 3 == 0) \\\n",
    "    .map(lambda x: x * 10) \\\n",
    "    .sum()\n",
    "\n",
    "print(f\"\\nResultado (encadenado): {resultado}\")\n",
    "\n",
    "# Output esperado:\n",
    "# Divisibles por 3: [3, 6, 9, 12, 15, 18]\n",
    "# Multiplicados por 10: [30, 60, 90, 120, 150, 180]\n",
    "# Suma total: 630"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 6.2: An√°lisis de Ventas con RDD\n",
    "\n",
    "Dado el siguiente RDD de ventas `(producto, cantidad, precio_unitario)`:\n",
    "1. Calcula el ingreso total por producto (cantidad √ó precio)\n",
    "2. Encuentra el producto con mayor ingreso\n",
    "\n",
    "**Pistas:**\n",
    "- Usa `map()` para calcular ingreso y crear pares (producto, ingreso)\n",
    "- Usa `reduceByKey()` para sumar por producto\n",
    "- Usa `max()` con una funci√≥n key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa el ejercicio\n",
    "ventas_data = [\n",
    "    (\"Laptop\", 2, 15000),\n",
    "    (\"Mouse\", 10, 350),\n",
    "    (\"Laptop\", 1, 15000),\n",
    "    (\"Monitor\", 3, 8000),\n",
    "    (\"Mouse\", 5, 350)\n",
    "]\n",
    "\n",
    "rdd_ventas = sc.parallelize(ventas_data)\n",
    "\n",
    "# Tu codigo aqui:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion: Analisis de ventas\n",
    "\n",
    "ventas_data = [\n",
    "    (\"Laptop\", 2, 15000),\n",
    "    (\"Mouse\", 10, 350),\n",
    "    (\"Laptop\", 1, 15000),\n",
    "    (\"Monitor\", 3, 8000),\n",
    "    (\"Mouse\", 5, 350)\n",
    "]\n",
    "\n",
    "rdd_ventas = sc.parallelize(ventas_data)\n",
    "\n",
    "# Paso 1: Calcular ingreso por venta y crear par (producto, ingreso)\n",
    "# venta[0] = producto, venta[1] = cantidad, venta[2] = precio\n",
    "rdd_ingresos = rdd_ventas.map(lambda v: (v[0], v[1] * v[2]))\n",
    "print(\"Ingresos por venta:\")\n",
    "for producto, ingreso in rdd_ingresos.collect():\n",
    "    print(f\"  {producto}: ${ingreso:,}\")\n",
    "\n",
    "# Paso 2: Sumar ingresos por producto\n",
    "ingresos_por_producto = rdd_ingresos.reduceByKey(lambda a, b: a + b)\n",
    "print(\"\\nIngreso total por producto:\")\n",
    "for producto, total in ingresos_por_producto.collect():\n",
    "    print(f\"  {producto}: ${total:,}\")\n",
    "\n",
    "# Paso 3: Encontrar el producto con mayor ingreso\n",
    "# max() con key=lambda x: x[1] compara por el segundo elemento (ingreso)\n",
    "mejor_producto = ingresos_por_producto.max(key=lambda x: x[1])\n",
    "print(f\"\\nProducto con mayor ingreso: {mejor_producto[0]} (${mejor_producto[1]:,})\")\n",
    "\n",
    "# Output esperado:\n",
    "# Ingreso total por producto:\n",
    "#   Laptop: $45,000\n",
    "#   Mouse: $5,250\n",
    "#   Monitor: $24,000\n",
    "# Producto con mayor ingreso: Laptop ($45,000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 6.3: Word Count Mejorado\n",
    "\n",
    "Mejora el Word Count para:\n",
    "1. Eliminar palabras de menos de 3 letras\n",
    "2. Eliminar signos de puntuaci√≥n\n",
    "3. Mostrar solo palabras que aparecen m√°s de 1 vez\n",
    "\n",
    "**Pistas:**\n",
    "- Usa `re` (expresiones regulares) o `str.isalpha()`\n",
    "- `len(palabra) >= 3` filtra por longitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa el ejercicio\n",
    "texto_ejercicio = \"\"\"\n",
    "Python es un lenguaje de programacion muy popular.\n",
    "Python se usa en ciencia de datos, web, y automatizacion.\n",
    "La comunidad de Python es muy grande y activa.\n",
    "Aprender Python es una gran decision para tu carrera.\n",
    "\"\"\"\n",
    "\n",
    "# Tu codigo aqui:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solucion: Word Count mejorado\n",
    "import re\n",
    "\n",
    "texto_ejercicio = \"\"\"\n",
    "Python es un lenguaje de programacion muy popular.\n",
    "Python se usa en ciencia de datos, web, y automatizacion.\n",
    "La comunidad de Python es muy grande y activa.\n",
    "Aprender Python es una gran decision para tu carrera.\n",
    "\"\"\"\n",
    "\n",
    "# Guardamos el texto\n",
    "ruta = \"/home/jovyan/data/sample/texto_python.txt\"\n",
    "with open(ruta, 'w') as f:\n",
    "    f.write(texto_ejercicio)\n",
    "\n",
    "def limpiar_palabra(palabra):\n",
    "    \"\"\"Elimina caracteres no alfabeticos y convierte a minusculas\"\"\"\n",
    "    # re.sub reemplaza todo lo que NO sea letra con vacio\n",
    "    return re.sub(r'[^a-z√°√©√≠√≥√∫√±]', '', palabra.lower())\n",
    "\n",
    "# Word Count mejorado\n",
    "resultado = sc.textFile(ruta) \\\n",
    "    .flatMap(lambda linea: linea.split()) \\\n",
    "    .map(limpiar_palabra) \\\n",
    "    .filter(lambda p: len(p) >= 3) \\\n",
    "    .map(lambda p: (p, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .filter(lambda x: x[1] > 1) \\\n",
    "    .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "print(\"Palabras que aparecen mas de 1 vez (sin palabras cortas):\")\n",
    "for palabra, conteo in resultado.collect():\n",
    "    print(f\"  '{palabra}': {conteo}\")\n",
    "\n",
    "# Output esperado:\n",
    "# 'python': 4\n",
    "# 'muy': 2\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## Resumen\n",
    "\n",
    "### Conceptos Clave\n",
    "- **RDD**: Colecci√≥n distribuida, resiliente e inmutable de datos\n",
    "- **Particiones**: Divisiones del RDD distribuidas en workers\n",
    "- **Transformaciones**: `map`, `filter`, `flatMap`, `reduceByKey` - son lazy\n",
    "- **Acciones**: `collect`, `count`, `reduce`, `take` - disparan ejecuci√≥n\n",
    "- **Pares clave-valor**: Patr√≥n fundamental para agregaciones\n",
    "\n",
    "### Conexi√≥n con AWS\n",
    "- **Amazon EMR**: Ejecuta jobs de Spark con RDDs en clusters grandes\n",
    "- **AWS Glue**: Usa DynamicFrames (similar a RDDs) internamente\n",
    "- **Amazon S3**: Los RDDs pueden leer/escribir directamente a S3\n",
    "\n",
    "### Siguiente Paso\n",
    "Contin√∫a con: `02_dataframes_api.ipynb` para aprender la API moderna de DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza de archivos temporales\n",
    "import os\n",
    "\n",
    "archivos = [\n",
    "    \"/home/jovyan/data/sample/ventas_rdd.txt\",\n",
    "    \"/home/jovyan/data/sample/texto_wordcount.txt\",\n",
    "    \"/home/jovyan/data/sample/texto_python.txt\"\n",
    "]\n",
    "\n",
    "for archivo in archivos:\n",
    "    if os.path.exists(archivo):\n",
    "        os.remove(archivo)\n",
    "\n",
    "print(\"Limpieza completada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
