{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Lab 01: Introducci√≥n a Big Data\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender qu√© es Big Data y por qu√© es importante\n",
    "- Entender las 5 Vs del Big Data con ejemplos del mundo real\n",
    "- Diferenciar entre datos estructurados, semi-estructurados y no estructurados\n",
    "- Crear tu primera SparkSession y cargar datos\n",
    "- Realizar operaciones b√°sicas de exploraci√≥n de datos\n",
    "\n",
    "## Prerequisitos\n",
    "- Lab 00: Setup del entorno completado\n",
    "- Docker Desktop corriendo con el cluster de Spark activo\n",
    "- Conocimientos b√°sicos de Python\n",
    "\n",
    "## ‚è±Ô∏è Tiempo Estimado\n",
    "**90-120 minutos**\n",
    "\n",
    "## üìö M√≥dulo AWS Academy Relacionado\n",
    "**M√≥dulo 3: Data Characteristics** - Las 5 Vs del Big Data, tipos de datos, ciclo de vida de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 1: ¬øQU√â ES BIG DATA? ===\n",
    "\n",
    "## 1. Introducci√≥n a Big Data\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "**Big Data** no es simplemente \"muchos datos\". Es un t√©rmino que describe conjuntos de datos tan grandes y complejos que las herramientas tradicionales (como Excel o bases de datos SQL simples) no pueden procesarlos eficientemente.\n",
    "\n",
    "#### üåç Analog√≠a del Mundo Real\n",
    "\n",
    "Imagina que tienes una biblioteca:\n",
    "- **Datos tradicionales**: Una biblioteca peque√±a con 1,000 libros. Puedes buscar manualmente.\n",
    "- **Big Data**: La Biblioteca del Congreso de EE.UU. con 170 millones de items. Necesitas sistemas automatizados.\n",
    "\n",
    "#### üìà ¬øCu√°ntos datos se generan?\n",
    "\n",
    "- **90% de los datos del mundo** se han creado en los √∫ltimos 2 a√±os\n",
    "- Cada d√≠a se generan **2.5 quintillones de bytes** de datos\n",
    "- Un carro aut√≥nomo genera **4 TB de datos por d√≠a**\n",
    "- Netflix procesa **500 mil millones de eventos diarios**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Librer√≠as b√°sicas importadas correctamente\n",
      "üìÖ Fecha de ejecuci√≥n: 2026-01-29 00:14\n"
     ]
    }
   ],
   "source": [
    "# === CELDA DE CONFIGURACI√ìN INICIAL ===\n",
    "# Esta celda configura el entorno y debe ejecutarse primero\n",
    "\n",
    "# Importamos las librer√≠as necesarias para este laboratorio\n",
    "import os                          # Para manejar rutas y variables de entorno\n",
    "import sys                         # Para acceder a informaci√≥n del sistema\n",
    "from datetime import datetime      # Para trabajar con fechas y horas\n",
    "import random                      # Para generar datos aleatorios de muestra\n",
    "\n",
    "# Agregamos la carpeta src al path para poder importar nuestros m√≥dulos\n",
    "sys.path.insert(0, '/home/jovyan/src')  # Ruta dentro del contenedor Docker\n",
    "\n",
    "# Mostramos mensaje de confirmaci√≥n\n",
    "print(\"‚úÖ Librer√≠as b√°sicas importadas correctamente\")\n",
    "print(f\"üìÖ Fecha de ejecuci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "\n",
    "# Output esperado:\n",
    "# ‚úÖ Librer√≠as b√°sicas importadas correctamente\n",
    "# üìÖ Fecha de ejecuci√≥n: 2026-01-28 10:30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 2: LAS 5 Vs DEL BIG DATA ===\n",
    "\n",
    "## 2. Las 5 Vs del Big Data\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "Las **5 Vs** son las caracter√≠sticas que definen Big Data. Son el \"ADN\" que diferencia Big Data de datos tradicionales.\n",
    "\n",
    "| V | Significado | Ejemplo del Mundo Real |\n",
    "|---|-------------|------------------------|\n",
    "| **Volume** | Cantidad de datos | Facebook almacena 300 PB de datos |\n",
    "| **Velocity** | Velocidad de generaci√≥n/procesamiento | Twitter: 500M tweets/d√≠a |\n",
    "| **Variety** | Diferentes tipos y formatos | Texto, im√°genes, video, sensores |\n",
    "| **Veracity** | Calidad y confiabilidad | ¬øLos datos son correctos? |\n",
    "| **Value** | Utilidad para el negocio | Insights que generan dinero |\n",
    "\n",
    "#### üöï Ejemplo con Taxis de NYC\n",
    "\n",
    "Los datos de taxis de Nueva York son un excelente ejemplo de Big Data:\n",
    "\n",
    "- **Volume**: 200+ millones de viajes por a√±o\n",
    "- **Velocity**: Miles de viajes inici√°ndose cada minuto\n",
    "- **Variety**: GPS, pagos, clima, tr√°fico\n",
    "- **Veracity**: Errores de GPS, datos faltantes\n",
    "- **Value**: Optimizar rutas, predecir demanda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä LAS 5 Vs DEL BIG DATA\n",
      "============================================================\n",
      "\n",
      "üî∑ VOLUME\n",
      "   Definici√≥n: Cantidad de datos generados y almacenados\n",
      "   Ejemplo Taxi: 1.5 millones de viajes en el dataset de entrenamiento\n",
      "   Unidades: Terabytes (TB), Petabytes (PB), Exabytes (EB)\n",
      "\n",
      "üî∑ VELOCITY\n",
      "   Definici√≥n: Velocidad a la que se generan y procesan los datos\n",
      "   Ejemplo Taxi: ~50,000 viajes por hora en hora pico\n",
      "   Unidades: Eventos por segundo, registros por minuto\n",
      "\n",
      "üî∑ VARIETY\n",
      "   Definici√≥n: Diferentes tipos y formatos de datos\n",
      "   Ejemplo Taxi: GPS (coordenadas), texto (direcciones), n√∫meros (tarifas)\n",
      "   Unidades: CSV, JSON, Parquet, im√°genes, video\n",
      "\n",
      "üî∑ VERACITY\n",
      "   Definici√≥n: Calidad, precisi√≥n y confiabilidad de los datos\n",
      "   Ejemplo Taxi: Coordenadas GPS incorrectas, duraciones negativas\n",
      "   Unidades: Porcentaje de datos v√°lidos, tasa de errores\n",
      "\n",
      "üî∑ VALUE\n",
      "   Definici√≥n: Valor de negocio que se puede extraer\n",
      "   Ejemplo Taxi: Predecir demanda, optimizar precios, reducir tiempos\n",
      "   Unidades: ROI, ahorro de costos, incremento de ingresos\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === VISUALIZACI√ìN DE LAS 5 Vs ===\n",
    "# Creamos un diccionario que representa las 5 Vs con ejemplos concretos\n",
    "\n",
    "# Definimos las 5 Vs como un diccionario de Python\n",
    "cinco_vs = {\n",
    "    \"Volume\": {\n",
    "        \"definicion\": \"Cantidad de datos generados y almacenados\",\n",
    "        \"ejemplo_taxi\": \"1.5 millones de viajes en el dataset de entrenamiento\",\n",
    "        \"unidades\": \"Terabytes (TB), Petabytes (PB), Exabytes (EB)\"\n",
    "    },\n",
    "    \"Velocity\": {\n",
    "        \"definicion\": \"Velocidad a la que se generan y procesan los datos\",\n",
    "        \"ejemplo_taxi\": \"~50,000 viajes por hora en hora pico\",\n",
    "        \"unidades\": \"Eventos por segundo, registros por minuto\"\n",
    "    },\n",
    "    \"Variety\": {\n",
    "        \"definicion\": \"Diferentes tipos y formatos de datos\",\n",
    "        \"ejemplo_taxi\": \"GPS (coordenadas), texto (direcciones), n√∫meros (tarifas)\",\n",
    "        \"unidades\": \"CSV, JSON, Parquet, im√°genes, video\"\n",
    "    },\n",
    "    \"Veracity\": {\n",
    "        \"definicion\": \"Calidad, precisi√≥n y confiabilidad de los datos\",\n",
    "        \"ejemplo_taxi\": \"Coordenadas GPS incorrectas, duraciones negativas\",\n",
    "        \"unidades\": \"Porcentaje de datos v√°lidos, tasa de errores\"\n",
    "    },\n",
    "    \"Value\": {\n",
    "        \"definicion\": \"Valor de negocio que se puede extraer\",\n",
    "        \"ejemplo_taxi\": \"Predecir demanda, optimizar precios, reducir tiempos\",\n",
    "        \"unidades\": \"ROI, ahorro de costos, incremento de ingresos\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Imprimimos cada V con formato legible\n",
    "print(\"=\" * 60)  # L√≠nea separadora de 60 caracteres\n",
    "print(\"üìä LAS 5 Vs DEL BIG DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Iteramos sobre cada V para mostrar su informaci√≥n\n",
    "for v_nombre, v_info in cinco_vs.items():  # v_nombre es la clave, v_info es el valor\n",
    "    print(f\"\\nüî∑ {v_nombre.upper()}\")      # Mostramos el nombre en may√∫sculas\n",
    "    print(f\"   Definici√≥n: {v_info['definicion']}\")\n",
    "    print(f\"   Ejemplo Taxi: {v_info['ejemplo_taxi']}\")\n",
    "    print(f\"   Unidades: {v_info['unidades']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Output esperado:\n",
    "# ============================================================\n",
    "# üìä LAS 5 Vs DEL BIG DATA\n",
    "# ============================================================\n",
    "# \n",
    "# üî∑ VOLUME\n",
    "#    Definici√≥n: Cantidad de datos generados y almacenados\n",
    "#    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 3: TIPOS DE DATOS ===\n",
    "\n",
    "## 3. Tipos de Datos en Big Data\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "Los datos se clasifican en tres categor√≠as principales seg√∫n su estructura:\n",
    "\n",
    "#### üìã Datos Estructurados (~10-20% del total)\n",
    "- Tienen un **esquema fijo** (columnas definidas)\n",
    "- Se almacenan en **tablas** (filas y columnas)\n",
    "- Ejemplos: bases de datos SQL, hojas de Excel\n",
    "- **En AWS**: Amazon RDS, Amazon Redshift\n",
    "\n",
    "#### üìÑ Datos Semi-estructurados (~5-10%)\n",
    "- Tienen **alguna organizaci√≥n** pero no esquema r√≠gido\n",
    "- Usan etiquetas o marcadores\n",
    "- Ejemplos: JSON, XML, logs de servidores\n",
    "- **En AWS**: Amazon DynamoDB, Amazon DocumentDB\n",
    "\n",
    "#### üé≠ Datos No Estructurados (~80%+)\n",
    "- **Sin formato predefinido**\n",
    "- Dif√≠ciles de procesar con herramientas tradicionales\n",
    "- Ejemplos: emails, videos, im√°genes, redes sociales\n",
    "- **En AWS**: Amazon S3, Amazon Comprehend (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EJEMPLOS DE TIPOS DE DATOS ===\n",
    "# Mostramos ejemplos concretos de cada tipo de datos\n",
    "\n",
    "# 1. DATOS ESTRUCTURADOS - Como una tabla de Excel\n",
    "print(\"üìã DATOS ESTRUCTURADOS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ejemplo: registro de un viaje de taxi (como fila de tabla)\n",
    "viaje_estructurado = {\n",
    "    \"trip_id\": \"T001\",                    # Identificador √∫nico del viaje\n",
    "    \"pickup_datetime\": \"2024-01-15 08:30\", # Fecha/hora de recogida\n",
    "    \"pickup_latitude\": 40.7589,            # Latitud del punto de recogida\n",
    "    \"pickup_longitude\": -73.9851,          # Longitud del punto de recogida\n",
    "    \"passenger_count\": 2,                  # N√∫mero de pasajeros\n",
    "    \"trip_duration_seconds\": 1245          # Duraci√≥n en segundos\n",
    "}\n",
    "\n",
    "# Mostramos cada campo del registro estructurado\n",
    "for campo, valor in viaje_estructurado.items():\n",
    "    print(f\"  {campo}: {valor}\")\n",
    "\n",
    "print(\"\\n‚úÖ Caracter√≠sticas: esquema fijo, tipos de datos definidos, f√°cil de consultar con SQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DATOS SEMI-ESTRUCTURADOS - Como JSON o XML\n",
    "print(\"üìÑ DATOS SEMI-ESTRUCTURADOS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ejemplo: evento de un viaje con datos anidados (JSON)\n",
    "viaje_semi_estructurado = {\n",
    "    \"event_type\": \"trip_completed\",        # Tipo de evento\n",
    "    \"timestamp\": \"2024-01-15T09:00:00Z\",  # Marca de tiempo ISO 8601\n",
    "    \"trip\": {                              # Objeto anidado con datos del viaje\n",
    "        \"id\": \"T001\",\n",
    "        \"duration\": 1245\n",
    "    },\n",
    "    \"driver\": {                            # Objeto anidado con datos del conductor\n",
    "        \"id\": \"D100\",\n",
    "        \"rating\": 4.8,\n",
    "        \"vehicle\": {                       # Objeto anidado dentro de otro\n",
    "            \"make\": \"Toyota\",\n",
    "            \"model\": \"Camry\",\n",
    "            \"year\": 2022\n",
    "        }\n",
    "    },\n",
    "    \"tags\": [\"airport\", \"business\", \"cash\"]  # Array de etiquetas (flexible)\n",
    "}\n",
    "\n",
    "# Importamos json para mostrar con formato bonito\n",
    "import json\n",
    "print(json.dumps(viaje_semi_estructurado, indent=2))  # indent=2 para indentar con 2 espacios\n",
    "\n",
    "print(\"\\n‚úÖ Caracter√≠sticas: estructura flexible, campos opcionales, datos anidados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DATOS NO ESTRUCTURADOS - Texto libre, sin formato\n",
    "print(\"üé≠ DATOS NO ESTRUCTURADOS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Ejemplo: comentario de un pasajero (texto libre)\n",
    "comentario_pasajero = \"\"\"\n",
    "El conductor fue muy amable y conoc√≠a bien la ciudad. \n",
    "Llegamos 5 minutos antes de lo esperado a pesar del tr√°fico \n",
    "en la 5ta Avenida. El auto estaba limpio y ten√≠a aire acondicionado.\n",
    "Definitivamente lo recomiendo! ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Comentario del pasajero:{comentario_pasajero}\")\n",
    "\n",
    "# Mostramos estad√≠sticas b√°sicas del texto\n",
    "num_palabras = len(comentario_pasajero.split())     # Contamos palabras\n",
    "num_caracteres = len(comentario_pasajero)           # Contamos caracteres\n",
    "tiene_emojis = \"‚≠ê\" in comentario_pasajero          # Verificamos si hay emojis\n",
    "\n",
    "print(f\"üìä Estad√≠sticas del texto:\")\n",
    "print(f\"   Palabras: {num_palabras}\")\n",
    "print(f\"   Caracteres: {num_caracteres}\")\n",
    "print(f\"   Contiene emojis: {tiene_emojis}\")\n",
    "\n",
    "print(\"\\n‚úÖ Caracter√≠sticas: sin esquema, dif√≠cil de procesar, requiere NLP para an√°lisis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 4: INTRODUCCI√ìN A PYSPARK ===\n",
    "\n",
    "## 4. Primeros Pasos con PySpark\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "**Apache Spark** es un motor de procesamiento distribuido para Big Data. **PySpark** es su interfaz para Python.\n",
    "\n",
    "#### üåç Analog√≠a del Mundo Real\n",
    "\n",
    "Imagina que necesitas contar todos los libros de una biblioteca gigante:\n",
    "- **Sin Spark**: Una persona cuenta libro por libro (lento)\n",
    "- **Con Spark**: 100 personas se dividen la biblioteca y cuentan en paralelo (r√°pido)\n",
    "\n",
    "#### üîë Conceptos Clave\n",
    "\n",
    "| Concepto | Descripci√≥n | Analog√≠a |\n",
    "|----------|-------------|----------|\n",
    "| **SparkSession** | Punto de entrada a Spark | La puerta de entrada al edificio |\n",
    "| **DataFrame** | Tabla de datos distribuida | Una hoja de Excel gigante |\n",
    "| **Transformaci√≥n** | Operaci√≥n que crea nuevo DataFrame | Filtrar o modificar datos |\n",
    "| **Acci√≥n** | Operaci√≥n que devuelve resultado | Contar, mostrar, guardar |\n",
    "\n",
    "#### üîó Conexi√≥n con AWS\n",
    "- **Amazon EMR**: Servicio administrado para ejecutar Spark en la nube\n",
    "- **AWS Glue**: ETL serverless que usa Spark internamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREAR SPARKSESSION ===\n",
    "# SparkSession es el punto de entrada para usar PySpark\n",
    "# Es como \"encender\" el motor de Spark\n",
    "\n",
    "# Importamos SparkSession desde pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos (o obtenemos si ya existe) una SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab01_BigData_Fundamentals\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Explicaci√≥n de cada configuraci√≥n:\n",
    "# .appName() -> Nombre de nuestra aplicaci√≥n (aparece en la UI de Spark)\n",
    "# .master()  -> URL del cluster Spark (spark-master es el nombre del contenedor)\n",
    "# .config()  -> Configuraciones adicionales (memoria asignada)\n",
    "# .getOrCreate() -> Obtiene sesi√≥n existente o crea una nueva\n",
    "\n",
    "# Verificamos que Spark est√° funcionando\n",
    "print(\"‚úÖ SparkSession creada correctamente!\")\n",
    "print(f\"üìå Versi√≥n de Spark: {spark.version}\")\n",
    "print(f\"üìå App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"üìå Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Output esperado:\n",
    "# ‚úÖ SparkSession creada correctamente!\n",
    "# üìå Versi√≥n de Spark: 3.5.0\n",
    "# üìå App Name: Lab01_BigData_Fundamentals\n",
    "# üìå Master: spark://spark-master:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERAR DATOS DE MUESTRA ===\n",
    "# Creamos un dataset peque√±o que simula viajes de taxi\n",
    "# Esto nos permite practicar sin necesidad del dataset completo\n",
    "\n",
    "import random                    # Para generar valores aleatorios\n",
    "from datetime import datetime, timedelta  # Para manejar fechas\n",
    "\n",
    "# Semilla para reproducibilidad (mismos resultados cada vez)\n",
    "random.seed(42)\n",
    "\n",
    "# Funci√≥n para generar un viaje de taxi aleatorio\n",
    "def generar_viaje(trip_id):\n",
    "    \"\"\"\n",
    "    Genera un registro de viaje de taxi con datos realistas.\n",
    "    \n",
    "    Par√°metros:\n",
    "        trip_id: Identificador √∫nico del viaje\n",
    "    \n",
    "    Retorna:\n",
    "        Diccionario con los datos del viaje\n",
    "    \"\"\"\n",
    "    # Fecha base: 15 de enero 2024\n",
    "    fecha_base = datetime(2024, 1, 15)\n",
    "    \n",
    "    # Generamos hora aleatoria (entre 0 y 23 horas)\n",
    "    hora_pickup = random.randint(0, 23)\n",
    "    \n",
    "    # Minutos aleatorios (0-59)\n",
    "    minutos_pickup = random.randint(0, 59)\n",
    "    \n",
    "    # Construimos la fecha/hora de recogida\n",
    "    pickup_datetime = fecha_base.replace(hour=hora_pickup, minute=minutos_pickup)\n",
    "    \n",
    "    # Duraci√≥n del viaje: entre 3 y 60 minutos (en segundos)\n",
    "    trip_duration = random.randint(180, 3600)\n",
    "    \n",
    "    # Coordenadas de NYC (Manhattan aproximado)\n",
    "    # Latitud: 40.70 a 40.85\n",
    "    pickup_lat = round(random.uniform(40.70, 40.85), 6)\n",
    "    pickup_lon = round(random.uniform(-74.02, -73.93), 6)\n",
    "    dropoff_lat = round(random.uniform(40.70, 40.85), 6)\n",
    "    dropoff_lon = round(random.uniform(-74.02, -73.93), 6)\n",
    "    \n",
    "    # N√∫mero de pasajeros: entre 1 y 6\n",
    "    passenger_count = random.randint(1, 6)\n",
    "    \n",
    "    # Vendor ID: 1 o 2 (dos compa√±√≠as de taxi)\n",
    "    vendor_id = random.choice([1, 2])\n",
    "    \n",
    "    # Retornamos el viaje como diccionario\n",
    "    return {\n",
    "        \"id\": f\"trip_{trip_id:05d}\",  # Formato: trip_00001\n",
    "        \"vendor_id\": vendor_id,\n",
    "        \"pickup_datetime\": pickup_datetime.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"passenger_count\": passenger_count,\n",
    "        \"pickup_longitude\": pickup_lon,\n",
    "        \"pickup_latitude\": pickup_lat,\n",
    "        \"dropoff_longitude\": dropoff_lon,\n",
    "        \"dropoff_latitude\": dropoff_lat,\n",
    "        \"trip_duration\": trip_duration\n",
    "    }\n",
    "\n",
    "# Generamos 1000 viajes de muestra\n",
    "num_viajes = 1000\n",
    "datos_viajes = [generar_viaje(i) for i in range(1, num_viajes + 1)]\n",
    "\n",
    "# Mostramos los primeros 3 viajes generados\n",
    "print(f\"‚úÖ Generados {num_viajes} viajes de muestra\")\n",
    "print(\"\\nüìã Primeros 3 viajes:\")\n",
    "for viaje in datos_viajes[:3]:\n",
    "    print(viaje)\n",
    "\n",
    "# Output esperado:\n",
    "# ‚úÖ Generados 1000 viajes de muestra\n",
    "# \n",
    "# üìã Primeros 3 viajes:\n",
    "# {'id': 'trip_00001', 'vendor_id': 1, 'pickup_datetime': '2024-01-15 06:24:00', ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CREAR DATAFRAME DE SPARK ===\n",
    "# Convertimos nuestra lista de diccionarios a un DataFrame de Spark\n",
    "\n",
    "# createDataFrame convierte datos de Python a formato distribuido de Spark\n",
    "df_viajes = spark.createDataFrame(datos_viajes)\n",
    "\n",
    "# Mostramos informaci√≥n b√°sica del DataFrame\n",
    "print(\"‚úÖ DataFrame creado correctamente!\")\n",
    "print(f\"\\nüìä N√∫mero de registros: {df_viajes.count()}\")\n",
    "print(f\"üìä N√∫mero de columnas: {len(df_viajes.columns)}\")\n",
    "\n",
    "# Mostramos el esquema (estructura) del DataFrame\n",
    "print(\"\\nüìã Esquema del DataFrame:\")\n",
    "df_viajes.printSchema()\n",
    "\n",
    "# Output esperado:\n",
    "# ‚úÖ DataFrame creado correctamente!\n",
    "# \n",
    "# üìä N√∫mero de registros: 1000\n",
    "# üìä N√∫mero de columnas: 9\n",
    "# \n",
    "# üìã Esquema del DataFrame:\n",
    "# root\n",
    "#  |-- id: string (nullable = true)\n",
    "#  |-- vendor_id: long (nullable = true)\n",
    "#  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXPLORACI√ìN B√ÅSICA DEL DATAFRAME ===\n",
    "# Aprendemos las operaciones m√°s comunes para explorar datos\n",
    "\n",
    "# 1. show() - Muestra las primeras filas (por defecto 20)\n",
    "print(\"üìã Primeras 5 filas del DataFrame:\")\n",
    "df_viajes.show(5, truncate=False)  # truncate=False muestra valores completos\n",
    "\n",
    "# 2. describe() - Estad√≠sticas descriptivas de columnas num√©ricas\n",
    "print(\"\\nüìä Estad√≠sticas descriptivas:\")\n",
    "df_viajes.describe().show()\n",
    "\n",
    "# Output esperado:\n",
    "# üìã Primeras 5 filas del DataFrame:\n",
    "# +------------+---------+-------------------+---------------+...\n",
    "# |id          |vendor_id|pickup_datetime    |passenger_count|...\n",
    "# +------------+---------+-------------------+---------------+...\n",
    "# |trip_00001  |1        |2024-01-15 06:24:00|4              |...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPERACIONES B√ÅSICAS CON DATAFRAMES ===\n",
    "# Aprendemos a filtrar, seleccionar y ordenar datos\n",
    "\n",
    "# Importamos funciones de Spark SQL\n",
    "from pyspark.sql.functions import col, avg, count, min, max\n",
    "\n",
    "# 1. SELECT - Seleccionar columnas espec√≠ficas\n",
    "print(\"üìã 1. Seleccionar columnas espec√≠ficas:\")\n",
    "df_viajes.select(\"id\", \"passenger_count\", \"trip_duration\").show(5)\n",
    "\n",
    "# 2. FILTER/WHERE - Filtrar filas seg√∫n condici√≥n\n",
    "print(\"üìã 2. Viajes con m√°s de 3 pasajeros:\")\n",
    "df_viajes.filter(col(\"passenger_count\") > 3).show(5)\n",
    "\n",
    "# 3. ORDER BY - Ordenar por una columna\n",
    "print(\"üìã 3. Top 5 viajes m√°s largos (por duraci√≥n):\")\n",
    "df_viajes.orderBy(col(\"trip_duration\").desc()).show(5)\n",
    "\n",
    "# 4. GROUP BY - Agrupar y agregar\n",
    "print(\"üìã 4. Promedio de pasajeros por vendor:\")\n",
    "df_viajes.groupBy(\"vendor_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_viajes\"),       # Contar viajes\n",
    "        avg(\"passenger_count\").alias(\"avg_pasajeros\"),  # Promedio pasajeros\n",
    "        avg(\"trip_duration\").alias(\"avg_duracion\")      # Promedio duraci√≥n\n",
    "    ) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 5: AN√ÅLISIS DEL DATASET ===\n",
    "\n",
    "## 5. An√°lisis Exploratorio de Datos (EDA)\n",
    "\n",
    "### Explicaci√≥n Conceptual\n",
    "\n",
    "El **An√°lisis Exploratorio de Datos (EDA)** es el primer paso en cualquier proyecto de datos. Nos ayuda a:\n",
    "\n",
    "1. **Entender** la estructura y contenido de los datos\n",
    "2. **Identificar** problemas de calidad (valores nulos, outliers)\n",
    "3. **Descubrir** patrones y relaciones iniciales\n",
    "4. **Formular** hip√≥tesis para an√°lisis posteriores\n",
    "\n",
    "#### üîó Conexi√≥n con AWS\n",
    "- **AWS Glue DataBrew**: Herramienta visual para EDA y limpieza de datos\n",
    "- **Amazon Athena**: Consultas SQL para explorar datos en S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AN√ÅLISIS DE CALIDAD DE DATOS ===\n",
    "# Verificamos la calidad de nuestros datos (Veracity - una de las 5 Vs)\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
    "\n",
    "print(\"üìä AN√ÅLISIS DE CALIDAD DE DATOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Contar valores nulos por columna\n",
    "print(\"\\n1Ô∏è‚É£ Valores nulos por columna:\")\n",
    "\n",
    "# Creamos expresiones para contar nulos en cada columna\n",
    "null_counts = df_viajes.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in df_viajes.columns]\n",
    ")\n",
    "null_counts.show()\n",
    "\n",
    "# 2. Verificar valores fuera de rango (outliers)\n",
    "print(\"2Ô∏è‚É£ Verificaci√≥n de rangos v√°lidos:\")\n",
    "\n",
    "# Viajes con duraci√≥n negativa o cero (error de datos)\n",
    "viajes_invalidos = df_viajes.filter(col(\"trip_duration\") <= 0).count()\n",
    "print(f\"   Viajes con duraci√≥n <= 0: {viajes_invalidos}\")\n",
    "\n",
    "# Viajes con m√°s de 6 pasajeros (m√°ximo t√≠pico de un taxi)\n",
    "viajes_muchos_pasajeros = df_viajes.filter(col(\"passenger_count\") > 6).count()\n",
    "print(f\"   Viajes con m√°s de 6 pasajeros: {viajes_muchos_pasajeros}\")\n",
    "\n",
    "# Viajes muy largos (m√°s de 2 horas = 7200 segundos)\n",
    "viajes_muy_largos = df_viajes.filter(col(\"trip_duration\") > 7200).count()\n",
    "print(f\"   Viajes de m√°s de 2 horas: {viajes_muy_largos}\")\n",
    "\n",
    "# 3. Resumen de calidad\n",
    "total_viajes = df_viajes.count()\n",
    "print(f\"\\n‚úÖ Resumen de calidad:\")\n",
    "print(f\"   Total de registros: {total_viajes}\")\n",
    "print(f\"   Registros v√°lidos: {total_viajes - viajes_invalidos}\")\n",
    "print(f\"   Porcentaje v√°lido: {((total_viajes - viajes_invalidos) / total_viajes) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === AN√ÅLISIS POR HORA DEL D√çA ===\n",
    "# Extraemos insights sobre patrones temporales\n",
    "\n",
    "from pyspark.sql.functions import hour, to_timestamp\n",
    "\n",
    "print(\"üìä AN√ÅLISIS TEMPORAL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Convertimos pickup_datetime a timestamp y extraemos la hora\n",
    "df_con_hora = df_viajes.withColumn(\n",
    "    \"hora_pickup\",                                    # Nombre de la nueva columna\n",
    "    hour(to_timestamp(col(\"pickup_datetime\")))        # Extraemos la hora (0-23)\n",
    ")\n",
    "\n",
    "# Agrupamos por hora y contamos viajes\n",
    "print(\"\\nüïê Viajes por hora del d√≠a:\")\n",
    "viajes_por_hora = df_con_hora.groupBy(\"hora_pickup\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_viajes\"),\n",
    "        avg(\"trip_duration\").alias(\"duracion_promedio\")\n",
    "    ) \\\n",
    "    .orderBy(\"hora_pickup\")\n",
    "\n",
    "viajes_por_hora.show(24)  # Mostramos las 24 horas\n",
    "\n",
    "# Identificamos horas pico\n",
    "print(\"\\nüîù Top 3 horas con m√°s viajes (hora pico):\")\n",
    "viajes_por_hora.orderBy(col(\"total_viajes\").desc()).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VISUALIZACI√ìN B√ÅSICA ===\n",
    "# Creamos gr√°ficos simples para entender los datos\n",
    "\n",
    "import matplotlib.pyplot as plt  # Librer√≠a de visualizaci√≥n\n",
    "\n",
    "# Convertimos datos de Spark a Pandas para graficar\n",
    "# NOTA: Solo hacemos esto con datos peque√±os (nuestro resumen)\n",
    "df_horas_pandas = viajes_por_hora.toPandas()  # Convertir a Pandas DataFrame\n",
    "\n",
    "# Creamos figura con 2 gr√°ficos\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # 1 fila, 2 columnas\n",
    "\n",
    "# Gr√°fico 1: Viajes por hora (barras)\n",
    "axes[0].bar(\n",
    "    df_horas_pandas['hora_pickup'],      # Eje X: horas\n",
    "    df_horas_pandas['total_viajes'],     # Eje Y: cantidad de viajes\n",
    "    color='steelblue'                    # Color de las barras\n",
    ")\n",
    "axes[0].set_xlabel('Hora del d√≠a')       # Etiqueta eje X\n",
    "axes[0].set_ylabel('N√∫mero de viajes')   # Etiqueta eje Y\n",
    "axes[0].set_title('üìä Viajes por Hora del D√≠a')  # T√≠tulo\n",
    "axes[0].set_xticks(range(0, 24))         # Mostrar todas las horas\n",
    "\n",
    "# Gr√°fico 2: Duraci√≥n promedio por hora (l√≠nea)\n",
    "axes[1].plot(\n",
    "    df_horas_pandas['hora_pickup'],           # Eje X: horas\n",
    "    df_horas_pandas['duracion_promedio'],     # Eje Y: duraci√≥n promedio\n",
    "    marker='o',                               # Marcador circular en cada punto\n",
    "    color='coral',                            # Color de la l√≠nea\n",
    "    linewidth=2                               # Grosor de l√≠nea\n",
    ")\n",
    "axes[1].set_xlabel('Hora del d√≠a')\n",
    "axes[1].set_ylabel('Duraci√≥n promedio (segundos)')\n",
    "axes[1].set_title('‚è±Ô∏è Duraci√≥n Promedio por Hora')\n",
    "axes[1].set_xticks(range(0, 24))\n",
    "\n",
    "# Ajustamos el layout y mostramos\n",
    "plt.tight_layout()  # Evita que se superpongan los gr√°ficos\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Observaciones:\")\n",
    "print(\"   - Los datos de muestra est√°n distribuidos uniformemente (generados aleatoriamente)\")\n",
    "print(\"   - Con datos reales, ver√≠amos patrones de hora pico (8-9 AM, 5-7 PM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === SECCI√ìN 6: EJERCICIOS PR√ÅCTICOS ===\n",
    "\n",
    "## üéØ Ejercicios\n",
    "\n",
    "Ahora es tu turno de practicar. Completa los siguientes ejercicios usando lo que aprendiste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 1: An√°lisis de Pasajeros\n",
    "\n",
    "**Objetivo**: Analizar la distribuci√≥n de pasajeros por viaje.\n",
    "\n",
    "**Instrucciones**:\n",
    "1. Cuenta cu√°ntos viajes hay para cada cantidad de pasajeros (1, 2, 3, etc.)\n",
    "2. Ordena el resultado de mayor a menor cantidad de viajes\n",
    "3. Calcula el porcentaje de viajes para cada cantidad de pasajeros\n",
    "\n",
    "**Pistas**:\n",
    "- Usa `groupBy(\"passenger_count\")` para agrupar\n",
    "- Usa `count(\"*\")` para contar\n",
    "- Para calcular porcentaje, divide entre el total y multiplica por 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ EJERCICIO 1: Tu c√≥digo aqu√≠\n",
    "# Completa el c√≥digo para analizar la distribuci√≥n de pasajeros\n",
    "\n",
    "# TODO: Agrupa por passenger_count y cuenta los viajes\n",
    "# viajes_por_pasajeros = df_viajes.groupBy(...)\n",
    "\n",
    "# TODO: Ordena de mayor a menor cantidad de viajes\n",
    "\n",
    "# TODO: Muestra el resultado\n",
    "\n",
    "print(\"Tu soluci√≥n aqu√≠...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ SOLUCI√ìN EJERCICIO 1: An√°lisis de Pasajeros\n",
    "\n",
    "from pyspark.sql.functions import col, count, round as spark_round\n",
    "\n",
    "# Obtenemos el total de viajes para calcular porcentajes\n",
    "total_viajes = df_viajes.count()  # Contamos todos los viajes\n",
    "\n",
    "# Agrupamos por n√∫mero de pasajeros y contamos\n",
    "viajes_por_pasajeros = df_viajes.groupBy(\"passenger_count\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_viajes\")  # Contamos viajes en cada grupo\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"porcentaje\",  # Nueva columna con el porcentaje\n",
    "        spark_round((col(\"total_viajes\") / total_viajes) * 100, 2)  # Redondeamos a 2 decimales\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_viajes\").desc())  # Ordenamos de mayor a menor\n",
    "\n",
    "# Mostramos el resultado\n",
    "print(\"üìä Distribuci√≥n de viajes por n√∫mero de pasajeros:\")\n",
    "print(f\"   (Total de viajes: {total_viajes})\\n\")\n",
    "viajes_por_pasajeros.show()\n",
    "\n",
    "# Output esperado:\n",
    "# üìä Distribuci√≥n de viajes por n√∫mero de pasajeros:\n",
    "#    (Total de viajes: 1000)\n",
    "# \n",
    "# +---------------+------------+----------+\n",
    "# |passenger_count|total_viajes|porcentaje|\n",
    "# +---------------+------------+----------+\n",
    "# |              1|         178|     17.80|\n",
    "# |              4|         174|     17.40|\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 2: Filtrado de Viajes Cortos y Largos\n",
    "\n",
    "**Objetivo**: Identificar viajes cortos (menos de 5 minutos) y viajes largos (m√°s de 45 minutos).\n",
    "\n",
    "**Instrucciones**:\n",
    "1. Filtra los viajes con duraci√≥n menor a 300 segundos (5 minutos)\n",
    "2. Filtra los viajes con duraci√≥n mayor a 2700 segundos (45 minutos)\n",
    "3. Cuenta cu√°ntos hay de cada tipo\n",
    "4. Muestra 3 ejemplos de cada categor√≠a\n",
    "\n",
    "**Pistas**:\n",
    "- Usa `filter(col(\"trip_duration\") < 300)` para filtrar\n",
    "- Recuerda que la duraci√≥n est√° en segundos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ EJERCICIO 2: Tu c√≥digo aqu√≠\n",
    "# Completa el c√≥digo para identificar viajes cortos y largos\n",
    "\n",
    "# TODO: Filtra viajes cortos (< 5 minutos = 300 segundos)\n",
    "# viajes_cortos = df_viajes.filter(...)\n",
    "\n",
    "# TODO: Filtra viajes largos (> 45 minutos = 2700 segundos)\n",
    "# viajes_largos = df_viajes.filter(...)\n",
    "\n",
    "# TODO: Cuenta y muestra resultados\n",
    "\n",
    "print(\"Tu soluci√≥n aqu√≠...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ SOLUCI√ìN EJERCICIO 2: Viajes Cortos y Largos\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Definimos los umbrales (en segundos)\n",
    "UMBRAL_CORTO = 300    # 5 minutos = 300 segundos\n",
    "UMBRAL_LARGO = 2700   # 45 minutos = 2700 segundos\n",
    "\n",
    "# Filtramos viajes cortos (menos de 5 minutos)\n",
    "viajes_cortos = df_viajes.filter(col(\"trip_duration\") < UMBRAL_CORTO)\n",
    "\n",
    "# Filtramos viajes largos (m√°s de 45 minutos)\n",
    "viajes_largos = df_viajes.filter(col(\"trip_duration\") > UMBRAL_LARGO)\n",
    "\n",
    "# Contamos cada categor√≠a\n",
    "num_cortos = viajes_cortos.count()  # Cantidad de viajes cortos\n",
    "num_largos = viajes_largos.count()  # Cantidad de viajes largos\n",
    "num_normales = df_viajes.count() - num_cortos - num_largos  # El resto\n",
    "\n",
    "# Mostramos resumen\n",
    "print(\"üìä CLASIFICACI√ìN DE VIAJES POR DURACI√ìN\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nüèÉ Viajes cortos (<5 min): {num_cortos}\")\n",
    "print(f\"üö∂ Viajes normales (5-45 min): {num_normales}\")\n",
    "print(f\"üê¢ Viajes largos (>45 min): {num_largos}\")\n",
    "\n",
    "# Mostramos ejemplos de cada categor√≠a\n",
    "print(\"\\nüìã Ejemplos de viajes CORTOS:\")\n",
    "viajes_cortos.select(\"id\", \"trip_duration\", \"passenger_count\").show(3)\n",
    "\n",
    "print(\"üìã Ejemplos de viajes LARGOS:\")\n",
    "viajes_largos.select(\"id\", \"trip_duration\", \"passenger_count\").show(3)\n",
    "\n",
    "# Output esperado:\n",
    "# üìä CLASIFICACI√ìN DE VIAJES POR DURACI√ìN\n",
    "# ==================================================\n",
    "# \n",
    "# üèÉ Viajes cortos (<5 min): ~35\n",
    "# üö∂ Viajes normales (5-45 min): ~900\n",
    "# üê¢ Viajes largos (>45 min): ~65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Ejercicio 3: Crear Nueva Columna Calculada\n",
    "\n",
    "**Objetivo**: Calcular la distancia aproximada del viaje usando las coordenadas.\n",
    "\n",
    "**Instrucciones**:\n",
    "1. Crea una nueva columna `distancia_euclidiana` que calcule la distancia aproximada\n",
    "2. La f√≥rmula simplificada es: `sqrt((lat2-lat1)^2 + (lon2-lon1)^2) * 111` km\n",
    "3. Agrupa por vendor_id y calcula la distancia promedio\n",
    "\n",
    "**Pistas**:\n",
    "- Usa `withColumn()` para agregar columnas\n",
    "- Importa `sqrt` y `pow` de `pyspark.sql.functions`\n",
    "- 1 grado ‚âà 111 km (aproximaci√≥n para NYC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ EJERCICIO 3: Tu c√≥digo aqu√≠\n",
    "# Completa el c√≥digo para calcular la distancia aproximada\n",
    "\n",
    "# TODO: Importar funciones necesarias (sqrt, pow)\n",
    "# from pyspark.sql.functions import sqrt, pow\n",
    "\n",
    "# TODO: Crear columna de distancia\n",
    "# df_con_distancia = df_viajes.withColumn(\"distancia_km\", ...)\n",
    "\n",
    "# TODO: Mostrar estad√≠sticas de distancia por vendor\n",
    "\n",
    "print(\"Tu soluci√≥n aqu√≠...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Soluci√≥n Ejercicio 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ SOLUCI√ìN EJERCICIO 3: Distancia Aproximada\n",
    "\n",
    "from pyspark.sql.functions import col, sqrt, pow as spark_pow, avg, round as spark_round\n",
    "\n",
    "# Factor de conversi√≥n: 1 grado ‚âà 111 km (aproximaci√≥n para latitudes medias)\n",
    "KM_POR_GRADO = 111\n",
    "\n",
    "# Calculamos la diferencia de coordenadas\n",
    "df_con_distancia = df_viajes.withColumn(\n",
    "    \"diff_lat\",  # Diferencia de latitud\n",
    "    col(\"dropoff_latitude\") - col(\"pickup_latitude\")\n",
    ").withColumn(\n",
    "    \"diff_lon\",  # Diferencia de longitud\n",
    "    col(\"dropoff_longitude\") - col(\"pickup_longitude\")\n",
    ").withColumn(\n",
    "    \"distancia_km\",  # Distancia euclidiana en km\n",
    "    spark_round(\n",
    "        sqrt(\n",
    "            spark_pow(col(\"diff_lat\"), 2) +  # (lat2 - lat1)^2\n",
    "            spark_pow(col(\"diff_lon\"), 2)    # (lon2 - lon1)^2\n",
    "        ) * KM_POR_GRADO,  # Multiplicamos por factor de conversi√≥n\n",
    "        2  # Redondeamos a 2 decimales\n",
    "    )\n",
    ")\n",
    "\n",
    "# Mostramos algunos ejemplos con la nueva columna\n",
    "print(\"üìã Viajes con distancia calculada:\")\n",
    "df_con_distancia.select(\n",
    "    \"id\", \"pickup_latitude\", \"pickup_longitude\",\n",
    "    \"dropoff_latitude\", \"dropoff_longitude\", \"distancia_km\"\n",
    ").show(5)\n",
    "\n",
    "# Estad√≠sticas de distancia por vendor\n",
    "print(\"\\nüìä Distancia promedio por vendor:\")\n",
    "df_con_distancia.groupBy(\"vendor_id\") \\\n",
    "    .agg(\n",
    "        spark_round(avg(\"distancia_km\"), 2).alias(\"distancia_promedio_km\"),\n",
    "        spark_round(avg(\"trip_duration\") / 60, 2).alias(\"duracion_promedio_min\")\n",
    "    ) \\\n",
    "    .show()\n",
    "\n",
    "# Estad√≠sticas generales de distancia\n",
    "print(\"\\nüìä Estad√≠sticas generales de distancia:\")\n",
    "df_con_distancia.select(\"distancia_km\").describe().show()\n",
    "\n",
    "# Output esperado:\n",
    "# üìã Viajes con distancia calculada:\n",
    "# +------------+---------------+----------------+----------------+-----------------+------------+\n",
    "# |          id|pickup_latitude|pickup_longitude|dropoff_latitude|dropoff_longitude|distancia_km|\n",
    "# +------------+---------------+----------------+----------------+-----------------+------------+\n",
    "# |  trip_00001|       40.80...|        -73.96..|        40.74...|         -73.98..|        7.23|\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# === RESUMEN FINAL ===\n",
    "\n",
    "## üìù Resumen\n",
    "\n",
    "### Conceptos Clave Aprendidos\n",
    "\n",
    "| Concepto | Descripci√≥n |\n",
    "|----------|-------------|\n",
    "| **Big Data** | Datos tan grandes/complejos que requieren herramientas especiales |\n",
    "| **5 Vs** | Volume, Velocity, Variety, Veracity, Value |\n",
    "| **Datos Estructurados** | Formato fijo, tablas SQL (~10-20%) |\n",
    "| **Datos Semi-estructurados** | JSON, XML, con alguna organizaci√≥n |\n",
    "| **Datos No Estructurados** | Texto libre, im√°genes, video (~80%) |\n",
    "| **SparkSession** | Punto de entrada a PySpark |\n",
    "| **DataFrame** | Tabla distribuida en Spark |\n",
    "| **EDA** | An√°lisis Exploratorio de Datos |\n",
    "\n",
    "### üîó Conexi√≥n con AWS Academy\n",
    "\n",
    "| Concepto Local | Servicio AWS |\n",
    "|----------------|---------------|\n",
    "| SparkSession | Amazon EMR, AWS Glue |\n",
    "| DataFrame | Glue DynamicFrame |\n",
    "| Datos estructurados | Amazon RDS, Redshift |\n",
    "| Datos semi-estructurados | DynamoDB, DocumentDB |\n",
    "| Datos no estructurados | Amazon S3 |\n",
    "| EDA | AWS Glue DataBrew |\n",
    "\n",
    "### ‚û°Ô∏è Siguiente Paso\n",
    "\n",
    "Contin√∫a con: **`labs/02_etl_pipeline/01_etl_concepts.ipynb`**\n",
    "\n",
    "En el pr√≥ximo laboratorio aprender√°s:\n",
    "- Qu√© es ETL (Extract, Transform, Load)\n",
    "- Diferencias entre ETL y ELT\n",
    "- C√≥mo construir un pipeline de datos completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LIMPIEZA FINAL ===\n",
    "# Siempre es buena pr√°ctica cerrar la SparkSession al terminar\n",
    "\n",
    "# Detenemos la SparkSession para liberar recursos\n",
    "spark.stop()\n",
    "\n",
    "print(\"‚úÖ SparkSession cerrada correctamente\")\n",
    "print(\"\\nüéâ ¬°Felicitaciones! Has completado el Lab 01: Fundamentos de Big Data\")\n",
    "print(\"\\nüìö Recuerda revisar los conceptos y practicar los ejercicios.\")\n",
    "print(\"‚û°Ô∏è Siguiente: Lab 02 - ETL Pipeline\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
